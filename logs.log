2022-11-07 16:14:26,042:INFO:PyCaret RegressionExperiment
2022-11-07 16:14:26,043:INFO:Logging name: reg-default-name
2022-11-07 16:14:26,044:INFO:ML Usecase: MLUsecase.REGRESSION
2022-11-07 16:14:26,044:INFO:version 3.0.0.rc4
2022-11-07 16:14:26,044:INFO:Initializing setup()
2022-11-07 16:14:26,044:INFO:self.USI: cea0
2022-11-07 16:14:26,044:INFO:self.variable_keys: {'display_container', 'gpu_param', '_all_models', 'log_plots_param', 'fold_shuffle_param', 'fold_groups_param', 'memory', 'data', 'y_test', 'target_param', 'fold_generator', 'html_param', '_gpu_n_jobs_param', 'exp_id', 'logging_param', '_all_models_internal', 'n_jobs_param', 'variable_keys', 'idx', 'transform_target_method_param', 'exp_name_log', 'y', 'X', '_ml_usecase', 'seed', 'X_test', 'y_train', 'pipeline', '_available_plots', 'USI', 'master_model_container', 'X_train', 'transform_target_param', '_all_metrics'}
2022-11-07 16:14:26,044:INFO:Checking environment
2022-11-07 16:14:26,045:INFO:python_version: 3.7.15
2022-11-07 16:14:26,045:INFO:python_build: ('default', 'Oct 12 2022 19:14:55')
2022-11-07 16:14:26,045:INFO:machine: x86_64
2022-11-07 16:14:26,045:INFO:platform: Linux-5.10.133+-x86_64-with-Ubuntu-18.04-bionic
2022-11-07 16:14:26,046:INFO:Memory: svmem(total=13616353280, available=11862036480, percent=12.9, used=1546706944, free=6883700736, active=682704896, inactive=5679345664, buffers=419213312, cached=4766732288, shared=1302528, slab=279064576)
2022-11-07 16:14:26,049:INFO:Physical Core: 1
2022-11-07 16:14:26,049:INFO:Logical Core: 2
2022-11-07 16:14:26,049:INFO:Checking libraries
2022-11-07 16:14:26,050:INFO:System:
2022-11-07 16:14:26,050:INFO:    python: 3.7.15 (default, Oct 12 2022, 19:14:55)  [GCC 7.5.0]
2022-11-07 16:14:26,050:INFO:executable: /usr/bin/python3
2022-11-07 16:14:26,050:INFO:   machine: Linux-5.10.133+-x86_64-with-Ubuntu-18.04-bionic
2022-11-07 16:14:26,050:INFO:PyCaret required dependencies:
2022-11-07 16:14:26,051:INFO:                 pip: 21.1.3
2022-11-07 16:14:26,051:INFO:          setuptools: 57.4.0
2022-11-07 16:14:26,051:INFO:             pycaret: 3.0.0rc4
2022-11-07 16:14:26,051:INFO:             IPython: 7.9.0
2022-11-07 16:14:26,051:INFO:          ipywidgets: 7.7.1
2022-11-07 16:14:26,052:INFO:                tqdm: 4.64.1
2022-11-07 16:14:26,052:INFO:               numpy: 1.21.6
2022-11-07 16:14:26,053:INFO:              pandas: 1.3.5
2022-11-07 16:14:26,053:INFO:              jinja2: 2.11.3
2022-11-07 16:14:26,053:INFO:               scipy: 1.7.3
2022-11-07 16:14:26,053:INFO:              joblib: 1.2.0
2022-11-07 16:14:26,053:INFO:             sklearn: 1.0.2
2022-11-07 16:14:26,054:INFO:                pyod: 1.0.6
2022-11-07 16:14:26,054:INFO:            imblearn: 0.8.1
2022-11-07 16:14:26,054:INFO:   category_encoders: 2.5.1.post0
2022-11-07 16:14:26,054:INFO:            lightgbm: 3.3.3
2022-11-07 16:14:26,054:INFO:               numba: 0.55.2
2022-11-07 16:14:26,054:INFO:            requests: 2.28.1
2022-11-07 16:14:26,054:INFO:          matplotlib: 3.5.3
2022-11-07 16:14:26,054:INFO:          scikitplot: 0.3.7
2022-11-07 16:14:26,054:INFO:         yellowbrick: 1.5
2022-11-07 16:14:26,054:INFO:              plotly: 5.5.0
2022-11-07 16:14:26,054:INFO:             kaleido: 0.2.1
2022-11-07 16:14:26,054:INFO:         statsmodels: 0.12.2
2022-11-07 16:14:26,054:INFO:              sktime: 0.13.4
2022-11-07 16:14:26,054:INFO:               tbats: 1.1.1
2022-11-07 16:14:26,055:INFO:            pmdarima: 1.8.5
2022-11-07 16:14:26,055:INFO:              psutil: 5.9.3
2022-11-07 16:14:26,055:INFO:PyCaret optional dependencies:
2022-11-07 16:14:26,065:INFO:                shap: Not installed
2022-11-07 16:14:26,065:INFO:           interpret: Not installed
2022-11-07 16:14:26,065:INFO:                umap: Not installed
2022-11-07 16:14:26,065:INFO:    pandas_profiling: 1.4.1
2022-11-07 16:14:26,065:INFO:  explainerdashboard: Not installed
2022-11-07 16:14:26,065:INFO:             autoviz: Not installed
2022-11-07 16:14:26,065:INFO:           fairlearn: Not installed
2022-11-07 16:14:26,066:INFO:             xgboost: 0.90
2022-11-07 16:14:26,066:INFO:            catboost: Not installed
2022-11-07 16:14:26,066:INFO:              kmodes: Not installed
2022-11-07 16:14:26,066:INFO:             mlxtend: 0.14.0
2022-11-07 16:14:26,066:INFO:       statsforecast: Not installed
2022-11-07 16:14:26,066:INFO:        tune_sklearn: Not installed
2022-11-07 16:14:26,066:INFO:                 ray: Not installed
2022-11-07 16:14:26,066:INFO:            hyperopt: 0.1.2
2022-11-07 16:14:26,066:INFO:              optuna: Not installed
2022-11-07 16:14:26,066:INFO:               skopt: Not installed
2022-11-07 16:14:26,066:INFO:              mlflow: Not installed
2022-11-07 16:14:26,066:INFO:              gradio: Not installed
2022-11-07 16:14:26,066:INFO:             fastapi: Not installed
2022-11-07 16:14:26,067:INFO:             uvicorn: Not installed
2022-11-07 16:14:26,067:INFO:              m2cgen: Not installed
2022-11-07 16:14:26,067:INFO:           evidently: Not installed
2022-11-07 16:14:26,067:INFO:                nltk: 3.7
2022-11-07 16:14:26,067:INFO:            pyLDAvis: Not installed
2022-11-07 16:14:26,067:INFO:              gensim: 3.6.0
2022-11-07 16:14:26,067:INFO:               spacy: 3.4.2
2022-11-07 16:14:26,067:INFO:           wordcloud: 1.8.2.2
2022-11-07 16:14:26,067:INFO:            textblob: 0.15.3
2022-11-07 16:14:26,067:INFO:               fugue: Not installed
2022-11-07 16:14:26,067:INFO:           streamlit: Not installed
2022-11-07 16:14:26,067:INFO:             prophet: 1.1.1
2022-11-07 16:14:26,067:INFO:None
2022-11-07 16:14:26,068:INFO:Set up data.
2022-11-07 16:14:26,102:INFO:Set up train/test split.
2022-11-07 16:14:26,110:INFO:Set up index.
2022-11-07 16:14:26,111:INFO:Set up folding strategy.
2022-11-07 16:14:26,111:INFO:Assigning column types.
2022-11-07 16:14:26,119:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2022-11-07 16:14:26,120:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2022-11-07 16:14:26,125:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2022-11-07 16:14:26,131:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2022-11-07 16:14:26,202:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2022-11-07 16:14:26,262:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2022-11-07 16:14:26,264:INFO:Soft dependency imported: xgboost: 0.90
2022-11-07 16:14:26,337:WARNING:Wrong xgboost version. Expected xgboost>=1.1.0, got xgboost==0.90
2022-11-07 16:14:26,492:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-11-07 16:14:26,493:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2022-11-07 16:14:26,499:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2022-11-07 16:14:26,505:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2022-11-07 16:14:26,576:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2022-11-07 16:14:26,630:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2022-11-07 16:14:26,631:INFO:Soft dependency imported: xgboost: 0.90
2022-11-07 16:14:26,632:WARNING:Wrong xgboost version. Expected xgboost>=1.1.0, got xgboost==0.90
2022-11-07 16:14:26,632:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-11-07 16:14:26,633:INFO:Engine successfully changes for model 'lasso' to 'sklearn'.
2022-11-07 16:14:26,638:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2022-11-07 16:14:26,644:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2022-11-07 16:14:26,719:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2022-11-07 16:14:26,774:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2022-11-07 16:14:26,774:INFO:Soft dependency imported: xgboost: 0.90
2022-11-07 16:14:26,775:WARNING:Wrong xgboost version. Expected xgboost>=1.1.0, got xgboost==0.90
2022-11-07 16:14:26,775:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-11-07 16:14:26,781:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2022-11-07 16:14:26,786:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2022-11-07 16:14:26,864:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2022-11-07 16:14:26,919:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2022-11-07 16:14:26,920:INFO:Soft dependency imported: xgboost: 0.90
2022-11-07 16:14:26,921:WARNING:Wrong xgboost version. Expected xgboost>=1.1.0, got xgboost==0.90
2022-11-07 16:14:26,921:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-11-07 16:14:26,922:INFO:Engine successfully changes for model 'ridge' to 'sklearn'.
2022-11-07 16:14:26,941:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2022-11-07 16:14:27,014:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2022-11-07 16:14:27,076:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2022-11-07 16:14:27,077:INFO:Soft dependency imported: xgboost: 0.90
2022-11-07 16:14:27,077:WARNING:Wrong xgboost version. Expected xgboost>=1.1.0, got xgboost==0.90
2022-11-07 16:14:27,077:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-11-07 16:14:27,091:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2022-11-07 16:14:27,162:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2022-11-07 16:14:27,218:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2022-11-07 16:14:27,219:INFO:Soft dependency imported: xgboost: 0.90
2022-11-07 16:14:27,220:WARNING:Wrong xgboost version. Expected xgboost>=1.1.0, got xgboost==0.90
2022-11-07 16:14:27,220:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-11-07 16:14:27,221:INFO:Engine successfully changes for model 'en' to 'sklearn'.
2022-11-07 16:14:27,310:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2022-11-07 16:14:27,365:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2022-11-07 16:14:27,366:INFO:Soft dependency imported: xgboost: 0.90
2022-11-07 16:14:27,367:WARNING:Wrong xgboost version. Expected xgboost>=1.1.0, got xgboost==0.90
2022-11-07 16:14:27,367:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-11-07 16:14:27,461:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2022-11-07 16:14:27,518:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2022-11-07 16:14:27,520:INFO:Soft dependency imported: xgboost: 0.90
2022-11-07 16:14:27,520:WARNING:Wrong xgboost version. Expected xgboost>=1.1.0, got xgboost==0.90
2022-11-07 16:14:27,520:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-11-07 16:14:27,521:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2022-11-07 16:14:27,602:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2022-11-07 16:14:27,656:INFO:Soft dependency imported: xgboost: 0.90
2022-11-07 16:14:27,657:WARNING:Wrong xgboost version. Expected xgboost>=1.1.0, got xgboost==0.90
2022-11-07 16:14:27,658:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-11-07 16:14:27,740:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2022-11-07 16:14:27,794:INFO:Soft dependency imported: xgboost: 0.90
2022-11-07 16:14:27,795:WARNING:Wrong xgboost version. Expected xgboost>=1.1.0, got xgboost==0.90
2022-11-07 16:14:27,795:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-11-07 16:14:27,796:INFO:Engine successfully changes for model 'svm' to 'sklearn'.
2022-11-07 16:14:27,931:INFO:Soft dependency imported: xgboost: 0.90
2022-11-07 16:14:27,932:WARNING:Wrong xgboost version. Expected xgboost>=1.1.0, got xgboost==0.90
2022-11-07 16:14:27,932:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-11-07 16:14:28,077:INFO:Soft dependency imported: xgboost: 0.90
2022-11-07 16:14:28,078:WARNING:Wrong xgboost version. Expected xgboost>=1.1.0, got xgboost==0.90
2022-11-07 16:14:28,078:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-11-07 16:14:28,080:INFO:Preparing preprocessing pipeline...
2022-11-07 16:14:28,082:INFO:Set up simple imputation.
2022-11-07 16:14:28,082:INFO:Set up variance threshold.
2022-11-07 16:14:28,261:INFO:Finished creating preprocessing pipeline.
2022-11-07 16:14:28,268:INFO:Pipeline: Pipeline(memory=Memory(location=/tmp/joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['1rd_red', ' 1rd_green',
                                             ' 1rd_blue', ' 1rd_r', ' 1rd_rg',
                                             ' 1rd_most_red', ' 1rd_least_red',
                                             ' 1rd_most_yellow', ' 1rd_size',
                                             ' 2nd_red', ' 2nd_green',
                                             ' 2nd_blue', ' 2rd_r', ' 2rd_rg',
                                             ' 2rd_most_red', ' 2rd_least_red',
                                             ' 2rd_most_yellow', ' 2nd_size',
                                             ' 3rd_red', ' 3rd_green',
                                             ' 3rd_bl...d_rg',
                                             ' 3rd_most_red', ' 3rd_least_red',
                                             ' 3rd_most_yellow', ' 3rd_size',
                                             ' 4th_red', ' 4th_green',
                                             ' 4th_blue', ...],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=[],
                                    transformer=SimpleImputer(fill_value='constant',
                                                              strategy='constant'))),
                ('low_variance',
                 TransformerWrapper(exclude=[],
                                    transformer=VarianceThreshold(threshold=0)))])
2022-11-07 16:14:28,268:INFO:Creating final display dataframe.
2022-11-07 16:14:28,875:INFO:Setup display_container:                Description             Value
0               Session id              4769
1                   Target         Sweetness
2              Target type        Regression
3               Data shape         (200, 56)
4         Train data shape         (139, 56)
5          Test data shape          (61, 56)
6         Numeric features                55
7               Preprocess              True
8          Imputation type            simple
9       Numeric imputation              mean
10  Categorical imputation          constant
11  Low variance threshold                 0
12          Fold Generator             KFold
13             Fold Number                10
14                CPU Jobs                -1
15                 Use GPU             False
16          Log Experiment             False
17         Experiment Name  reg-default-name
18                     USI              cea0
2022-11-07 16:14:29,037:INFO:Soft dependency imported: xgboost: 0.90
2022-11-07 16:14:29,037:WARNING:Wrong xgboost version. Expected xgboost>=1.1.0, got xgboost==0.90
2022-11-07 16:14:29,038:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-11-07 16:14:29,175:INFO:Soft dependency imported: xgboost: 0.90
2022-11-07 16:14:29,175:WARNING:Wrong xgboost version. Expected xgboost>=1.1.0, got xgboost==0.90
2022-11-07 16:14:29,176:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-11-07 16:14:29,185:INFO:setup() successfully completed in 3.15s...............
2022-11-07 16:14:49,751:INFO:Initializing compare_models()
2022-11-07 16:14:49,753:INFO:compare_models(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f20cc3726d0>, include=None, fold=None, round=4, cross_validation=True, sort=R2, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.regression.oop.RegressionExperiment object at 0x7f20cc3726d0>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'R2', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.regression.oop.RegressionExperiment'>}, exclude=None)
2022-11-07 16:14:49,753:INFO:Checking exceptions
2022-11-07 16:14:49,759:INFO:Preparing display monitor
2022-11-07 16:14:49,846:INFO:Initializing Linear Regression
2022-11-07 16:14:49,846:INFO:Total runtime is 6.413459777832031e-06 minutes
2022-11-07 16:14:49,858:INFO:SubProcess create_model() called ==================================
2022-11-07 16:14:49,859:INFO:Initializing create_model()
2022-11-07 16:14:49,859:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f20cc3726d0>, estimator=lr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f20c2259610>, model_only=True, return_train_score=False, kwargs={})
2022-11-07 16:14:49,859:INFO:Checking exceptions
2022-11-07 16:14:49,863:INFO:Importing libraries
2022-11-07 16:14:49,865:INFO:Copying training dataset
2022-11-07 16:14:49,875:INFO:Defining folds
2022-11-07 16:14:49,875:INFO:Declaring metric variables
2022-11-07 16:14:49,886:INFO:Importing untrained model
2022-11-07 16:14:49,894:INFO:Linear Regression Imported successfully
2022-11-07 16:14:49,914:INFO:Starting cross validation
2022-11-07 16:14:49,927:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-11-07 16:14:58,580:INFO:Calculating mean and std
2022-11-07 16:14:58,585:INFO:Creating metrics dataframe
2022-11-07 16:14:58,599:INFO:Uploading results into container
2022-11-07 16:14:58,600:INFO:Uploading model into container now
2022-11-07 16:14:58,600:INFO:master_model_container: 1
2022-11-07 16:14:58,600:INFO:display_container: 2
2022-11-07 16:14:58,601:INFO:LinearRegression(n_jobs=-1)
2022-11-07 16:14:58,601:INFO:create_model() successfully completed......................................
2022-11-07 16:14:58,824:INFO:SubProcess create_model() end ==================================
2022-11-07 16:14:58,824:INFO:Creating metrics dataframe
2022-11-07 16:14:58,846:INFO:Initializing Lasso Regression
2022-11-07 16:14:58,846:INFO:Total runtime is 0.1500043511390686 minutes
2022-11-07 16:14:58,857:INFO:SubProcess create_model() called ==================================
2022-11-07 16:14:58,858:INFO:Initializing create_model()
2022-11-07 16:14:58,858:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f20cc3726d0>, estimator=lasso, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f20c2259610>, model_only=True, return_train_score=False, kwargs={})
2022-11-07 16:14:58,858:INFO:Checking exceptions
2022-11-07 16:14:58,862:INFO:Importing libraries
2022-11-07 16:14:58,862:INFO:Copying training dataset
2022-11-07 16:14:58,871:INFO:Defining folds
2022-11-07 16:14:58,872:INFO:Declaring metric variables
2022-11-07 16:14:58,882:INFO:Importing untrained model
2022-11-07 16:14:58,891:INFO:Lasso Regression Imported successfully
2022-11-07 16:14:58,909:INFO:Starting cross validation
2022-11-07 16:14:58,914:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-11-07 16:14:58,970:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.736e-02, tolerance: 2.137e-02
  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive

2022-11-07 16:14:59,028:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.225e+00, tolerance: 2.197e-02
  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive

2022-11-07 16:14:59,030:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.574e+00, tolerance: 2.363e-02
  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive

2022-11-07 16:14:59,112:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.894e-01, tolerance: 2.317e-02
  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive

2022-11-07 16:14:59,154:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.094e-01, tolerance: 2.201e-02
  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive

2022-11-07 16:14:59,209:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.128e-01, tolerance: 2.033e-02
  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive

2022-11-07 16:14:59,209:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.448e-01, tolerance: 2.264e-02
  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive

2022-11-07 16:14:59,268:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.714e-01, tolerance: 2.274e-02
  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive

2022-11-07 16:14:59,286:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.413e-02, tolerance: 2.347e-02
  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive

2022-11-07 16:14:59,325:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.914e-01, tolerance: 2.344e-02
  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive

2022-11-07 16:14:59,341:INFO:Calculating mean and std
2022-11-07 16:14:59,343:INFO:Creating metrics dataframe
2022-11-07 16:14:59,356:INFO:Uploading results into container
2022-11-07 16:14:59,357:INFO:Uploading model into container now
2022-11-07 16:14:59,358:INFO:master_model_container: 2
2022-11-07 16:14:59,358:INFO:display_container: 2
2022-11-07 16:14:59,358:INFO:Lasso(random_state=4769)
2022-11-07 16:14:59,358:INFO:create_model() successfully completed......................................
2022-11-07 16:14:59,493:INFO:SubProcess create_model() end ==================================
2022-11-07 16:14:59,494:INFO:Creating metrics dataframe
2022-11-07 16:14:59,515:INFO:Initializing Ridge Regression
2022-11-07 16:14:59,516:INFO:Total runtime is 0.1611649473508199 minutes
2022-11-07 16:14:59,525:INFO:SubProcess create_model() called ==================================
2022-11-07 16:14:59,526:INFO:Initializing create_model()
2022-11-07 16:14:59,526:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f20cc3726d0>, estimator=ridge, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f20c2259610>, model_only=True, return_train_score=False, kwargs={})
2022-11-07 16:14:59,526:INFO:Checking exceptions
2022-11-07 16:14:59,530:INFO:Importing libraries
2022-11-07 16:14:59,530:INFO:Copying training dataset
2022-11-07 16:14:59,539:INFO:Defining folds
2022-11-07 16:14:59,540:INFO:Declaring metric variables
2022-11-07 16:14:59,549:INFO:Importing untrained model
2022-11-07 16:14:59,557:INFO:Ridge Regression Imported successfully
2022-11-07 16:14:59,571:INFO:Starting cross validation
2022-11-07 16:14:59,573:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-11-07 16:14:59,910:INFO:Calculating mean and std
2022-11-07 16:14:59,914:INFO:Creating metrics dataframe
2022-11-07 16:14:59,926:INFO:Uploading results into container
2022-11-07 16:14:59,926:INFO:Uploading model into container now
2022-11-07 16:14:59,927:INFO:master_model_container: 3
2022-11-07 16:14:59,927:INFO:display_container: 2
2022-11-07 16:14:59,928:INFO:Ridge(random_state=4769)
2022-11-07 16:14:59,928:INFO:create_model() successfully completed......................................
2022-11-07 16:15:00,057:INFO:SubProcess create_model() end ==================================
2022-11-07 16:15:00,058:INFO:Creating metrics dataframe
2022-11-07 16:15:00,083:INFO:Initializing Elastic Net
2022-11-07 16:15:00,084:INFO:Total runtime is 0.17062922716140747 minutes
2022-11-07 16:15:00,093:INFO:SubProcess create_model() called ==================================
2022-11-07 16:15:00,093:INFO:Initializing create_model()
2022-11-07 16:15:00,094:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f20cc3726d0>, estimator=en, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f20c2259610>, model_only=True, return_train_score=False, kwargs={})
2022-11-07 16:15:00,094:INFO:Checking exceptions
2022-11-07 16:15:00,097:INFO:Importing libraries
2022-11-07 16:15:00,097:INFO:Copying training dataset
2022-11-07 16:15:00,104:INFO:Defining folds
2022-11-07 16:15:00,107:INFO:Declaring metric variables
2022-11-07 16:15:00,116:INFO:Importing untrained model
2022-11-07 16:15:00,125:INFO:Elastic Net Imported successfully
2022-11-07 16:15:00,141:INFO:Starting cross validation
2022-11-07 16:15:00,145:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-11-07 16:15:00,207:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.101e+00, tolerance: 2.137e-02
  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive

2022-11-07 16:15:00,255:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.995e+00, tolerance: 2.363e-02
  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive

2022-11-07 16:15:00,277:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.520e+00, tolerance: 2.197e-02
  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive

2022-11-07 16:15:00,352:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.001e+00, tolerance: 2.201e-02
  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive

2022-11-07 16:15:00,375:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.489e+00, tolerance: 2.317e-02
  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive

2022-11-07 16:15:00,408:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.759e+00, tolerance: 2.264e-02
  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive

2022-11-07 16:15:00,459:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.461e+00, tolerance: 2.033e-02
  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive

2022-11-07 16:15:00,467:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.002e+00, tolerance: 2.274e-02
  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive

2022-11-07 16:15:00,515:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.351e-01, tolerance: 2.347e-02
  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive

2022-11-07 16:15:00,525:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.422e-01, tolerance: 2.344e-02
  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive

2022-11-07 16:15:00,543:INFO:Calculating mean and std
2022-11-07 16:15:00,546:INFO:Creating metrics dataframe
2022-11-07 16:15:00,559:INFO:Uploading results into container
2022-11-07 16:15:00,560:INFO:Uploading model into container now
2022-11-07 16:15:00,561:INFO:master_model_container: 4
2022-11-07 16:15:00,561:INFO:display_container: 2
2022-11-07 16:15:00,562:INFO:ElasticNet(random_state=4769)
2022-11-07 16:15:00,562:INFO:create_model() successfully completed......................................
2022-11-07 16:15:00,692:INFO:SubProcess create_model() end ==================================
2022-11-07 16:15:00,693:INFO:Creating metrics dataframe
2022-11-07 16:15:00,712:INFO:Initializing Least Angle Regression
2022-11-07 16:15:00,712:INFO:Total runtime is 0.18110638856887817 minutes
2022-11-07 16:15:00,720:INFO:SubProcess create_model() called ==================================
2022-11-07 16:15:00,721:INFO:Initializing create_model()
2022-11-07 16:15:00,722:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f20cc3726d0>, estimator=lar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f20c2259610>, model_only=True, return_train_score=False, kwargs={})
2022-11-07 16:15:00,722:INFO:Checking exceptions
2022-11-07 16:15:00,725:INFO:Importing libraries
2022-11-07 16:15:00,725:INFO:Copying training dataset
2022-11-07 16:15:00,731:INFO:Defining folds
2022-11-07 16:15:00,731:INFO:Declaring metric variables
2022-11-07 16:15:00,743:INFO:Importing untrained model
2022-11-07 16:15:00,757:INFO:Least Angle Regression Imported successfully
2022-11-07 16:15:00,777:INFO:Starting cross validation
2022-11-07 16:15:00,784:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-11-07 16:15:00,832:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2022-11-07 16:15:00,847:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2022-11-07 16:15:00,864:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 62 iterations, i.e. alpha=2.135e-01, with an active set of 46 regressors, and the smallest cholesky pivot element being 8.560e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:15:00,865:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 64 iterations, i.e. alpha=1.641e-01, with an active set of 47 regressors, and the smallest cholesky pivot element being 7.451e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:15:00,865:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 64 iterations, i.e. alpha=1.548e-01, with an active set of 47 regressors, and the smallest cholesky pivot element being 7.451e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:15:00,866:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 60 iterations, i.e. alpha=1.143e-01, with an active set of 48 regressors, and the smallest cholesky pivot element being 7.814e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:15:00,866:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 67 iterations, i.e. alpha=8.314e-02, with an active set of 49 regressors, and the smallest cholesky pivot element being 7.068e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:15:00,867:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 67 iterations, i.e. alpha=7.811e-02, with an active set of 49 regressors, and the smallest cholesky pivot element being 7.300e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:15:00,868:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 68 iterations, i.e. alpha=3.865e-02, with an active set of 50 regressors, and the smallest cholesky pivot element being 7.300e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:15:00,868:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 68 iterations, i.e. alpha=3.141e-02, with an active set of 50 regressors, and the smallest cholesky pivot element being 7.068e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:15:00,869:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 70 iterations, i.e. alpha=3.640e-02, with an active set of 51 regressors, and the smallest cholesky pivot element being 6.580e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:15:00,866:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 60 iterations, i.e. alpha=8.919e-02, with an active set of 48 regressors, and the smallest cholesky pivot element being 8.297e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:15:00,870:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 62 iterations, i.e. alpha=7.325e-02, with an active set of 49 regressors, and the smallest cholesky pivot element being 6.234e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:15:00,871:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 63 iterations, i.e. alpha=2.935e-02, with an active set of 50 regressors, and the smallest cholesky pivot element being 6.144e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:15:00,871:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 63 iterations, i.e. alpha=2.018e-02, with an active set of 50 regressors, and the smallest cholesky pivot element being 6.144e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:15:00,871:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 63 iterations, i.e. alpha=1.872e-02, with an active set of 50 regressors, and the smallest cholesky pivot element being 8.162e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:15:00,872:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 63 iterations, i.e. alpha=1.656e-02, with an active set of 50 regressors, and the smallest cholesky pivot element being 7.814e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:15:00,873:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 65 iterations, i.e. alpha=1.218e-04, with an active set of 51 regressors, and the smallest cholesky pivot element being 8.025e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:15:00,874:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 65 iterations, i.e. alpha=7.852e-05, with an active set of 51 regressors, and the smallest cholesky pivot element being 6.053e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:15:00,874:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 65 iterations, i.e. alpha=7.802e-05, with an active set of 51 regressors, and the smallest cholesky pivot element being 6.495e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:15:00,875:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 66 iterations, i.e. alpha=1.488e-05, with an active set of 52 regressors, and the smallest cholesky pivot element being 7.885e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:15:00,875:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 66 iterations, i.e. alpha=5.438e-06, with an active set of 52 regressors, and the smallest cholesky pivot element being 7.885e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:15:00,875:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 66 iterations, i.e. alpha=3.434e-06, with an active set of 52 regressors, and the smallest cholesky pivot element being 6.495e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:15:00,872:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 71 iterations, i.e. alpha=6.248e-03, with an active set of 52 regressors, and the smallest cholesky pivot element being 8.297e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:15:00,876:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 71 iterations, i.e. alpha=3.198e-03, with an active set of 52 regressors, and the smallest cholesky pivot element being 6.409e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:15:00,876:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 71 iterations, i.e. alpha=1.948e-03, with an active set of 52 regressors, and the smallest cholesky pivot element being 7.224e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:15:00,936:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2022-11-07 16:15:00,948:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2022-11-07 16:15:00,954:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 75 iterations, i.e. alpha=3.507e-01, with an active set of 50 regressors, and the smallest cholesky pivot element being 7.885e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:15:00,957:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 75 iterations, i.e. alpha=2.469e-01, with an active set of 50 regressors, and the smallest cholesky pivot element being 7.885e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:15:00,958:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 75 iterations, i.e. alpha=1.832e-01, with an active set of 50 regressors, and the smallest cholesky pivot element being 8.625e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:15:00,959:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 80 iterations, i.e. alpha=1.183e-01, with an active set of 52 regressors, and the smallest cholesky pivot element being 6.664e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:15:00,960:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 80 iterations, i.e. alpha=8.047e-02, with an active set of 52 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:15:00,960:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 80 iterations, i.e. alpha=3.159e-02, with an active set of 52 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:15:00,980:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 59 iterations, i.e. alpha=2.105e-02, with an active set of 47 regressors, and the smallest cholesky pivot element being 6.829e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:15:00,982:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 68 iterations, i.e. alpha=1.257e-02, with an active set of 52 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:15:00,983:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 68 iterations, i.e. alpha=5.557e-03, with an active set of 52 regressors, and the smallest cholesky pivot element being 6.144e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:15:00,983:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 68 iterations, i.e. alpha=3.699e-03, with an active set of 52 regressors, and the smallest cholesky pivot element being 6.144e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:15:01,035:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2022-11-07 16:15:01,050:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2022-11-07 16:15:01,054:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 76 iterations, i.e. alpha=5.089e-01, with an active set of 50 regressors, and the smallest cholesky pivot element being 6.580e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:15:01,059:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 76 iterations, i.e. alpha=3.081e-01, with an active set of 50 regressors, and the smallest cholesky pivot element being 8.495e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:15:01,059:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 78 iterations, i.e. alpha=1.774e-01, with an active set of 51 regressors, and the smallest cholesky pivot element being 6.747e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:15:01,060:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 78 iterations, i.e. alpha=1.147e-01, with an active set of 51 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:15:01,060:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 78 iterations, i.e. alpha=7.414e-03, with an active set of 51 regressors, and the smallest cholesky pivot element being 6.053e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:15:01,061:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 78 iterations, i.e. alpha=4.213e-03, with an active set of 51 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:15:01,071:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 67 iterations, i.e. alpha=5.183e+03, with an active set of 46 regressors, and the smallest cholesky pivot element being 8.560e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:15:01,071:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 67 iterations, i.e. alpha=5.066e+03, with an active set of 46 regressors, and the smallest cholesky pivot element being 7.068e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:15:01,072:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 69 iterations, i.e. alpha=3.954e+03, with an active set of 47 regressors, and the smallest cholesky pivot element being 9.424e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:15:01,074:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 75 iterations, i.e. alpha=5.333e+03, with an active set of 50 regressors, and the smallest cholesky pivot element being 7.300e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:15:01,076:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 79 iterations, i.e. alpha=5.491e+03, with an active set of 51 regressors, and the smallest cholesky pivot element being 6.664e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:15:01,076:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 79 iterations, i.e. alpha=3.318e+03, with an active set of 51 regressors, and the smallest cholesky pivot element being 7.300e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:15:01,076:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 79 iterations, i.e. alpha=3.167e+03, with an active set of 51 regressors, and the smallest cholesky pivot element being 6.053e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:15:01,077:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 79 iterations, i.e. alpha=2.158e+03, with an active set of 51 regressors, and the smallest cholesky pivot element being 6.053e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:15:01,112:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2022-11-07 16:15:01,132:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2022-11-07 16:15:01,136:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 58 iterations, i.e. alpha=2.927e-02, with an active set of 45 regressors, and the smallest cholesky pivot element being 8.941e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:15:01,141:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 75 iterations, i.e. alpha=1.177e-01, with an active set of 52 regressors, and the smallest cholesky pivot element being 6.144e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:15:01,141:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 75 iterations, i.e. alpha=1.155e-01, with an active set of 52 regressors, and the smallest cholesky pivot element being 6.144e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:15:01,142:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 75 iterations, i.e. alpha=1.154e-01, with an active set of 52 regressors, and the smallest cholesky pivot element being 8.689e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:15:01,147:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 60 iterations, i.e. alpha=2.290e+03, with an active set of 45 regressors, and the smallest cholesky pivot element being 6.322e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:15:01,149:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 65 iterations, i.e. alpha=2.023e+03, with an active set of 47 regressors, and the smallest cholesky pivot element being 9.003e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:15:01,151:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 72 iterations, i.e. alpha=1.123e+03, with an active set of 51 regressors, and the smallest cholesky pivot element being 5.867e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:15:01,152:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 72 iterations, i.e. alpha=1.120e+03, with an active set of 51 regressors, and the smallest cholesky pivot element being 6.234e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:15:01,152:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 72 iterations, i.e. alpha=6.473e+02, with an active set of 51 regressors, and the smallest cholesky pivot element being 6.234e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:15:01,153:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 72 iterations, i.e. alpha=7.759e+01, with an active set of 51 regressors, and the smallest cholesky pivot element being 7.743e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:15:01,190:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2022-11-07 16:15:01,205:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2022-11-07 16:15:01,206:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 58 iterations, i.e. alpha=5.058e-03, with an active set of 46 regressors, and the smallest cholesky pivot element being 6.234e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:15:01,207:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 59 iterations, i.e. alpha=3.984e-03, with an active set of 47 regressors, and the smallest cholesky pivot element being 6.144e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:15:01,208:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 60 iterations, i.e. alpha=2.165e-03, with an active set of 48 regressors, and the smallest cholesky pivot element being 5.867e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:15:01,209:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 64 iterations, i.e. alpha=1.077e-03, with an active set of 51 regressors, and the smallest cholesky pivot element being 8.363e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:15:01,210:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 65 iterations, i.e. alpha=5.060e-04, with an active set of 52 regressors, and the smallest cholesky pivot element being 8.229e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:15:01,210:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 65 iterations, i.e. alpha=4.007e-04, with an active set of 52 regressors, and the smallest cholesky pivot element being 8.229e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:15:01,211:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 65 iterations, i.e. alpha=3.320e-05, with an active set of 52 regressors, and the smallest cholesky pivot element being 8.025e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:15:01,221:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 62 iterations, i.e. alpha=2.903e-02, with an active set of 46 regressors, and the smallest cholesky pivot element being 7.814e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:15:01,223:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 69 iterations, i.e. alpha=2.040e-02, with an active set of 50 regressors, and the smallest cholesky pivot element being 6.989e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:15:01,225:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 70 iterations, i.e. alpha=5.525e-03, with an active set of 51 regressors, and the smallest cholesky pivot element being 6.409e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:15:01,226:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 71 iterations, i.e. alpha=2.215e-03, with an active set of 52 regressors, and the smallest cholesky pivot element being 9.424e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:15:01,226:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 71 iterations, i.e. alpha=3.971e-05, with an active set of 52 regressors, and the smallest cholesky pivot element being 9.424e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:15:01,227:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 71 iterations, i.e. alpha=2.503e-05, with an active set of 52 regressors, and the smallest cholesky pivot element being 7.525e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:15:01,244:INFO:Calculating mean and std
2022-11-07 16:15:01,246:INFO:Creating metrics dataframe
2022-11-07 16:15:01,260:INFO:Uploading results into container
2022-11-07 16:15:01,261:INFO:Uploading model into container now
2022-11-07 16:15:01,262:INFO:master_model_container: 5
2022-11-07 16:15:01,263:INFO:display_container: 2
2022-11-07 16:15:01,263:INFO:Lars(random_state=4769)
2022-11-07 16:15:01,263:INFO:create_model() successfully completed......................................
2022-11-07 16:15:01,397:INFO:SubProcess create_model() end ==================================
2022-11-07 16:15:01,398:INFO:Creating metrics dataframe
2022-11-07 16:15:01,418:INFO:Initializing Lasso Least Angle Regression
2022-11-07 16:15:01,418:INFO:Total runtime is 0.19287121295928955 minutes
2022-11-07 16:15:01,427:INFO:SubProcess create_model() called ==================================
2022-11-07 16:15:01,427:INFO:Initializing create_model()
2022-11-07 16:15:01,428:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f20cc3726d0>, estimator=llar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f20c2259610>, model_only=True, return_train_score=False, kwargs={})
2022-11-07 16:15:01,428:INFO:Checking exceptions
2022-11-07 16:15:01,431:INFO:Importing libraries
2022-11-07 16:15:01,432:INFO:Copying training dataset
2022-11-07 16:15:01,437:INFO:Defining folds
2022-11-07 16:15:01,438:INFO:Declaring metric variables
2022-11-07 16:15:01,447:INFO:Importing untrained model
2022-11-07 16:15:01,457:INFO:Lasso Least Angle Regression Imported successfully
2022-11-07 16:15:01,471:INFO:Starting cross validation
2022-11-07 16:15:01,473:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-11-07 16:15:01,522:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  FutureWarning,

2022-11-07 16:15:01,552:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  FutureWarning,

2022-11-07 16:15:01,592:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  FutureWarning,

2022-11-07 16:15:01,622:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  FutureWarning,

2022-11-07 16:15:01,664:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  FutureWarning,

2022-11-07 16:15:01,710:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  FutureWarning,

2022-11-07 16:15:01,713:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  FutureWarning,

2022-11-07 16:15:01,754:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  FutureWarning,

2022-11-07 16:15:01,781:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  FutureWarning,

2022-11-07 16:15:01,799:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  FutureWarning,

2022-11-07 16:15:01,814:INFO:Calculating mean and std
2022-11-07 16:15:01,817:INFO:Creating metrics dataframe
2022-11-07 16:15:01,829:INFO:Uploading results into container
2022-11-07 16:15:01,831:INFO:Uploading model into container now
2022-11-07 16:15:01,832:INFO:master_model_container: 6
2022-11-07 16:15:01,832:INFO:display_container: 2
2022-11-07 16:15:01,833:INFO:LassoLars(random_state=4769)
2022-11-07 16:15:01,833:INFO:create_model() successfully completed......................................
2022-11-07 16:15:01,965:INFO:SubProcess create_model() end ==================================
2022-11-07 16:15:01,965:INFO:Creating metrics dataframe
2022-11-07 16:15:01,986:INFO:Initializing Orthogonal Matching Pursuit
2022-11-07 16:15:01,987:INFO:Total runtime is 0.20234455664952597 minutes
2022-11-07 16:15:01,997:INFO:SubProcess create_model() called ==================================
2022-11-07 16:15:01,998:INFO:Initializing create_model()
2022-11-07 16:15:01,998:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f20cc3726d0>, estimator=omp, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f20c2259610>, model_only=True, return_train_score=False, kwargs={})
2022-11-07 16:15:01,998:INFO:Checking exceptions
2022-11-07 16:15:02,011:INFO:Importing libraries
2022-11-07 16:15:02,011:INFO:Copying training dataset
2022-11-07 16:15:02,020:INFO:Defining folds
2022-11-07 16:15:02,020:INFO:Declaring metric variables
2022-11-07 16:15:02,028:INFO:Importing untrained model
2022-11-07 16:15:02,036:INFO:Orthogonal Matching Pursuit Imported successfully
2022-11-07 16:15:02,052:INFO:Starting cross validation
2022-11-07 16:15:02,054:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-11-07 16:15:02,122:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2022-11-07 16:15:02,147:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2022-11-07 16:15:02,204:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2022-11-07 16:15:02,236:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2022-11-07 16:15:02,273:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2022-11-07 16:15:02,319:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2022-11-07 16:15:02,322:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2022-11-07 16:15:02,366:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2022-11-07 16:15:02,385:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2022-11-07 16:15:02,411:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2022-11-07 16:15:02,427:INFO:Calculating mean and std
2022-11-07 16:15:02,429:INFO:Creating metrics dataframe
2022-11-07 16:15:02,438:INFO:Uploading results into container
2022-11-07 16:15:02,439:INFO:Uploading model into container now
2022-11-07 16:15:02,440:INFO:master_model_container: 7
2022-11-07 16:15:02,440:INFO:display_container: 2
2022-11-07 16:15:02,441:INFO:OrthogonalMatchingPursuit()
2022-11-07 16:15:02,441:INFO:create_model() successfully completed......................................
2022-11-07 16:15:02,575:INFO:SubProcess create_model() end ==================================
2022-11-07 16:15:02,577:INFO:Creating metrics dataframe
2022-11-07 16:15:02,599:INFO:Initializing Bayesian Ridge
2022-11-07 16:15:02,600:INFO:Total runtime is 0.21256160338719687 minutes
2022-11-07 16:15:02,610:INFO:SubProcess create_model() called ==================================
2022-11-07 16:15:02,611:INFO:Initializing create_model()
2022-11-07 16:15:02,611:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f20cc3726d0>, estimator=br, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f20c2259610>, model_only=True, return_train_score=False, kwargs={})
2022-11-07 16:15:02,612:INFO:Checking exceptions
2022-11-07 16:15:02,616:INFO:Importing libraries
2022-11-07 16:15:02,617:INFO:Copying training dataset
2022-11-07 16:15:02,624:INFO:Defining folds
2022-11-07 16:15:02,626:INFO:Declaring metric variables
2022-11-07 16:15:02,635:INFO:Importing untrained model
2022-11-07 16:15:02,645:INFO:Bayesian Ridge Imported successfully
2022-11-07 16:15:02,664:INFO:Starting cross validation
2022-11-07 16:15:02,669:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-11-07 16:15:03,022:INFO:Calculating mean and std
2022-11-07 16:15:03,024:INFO:Creating metrics dataframe
2022-11-07 16:15:03,038:INFO:Uploading results into container
2022-11-07 16:15:03,039:INFO:Uploading model into container now
2022-11-07 16:15:03,040:INFO:master_model_container: 8
2022-11-07 16:15:03,040:INFO:display_container: 2
2022-11-07 16:15:03,040:INFO:BayesianRidge()
2022-11-07 16:15:03,041:INFO:create_model() successfully completed......................................
2022-11-07 16:15:03,172:INFO:SubProcess create_model() end ==================================
2022-11-07 16:15:03,172:INFO:Creating metrics dataframe
2022-11-07 16:15:03,198:INFO:Initializing Passive Aggressive Regressor
2022-11-07 16:15:03,198:INFO:Total runtime is 0.22253581682840984 minutes
2022-11-07 16:15:03,207:INFO:SubProcess create_model() called ==================================
2022-11-07 16:15:03,208:INFO:Initializing create_model()
2022-11-07 16:15:03,208:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f20cc3726d0>, estimator=par, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f20c2259610>, model_only=True, return_train_score=False, kwargs={})
2022-11-07 16:15:03,208:INFO:Checking exceptions
2022-11-07 16:15:03,211:INFO:Importing libraries
2022-11-07 16:15:03,212:INFO:Copying training dataset
2022-11-07 16:15:03,217:INFO:Defining folds
2022-11-07 16:15:03,217:INFO:Declaring metric variables
2022-11-07 16:15:03,230:INFO:Importing untrained model
2022-11-07 16:15:03,242:INFO:Passive Aggressive Regressor Imported successfully
2022-11-07 16:15:03,261:INFO:Starting cross validation
2022-11-07 16:15:03,263:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-11-07 16:15:03,602:INFO:Calculating mean and std
2022-11-07 16:15:03,605:INFO:Creating metrics dataframe
2022-11-07 16:15:03,617:INFO:Uploading results into container
2022-11-07 16:15:03,618:INFO:Uploading model into container now
2022-11-07 16:15:03,619:INFO:master_model_container: 9
2022-11-07 16:15:03,619:INFO:display_container: 2
2022-11-07 16:15:03,620:INFO:PassiveAggressiveRegressor(random_state=4769)
2022-11-07 16:15:03,620:INFO:create_model() successfully completed......................................
2022-11-07 16:15:03,751:INFO:SubProcess create_model() end ==================================
2022-11-07 16:15:03,751:INFO:Creating metrics dataframe
2022-11-07 16:15:03,771:INFO:Initializing Huber Regressor
2022-11-07 16:15:03,771:INFO:Total runtime is 0.23208903868993125 minutes
2022-11-07 16:15:03,782:INFO:SubProcess create_model() called ==================================
2022-11-07 16:15:03,783:INFO:Initializing create_model()
2022-11-07 16:15:03,785:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f20cc3726d0>, estimator=huber, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f20c2259610>, model_only=True, return_train_score=False, kwargs={})
2022-11-07 16:15:03,786:INFO:Checking exceptions
2022-11-07 16:15:03,788:INFO:Importing libraries
2022-11-07 16:15:03,789:INFO:Copying training dataset
2022-11-07 16:15:03,796:INFO:Defining folds
2022-11-07 16:15:03,798:INFO:Declaring metric variables
2022-11-07 16:15:03,808:INFO:Importing untrained model
2022-11-07 16:15:03,815:INFO:Huber Regressor Imported successfully
2022-11-07 16:15:03,836:INFO:Starting cross validation
2022-11-07 16:15:03,841:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-11-07 16:15:03,935:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2022-11-07 16:15:04,007:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2022-11-07 16:15:04,064:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2022-11-07 16:15:04,158:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2022-11-07 16:15:04,171:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2022-11-07 16:15:04,293:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2022-11-07 16:15:04,305:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2022-11-07 16:15:04,392:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2022-11-07 16:15:04,397:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2022-11-07 16:15:04,461:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2022-11-07 16:15:04,475:INFO:Calculating mean and std
2022-11-07 16:15:04,478:INFO:Creating metrics dataframe
2022-11-07 16:15:04,495:INFO:Uploading results into container
2022-11-07 16:15:04,496:INFO:Uploading model into container now
2022-11-07 16:15:04,497:INFO:master_model_container: 10
2022-11-07 16:15:04,497:INFO:display_container: 2
2022-11-07 16:15:04,497:INFO:HuberRegressor()
2022-11-07 16:15:04,498:INFO:create_model() successfully completed......................................
2022-11-07 16:15:04,628:INFO:SubProcess create_model() end ==================================
2022-11-07 16:15:04,629:INFO:Creating metrics dataframe
2022-11-07 16:15:04,649:INFO:Initializing K Neighbors Regressor
2022-11-07 16:15:04,650:INFO:Total runtime is 0.2467259724934896 minutes
2022-11-07 16:15:04,659:INFO:SubProcess create_model() called ==================================
2022-11-07 16:15:04,660:INFO:Initializing create_model()
2022-11-07 16:15:04,663:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f20cc3726d0>, estimator=knn, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f20c2259610>, model_only=True, return_train_score=False, kwargs={})
2022-11-07 16:15:04,663:INFO:Checking exceptions
2022-11-07 16:15:04,666:INFO:Importing libraries
2022-11-07 16:15:04,667:INFO:Copying training dataset
2022-11-07 16:15:04,682:INFO:Defining folds
2022-11-07 16:15:04,683:INFO:Declaring metric variables
2022-11-07 16:15:04,692:INFO:Importing untrained model
2022-11-07 16:15:04,700:INFO:K Neighbors Regressor Imported successfully
2022-11-07 16:15:04,716:INFO:Starting cross validation
2022-11-07 16:15:04,719:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-11-07 16:15:05,531:INFO:Calculating mean and std
2022-11-07 16:15:05,533:INFO:Creating metrics dataframe
2022-11-07 16:15:05,548:INFO:Uploading results into container
2022-11-07 16:15:05,549:INFO:Uploading model into container now
2022-11-07 16:15:05,550:INFO:master_model_container: 11
2022-11-07 16:15:05,550:INFO:display_container: 2
2022-11-07 16:15:05,550:INFO:KNeighborsRegressor(n_jobs=-1)
2022-11-07 16:15:05,550:INFO:create_model() successfully completed......................................
2022-11-07 16:15:05,684:INFO:SubProcess create_model() end ==================================
2022-11-07 16:15:05,684:INFO:Creating metrics dataframe
2022-11-07 16:15:05,705:INFO:Initializing Decision Tree Regressor
2022-11-07 16:15:05,706:INFO:Total runtime is 0.2643327077229818 minutes
2022-11-07 16:15:05,715:INFO:SubProcess create_model() called ==================================
2022-11-07 16:15:05,716:INFO:Initializing create_model()
2022-11-07 16:15:05,716:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f20cc3726d0>, estimator=dt, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f20c2259610>, model_only=True, return_train_score=False, kwargs={})
2022-11-07 16:15:05,716:INFO:Checking exceptions
2022-11-07 16:15:05,720:INFO:Importing libraries
2022-11-07 16:15:05,720:INFO:Copying training dataset
2022-11-07 16:15:05,725:INFO:Defining folds
2022-11-07 16:15:05,726:INFO:Declaring metric variables
2022-11-07 16:15:05,735:INFO:Importing untrained model
2022-11-07 16:15:05,748:INFO:Decision Tree Regressor Imported successfully
2022-11-07 16:15:05,764:INFO:Starting cross validation
2022-11-07 16:15:05,768:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-11-07 16:15:06,124:INFO:Calculating mean and std
2022-11-07 16:15:06,128:INFO:Creating metrics dataframe
2022-11-07 16:15:06,137:INFO:Uploading results into container
2022-11-07 16:15:06,138:INFO:Uploading model into container now
2022-11-07 16:15:06,139:INFO:master_model_container: 12
2022-11-07 16:15:06,139:INFO:display_container: 2
2022-11-07 16:15:06,140:INFO:DecisionTreeRegressor(random_state=4769)
2022-11-07 16:15:06,140:INFO:create_model() successfully completed......................................
2022-11-07 16:15:06,276:INFO:SubProcess create_model() end ==================================
2022-11-07 16:15:06,276:INFO:Creating metrics dataframe
2022-11-07 16:15:06,301:INFO:Initializing Random Forest Regressor
2022-11-07 16:15:06,302:INFO:Total runtime is 0.2742711027463277 minutes
2022-11-07 16:15:06,313:INFO:SubProcess create_model() called ==================================
2022-11-07 16:15:06,314:INFO:Initializing create_model()
2022-11-07 16:15:06,315:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f20cc3726d0>, estimator=rf, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f20c2259610>, model_only=True, return_train_score=False, kwargs={})
2022-11-07 16:15:06,315:INFO:Checking exceptions
2022-11-07 16:15:06,319:INFO:Importing libraries
2022-11-07 16:15:06,319:INFO:Copying training dataset
2022-11-07 16:15:06,324:INFO:Defining folds
2022-11-07 16:15:06,325:INFO:Declaring metric variables
2022-11-07 16:15:06,338:INFO:Importing untrained model
2022-11-07 16:15:06,345:INFO:Random Forest Regressor Imported successfully
2022-11-07 16:15:06,366:INFO:Starting cross validation
2022-11-07 16:15:06,369:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-11-07 16:15:09,893:INFO:Calculating mean and std
2022-11-07 16:15:09,895:INFO:Creating metrics dataframe
2022-11-07 16:15:09,911:INFO:Uploading results into container
2022-11-07 16:15:09,912:INFO:Uploading model into container now
2022-11-07 16:15:09,913:INFO:master_model_container: 13
2022-11-07 16:15:09,913:INFO:display_container: 2
2022-11-07 16:15:09,914:INFO:RandomForestRegressor(n_jobs=-1, random_state=4769)
2022-11-07 16:15:09,914:INFO:create_model() successfully completed......................................
2022-11-07 16:15:10,045:INFO:SubProcess create_model() end ==================================
2022-11-07 16:15:10,046:INFO:Creating metrics dataframe
2022-11-07 16:15:10,067:INFO:Initializing Extra Trees Regressor
2022-11-07 16:15:10,068:INFO:Total runtime is 0.3370339473088582 minutes
2022-11-07 16:15:10,077:INFO:SubProcess create_model() called ==================================
2022-11-07 16:15:10,078:INFO:Initializing create_model()
2022-11-07 16:15:10,079:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f20cc3726d0>, estimator=et, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f20c2259610>, model_only=True, return_train_score=False, kwargs={})
2022-11-07 16:15:10,079:INFO:Checking exceptions
2022-11-07 16:15:10,083:INFO:Importing libraries
2022-11-07 16:15:10,084:INFO:Copying training dataset
2022-11-07 16:15:10,088:INFO:Defining folds
2022-11-07 16:15:10,089:INFO:Declaring metric variables
2022-11-07 16:15:10,103:INFO:Importing untrained model
2022-11-07 16:15:10,112:INFO:Extra Trees Regressor Imported successfully
2022-11-07 16:15:10,129:INFO:Starting cross validation
2022-11-07 16:15:10,131:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-11-07 16:15:13,266:INFO:Calculating mean and std
2022-11-07 16:15:13,269:INFO:Creating metrics dataframe
2022-11-07 16:15:13,278:INFO:Uploading results into container
2022-11-07 16:15:13,283:INFO:Uploading model into container now
2022-11-07 16:15:13,284:INFO:master_model_container: 14
2022-11-07 16:15:13,284:INFO:display_container: 2
2022-11-07 16:15:13,284:INFO:ExtraTreesRegressor(n_jobs=-1, random_state=4769)
2022-11-07 16:15:13,285:INFO:create_model() successfully completed......................................
2022-11-07 16:15:13,417:INFO:SubProcess create_model() end ==================================
2022-11-07 16:15:13,417:INFO:Creating metrics dataframe
2022-11-07 16:15:13,444:INFO:Initializing AdaBoost Regressor
2022-11-07 16:15:13,448:INFO:Total runtime is 0.39336886803309123 minutes
2022-11-07 16:15:13,456:INFO:SubProcess create_model() called ==================================
2022-11-07 16:15:13,456:INFO:Initializing create_model()
2022-11-07 16:15:13,457:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f20cc3726d0>, estimator=ada, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f20c2259610>, model_only=True, return_train_score=False, kwargs={})
2022-11-07 16:15:13,457:INFO:Checking exceptions
2022-11-07 16:15:13,460:INFO:Importing libraries
2022-11-07 16:15:13,462:INFO:Copying training dataset
2022-11-07 16:15:13,467:INFO:Defining folds
2022-11-07 16:15:13,470:INFO:Declaring metric variables
2022-11-07 16:15:13,480:INFO:Importing untrained model
2022-11-07 16:15:13,491:INFO:AdaBoost Regressor Imported successfully
2022-11-07 16:15:13,508:INFO:Starting cross validation
2022-11-07 16:15:13,512:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-11-07 16:15:15,847:INFO:Calculating mean and std
2022-11-07 16:15:15,851:INFO:Creating metrics dataframe
2022-11-07 16:15:15,860:INFO:Uploading results into container
2022-11-07 16:15:15,861:INFO:Uploading model into container now
2022-11-07 16:15:15,862:INFO:master_model_container: 15
2022-11-07 16:15:15,862:INFO:display_container: 2
2022-11-07 16:15:15,863:INFO:AdaBoostRegressor(random_state=4769)
2022-11-07 16:15:15,863:INFO:create_model() successfully completed......................................
2022-11-07 16:15:15,993:INFO:SubProcess create_model() end ==================================
2022-11-07 16:15:15,994:INFO:Creating metrics dataframe
2022-11-07 16:15:16,021:INFO:Initializing Gradient Boosting Regressor
2022-11-07 16:15:16,026:INFO:Total runtime is 0.43633695046106974 minutes
2022-11-07 16:15:16,035:INFO:SubProcess create_model() called ==================================
2022-11-07 16:15:16,036:INFO:Initializing create_model()
2022-11-07 16:15:16,037:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f20cc3726d0>, estimator=gbr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f20c2259610>, model_only=True, return_train_score=False, kwargs={})
2022-11-07 16:15:16,037:INFO:Checking exceptions
2022-11-07 16:15:16,040:INFO:Importing libraries
2022-11-07 16:15:16,040:INFO:Copying training dataset
2022-11-07 16:15:16,047:INFO:Defining folds
2022-11-07 16:15:16,047:INFO:Declaring metric variables
2022-11-07 16:15:16,058:INFO:Importing untrained model
2022-11-07 16:15:16,066:INFO:Gradient Boosting Regressor Imported successfully
2022-11-07 16:15:16,082:INFO:Starting cross validation
2022-11-07 16:15:16,084:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-11-07 16:15:17,788:INFO:Calculating mean and std
2022-11-07 16:15:17,790:INFO:Creating metrics dataframe
2022-11-07 16:15:17,801:INFO:Uploading results into container
2022-11-07 16:15:17,802:INFO:Uploading model into container now
2022-11-07 16:15:17,803:INFO:master_model_container: 16
2022-11-07 16:15:17,803:INFO:display_container: 2
2022-11-07 16:15:17,804:INFO:GradientBoostingRegressor(random_state=4769)
2022-11-07 16:15:17,804:INFO:create_model() successfully completed......................................
2022-11-07 16:15:17,936:INFO:SubProcess create_model() end ==================================
2022-11-07 16:15:17,937:INFO:Creating metrics dataframe
2022-11-07 16:15:17,964:INFO:Initializing Light Gradient Boosting Machine
2022-11-07 16:15:17,966:INFO:Total runtime is 0.4686692992846171 minutes
2022-11-07 16:15:17,974:INFO:SubProcess create_model() called ==================================
2022-11-07 16:15:17,975:INFO:Initializing create_model()
2022-11-07 16:15:17,980:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f20cc3726d0>, estimator=lightgbm, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f20c2259610>, model_only=True, return_train_score=False, kwargs={})
2022-11-07 16:15:17,980:INFO:Checking exceptions
2022-11-07 16:15:17,983:INFO:Importing libraries
2022-11-07 16:15:17,983:INFO:Copying training dataset
2022-11-07 16:15:17,987:INFO:Defining folds
2022-11-07 16:15:17,990:INFO:Declaring metric variables
2022-11-07 16:15:17,999:INFO:Importing untrained model
2022-11-07 16:15:18,008:INFO:Light Gradient Boosting Machine Imported successfully
2022-11-07 16:15:18,026:INFO:Starting cross validation
2022-11-07 16:15:18,028:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-11-07 16:15:19,246:INFO:Calculating mean and std
2022-11-07 16:15:19,248:INFO:Creating metrics dataframe
2022-11-07 16:15:19,258:INFO:Uploading results into container
2022-11-07 16:15:19,258:INFO:Uploading model into container now
2022-11-07 16:15:19,259:INFO:master_model_container: 17
2022-11-07 16:15:19,259:INFO:display_container: 2
2022-11-07 16:15:19,260:INFO:LGBMRegressor(random_state=4769)
2022-11-07 16:15:19,260:INFO:create_model() successfully completed......................................
2022-11-07 16:15:19,395:INFO:SubProcess create_model() end ==================================
2022-11-07 16:15:19,395:INFO:Creating metrics dataframe
2022-11-07 16:15:19,420:INFO:Initializing Dummy Regressor
2022-11-07 16:15:19,421:INFO:Total runtime is 0.4929132143656413 minutes
2022-11-07 16:15:19,431:INFO:SubProcess create_model() called ==================================
2022-11-07 16:15:19,432:INFO:Initializing create_model()
2022-11-07 16:15:19,432:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f20cc3726d0>, estimator=dummy, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f20c2259610>, model_only=True, return_train_score=False, kwargs={})
2022-11-07 16:15:19,432:INFO:Checking exceptions
2022-11-07 16:15:19,435:INFO:Importing libraries
2022-11-07 16:15:19,435:INFO:Copying training dataset
2022-11-07 16:15:19,442:INFO:Defining folds
2022-11-07 16:15:19,443:INFO:Declaring metric variables
2022-11-07 16:15:19,457:INFO:Importing untrained model
2022-11-07 16:15:19,466:INFO:Dummy Regressor Imported successfully
2022-11-07 16:15:19,486:INFO:Starting cross validation
2022-11-07 16:15:19,490:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-11-07 16:15:19,806:INFO:Calculating mean and std
2022-11-07 16:15:19,809:INFO:Creating metrics dataframe
2022-11-07 16:15:19,822:INFO:Uploading results into container
2022-11-07 16:15:19,823:INFO:Uploading model into container now
2022-11-07 16:15:19,824:INFO:master_model_container: 18
2022-11-07 16:15:19,824:INFO:display_container: 2
2022-11-07 16:15:19,825:INFO:DummyRegressor()
2022-11-07 16:15:19,825:INFO:create_model() successfully completed......................................
2022-11-07 16:15:19,958:INFO:SubProcess create_model() end ==================================
2022-11-07 16:15:19,959:INFO:Creating metrics dataframe
2022-11-07 16:15:20,013:INFO:Initializing create_model()
2022-11-07 16:15:20,014:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f20cc3726d0>, estimator=ExtraTreesRegressor(n_jobs=-1, random_state=4769), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2022-11-07 16:15:20,017:INFO:Checking exceptions
2022-11-07 16:15:20,023:INFO:Importing libraries
2022-11-07 16:15:20,024:INFO:Copying training dataset
2022-11-07 16:15:20,027:INFO:Defining folds
2022-11-07 16:15:20,027:INFO:Declaring metric variables
2022-11-07 16:15:20,028:INFO:Importing untrained model
2022-11-07 16:15:20,028:INFO:Declaring custom model
2022-11-07 16:15:20,029:INFO:Extra Trees Regressor Imported successfully
2022-11-07 16:15:20,031:INFO:Cross validation set to False
2022-11-07 16:15:20,031:INFO:Fitting Model
2022-11-07 16:15:20,334:INFO:ExtraTreesRegressor(n_jobs=-1, random_state=4769)
2022-11-07 16:15:20,334:INFO:create_model() successfully completed......................................
2022-11-07 16:15:20,557:INFO:master_model_container: 18
2022-11-07 16:15:20,557:INFO:display_container: 2
2022-11-07 16:15:20,558:INFO:ExtraTreesRegressor(n_jobs=-1, random_state=4769)
2022-11-07 16:15:20,559:INFO:compare_models() successfully completed......................................
2022-11-07 16:15:35,710:INFO:Initializing plot_model()
2022-11-07 16:15:35,712:INFO:plot_model(plot=residuals, fold=None, use_train_data=False, verbose=True, display=None, display_format=None, estimator=ExtraTreesRegressor(n_jobs=-1, random_state=4769), feature_name=None, fit_kwargs=None, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x7f20cc3726d0>, system=True)
2022-11-07 16:15:35,712:INFO:Checking exceptions
2022-11-07 16:15:35,821:INFO:Preloading libraries
2022-11-07 16:15:35,840:INFO:Copying training dataset
2022-11-07 16:15:35,841:INFO:Plot type: residuals
2022-11-07 16:15:55,354:INFO:Initializing evaluate_model()
2022-11-07 16:15:55,354:INFO:evaluate_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f20cc3726d0>, estimator=ExtraTreesRegressor(n_jobs=-1, random_state=4769), fold=None, fit_kwargs=None, plot_kwargs=None, feature_name=None, groups=None, use_train_data=False)
2022-11-07 16:15:55,406:INFO:Initializing plot_model()
2022-11-07 16:15:55,407:INFO:plot_model(plot=pipeline, fold=KFold(n_splits=10, random_state=None, shuffle=False), use_train_data=False, verbose=False, display=None, display_format=None, estimator=ExtraTreesRegressor(n_jobs=-1, random_state=4769), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x7f20cc3726d0>, system=True)
2022-11-07 16:15:55,407:INFO:Checking exceptions
2022-11-07 16:15:55,512:INFO:Preloading libraries
2022-11-07 16:15:55,522:INFO:Copying training dataset
2022-11-07 16:15:55,523:INFO:Plot type: pipeline
2022-11-07 16:16:48,285:INFO:PyCaret RegressionExperiment
2022-11-07 16:16:48,286:INFO:Logging name: reg-default-name
2022-11-07 16:16:48,286:INFO:ML Usecase: MLUsecase.REGRESSION
2022-11-07 16:16:48,286:INFO:version 3.0.0.rc4
2022-11-07 16:16:48,286:INFO:Initializing setup()
2022-11-07 16:16:48,286:INFO:self.USI: 607b
2022-11-07 16:16:48,286:INFO:self.variable_keys: {'display_container', 'gpu_param', '_all_models', 'log_plots_param', 'fold_shuffle_param', 'fold_groups_param', 'memory', 'data', 'y_test', 'target_param', 'fold_generator', 'html_param', '_gpu_n_jobs_param', 'exp_id', 'logging_param', '_all_models_internal', 'n_jobs_param', 'variable_keys', 'idx', 'transform_target_method_param', 'exp_name_log', 'y', 'X', '_ml_usecase', 'seed', 'X_test', 'y_train', 'pipeline', '_available_plots', 'USI', 'master_model_container', 'X_train', 'transform_target_param', '_all_metrics'}
2022-11-07 16:16:48,286:INFO:Checking environment
2022-11-07 16:16:48,287:INFO:python_version: 3.7.15
2022-11-07 16:16:48,287:INFO:python_build: ('default', 'Oct 12 2022 19:14:55')
2022-11-07 16:16:48,287:INFO:machine: x86_64
2022-11-07 16:16:48,287:INFO:platform: Linux-5.10.133+-x86_64-with-Ubuntu-18.04-bionic
2022-11-07 16:16:48,287:INFO:Memory: svmem(total=13616353280, available=11643867136, percent=14.5, used=1803685888, free=6580043776, active=710119424, inactive=5952065536, buffers=420372480, cached=4812251136, shared=1339392, slab=281858048)
2022-11-07 16:16:48,288:INFO:Physical Core: 1
2022-11-07 16:16:48,288:INFO:Logical Core: 2
2022-11-07 16:16:48,288:INFO:Checking libraries
2022-11-07 16:16:48,289:INFO:System:
2022-11-07 16:16:48,289:INFO:    python: 3.7.15 (default, Oct 12 2022, 19:14:55)  [GCC 7.5.0]
2022-11-07 16:16:48,289:INFO:executable: /usr/bin/python3
2022-11-07 16:16:48,289:INFO:   machine: Linux-5.10.133+-x86_64-with-Ubuntu-18.04-bionic
2022-11-07 16:16:48,289:INFO:PyCaret required dependencies:
2022-11-07 16:16:48,289:INFO:                 pip: 21.1.3
2022-11-07 16:16:48,290:INFO:          setuptools: 57.4.0
2022-11-07 16:16:48,290:INFO:             pycaret: 3.0.0rc4
2022-11-07 16:16:48,290:INFO:             IPython: 7.9.0
2022-11-07 16:16:48,290:INFO:          ipywidgets: 7.7.1
2022-11-07 16:16:48,290:INFO:                tqdm: 4.64.1
2022-11-07 16:16:48,290:INFO:               numpy: 1.21.6
2022-11-07 16:16:48,291:INFO:              pandas: 1.3.5
2022-11-07 16:16:48,291:INFO:              jinja2: 2.11.3
2022-11-07 16:16:48,291:INFO:               scipy: 1.7.3
2022-11-07 16:16:48,291:INFO:              joblib: 1.2.0
2022-11-07 16:16:48,291:INFO:             sklearn: 1.0.2
2022-11-07 16:16:48,291:INFO:                pyod: 1.0.6
2022-11-07 16:16:48,293:INFO:            imblearn: 0.8.1
2022-11-07 16:16:48,293:INFO:   category_encoders: 2.5.1.post0
2022-11-07 16:16:48,293:INFO:            lightgbm: 3.3.3
2022-11-07 16:16:48,293:INFO:               numba: 0.55.2
2022-11-07 16:16:48,294:INFO:            requests: 2.28.1
2022-11-07 16:16:48,294:INFO:          matplotlib: 3.5.3
2022-11-07 16:16:48,294:INFO:          scikitplot: 0.3.7
2022-11-07 16:16:48,294:INFO:         yellowbrick: 1.5
2022-11-07 16:16:48,294:INFO:              plotly: 5.5.0
2022-11-07 16:16:48,294:INFO:             kaleido: 0.2.1
2022-11-07 16:16:48,294:INFO:         statsmodels: 0.12.2
2022-11-07 16:16:48,294:INFO:              sktime: 0.13.4
2022-11-07 16:16:48,294:INFO:               tbats: 1.1.1
2022-11-07 16:16:48,294:INFO:            pmdarima: 1.8.5
2022-11-07 16:16:48,295:INFO:              psutil: 5.9.3
2022-11-07 16:16:48,295:INFO:PyCaret optional dependencies:
2022-11-07 16:16:48,295:INFO:                shap: Not installed
2022-11-07 16:16:48,295:INFO:           interpret: Not installed
2022-11-07 16:16:48,295:INFO:                umap: Not installed
2022-11-07 16:16:48,295:INFO:    pandas_profiling: 1.4.1
2022-11-07 16:16:48,295:INFO:  explainerdashboard: Not installed
2022-11-07 16:16:48,296:INFO:             autoviz: Not installed
2022-11-07 16:16:48,296:INFO:           fairlearn: Not installed
2022-11-07 16:16:48,296:INFO:             xgboost: 0.90
2022-11-07 16:16:48,296:INFO:            catboost: Not installed
2022-11-07 16:16:48,296:INFO:              kmodes: Not installed
2022-11-07 16:16:48,296:INFO:             mlxtend: 0.14.0
2022-11-07 16:16:48,296:INFO:       statsforecast: Not installed
2022-11-07 16:16:48,297:INFO:        tune_sklearn: Not installed
2022-11-07 16:16:48,300:INFO:                 ray: Not installed
2022-11-07 16:16:48,300:INFO:            hyperopt: 0.1.2
2022-11-07 16:16:48,302:INFO:              optuna: Not installed
2022-11-07 16:16:48,302:INFO:               skopt: Not installed
2022-11-07 16:16:48,302:INFO:              mlflow: Not installed
2022-11-07 16:16:48,302:INFO:              gradio: Not installed
2022-11-07 16:16:48,302:INFO:             fastapi: Not installed
2022-11-07 16:16:48,303:INFO:             uvicorn: Not installed
2022-11-07 16:16:48,303:INFO:              m2cgen: Not installed
2022-11-07 16:16:48,303:INFO:           evidently: Not installed
2022-11-07 16:16:48,303:INFO:                nltk: 3.7
2022-11-07 16:16:48,303:INFO:            pyLDAvis: Not installed
2022-11-07 16:16:48,303:INFO:              gensim: 3.6.0
2022-11-07 16:16:48,303:INFO:               spacy: 3.4.2
2022-11-07 16:16:48,303:INFO:           wordcloud: 1.8.2.2
2022-11-07 16:16:48,303:INFO:            textblob: 0.15.3
2022-11-07 16:16:48,304:INFO:               fugue: Not installed
2022-11-07 16:16:48,304:INFO:           streamlit: Not installed
2022-11-07 16:16:48,304:INFO:             prophet: 1.1.1
2022-11-07 16:16:48,304:INFO:None
2022-11-07 16:16:48,304:INFO:Set up data.
2022-11-07 16:16:56,984:INFO:PyCaret RegressionExperiment
2022-11-07 16:16:56,984:INFO:Logging name: reg-default-name
2022-11-07 16:16:56,985:INFO:ML Usecase: MLUsecase.REGRESSION
2022-11-07 16:16:56,985:INFO:version 3.0.0.rc4
2022-11-07 16:16:56,985:INFO:Initializing setup()
2022-11-07 16:16:56,985:INFO:self.USI: 05ad
2022-11-07 16:16:56,985:INFO:self.variable_keys: {'display_container', 'gpu_param', '_all_models', 'log_plots_param', 'fold_shuffle_param', 'fold_groups_param', 'memory', 'data', 'y_test', 'target_param', 'fold_generator', 'html_param', '_gpu_n_jobs_param', 'exp_id', 'logging_param', '_all_models_internal', 'n_jobs_param', 'variable_keys', 'idx', 'transform_target_method_param', 'exp_name_log', 'y', 'X', '_ml_usecase', 'seed', 'X_test', 'y_train', 'pipeline', '_available_plots', 'USI', 'master_model_container', 'X_train', 'transform_target_param', '_all_metrics'}
2022-11-07 16:16:56,985:INFO:Checking environment
2022-11-07 16:16:56,985:INFO:python_version: 3.7.15
2022-11-07 16:16:56,986:INFO:python_build: ('default', 'Oct 12 2022 19:14:55')
2022-11-07 16:16:56,986:INFO:machine: x86_64
2022-11-07 16:16:56,986:INFO:platform: Linux-5.10.133+-x86_64-with-Ubuntu-18.04-bionic
2022-11-07 16:16:56,986:INFO:Memory: svmem(total=13616353280, available=11631624192, percent=14.6, used=1816899584, free=6566592512, active=710160384, inactive=5965279232, buffers=420397056, cached=4812464128, shared=1339392, slab=281874432)
2022-11-07 16:16:56,987:INFO:Physical Core: 1
2022-11-07 16:16:56,987:INFO:Logical Core: 2
2022-11-07 16:16:56,987:INFO:Checking libraries
2022-11-07 16:16:56,987:INFO:System:
2022-11-07 16:16:56,987:INFO:    python: 3.7.15 (default, Oct 12 2022, 19:14:55)  [GCC 7.5.0]
2022-11-07 16:16:56,987:INFO:executable: /usr/bin/python3
2022-11-07 16:16:56,988:INFO:   machine: Linux-5.10.133+-x86_64-with-Ubuntu-18.04-bionic
2022-11-07 16:16:56,988:INFO:PyCaret required dependencies:
2022-11-07 16:16:56,988:INFO:                 pip: 21.1.3
2022-11-07 16:16:56,988:INFO:          setuptools: 57.4.0
2022-11-07 16:16:56,988:INFO:             pycaret: 3.0.0rc4
2022-11-07 16:16:56,988:INFO:             IPython: 7.9.0
2022-11-07 16:16:56,988:INFO:          ipywidgets: 7.7.1
2022-11-07 16:16:56,989:INFO:                tqdm: 4.64.1
2022-11-07 16:16:56,989:INFO:               numpy: 1.21.6
2022-11-07 16:16:56,989:INFO:              pandas: 1.3.5
2022-11-07 16:16:56,989:INFO:              jinja2: 2.11.3
2022-11-07 16:16:56,989:INFO:               scipy: 1.7.3
2022-11-07 16:16:56,989:INFO:              joblib: 1.2.0
2022-11-07 16:16:56,989:INFO:             sklearn: 1.0.2
2022-11-07 16:16:56,989:INFO:                pyod: 1.0.6
2022-11-07 16:16:56,989:INFO:            imblearn: 0.8.1
2022-11-07 16:16:56,989:INFO:   category_encoders: 2.5.1.post0
2022-11-07 16:16:56,989:INFO:            lightgbm: 3.3.3
2022-11-07 16:16:56,989:INFO:               numba: 0.55.2
2022-11-07 16:16:56,989:INFO:            requests: 2.28.1
2022-11-07 16:16:56,989:INFO:          matplotlib: 3.5.3
2022-11-07 16:16:56,990:INFO:          scikitplot: 0.3.7
2022-11-07 16:16:56,990:INFO:         yellowbrick: 1.5
2022-11-07 16:16:56,990:INFO:              plotly: 5.5.0
2022-11-07 16:16:56,990:INFO:             kaleido: 0.2.1
2022-11-07 16:16:56,990:INFO:         statsmodels: 0.12.2
2022-11-07 16:16:56,990:INFO:              sktime: 0.13.4
2022-11-07 16:16:56,990:INFO:               tbats: 1.1.1
2022-11-07 16:16:56,990:INFO:            pmdarima: 1.8.5
2022-11-07 16:16:56,990:INFO:              psutil: 5.9.3
2022-11-07 16:16:56,990:INFO:PyCaret optional dependencies:
2022-11-07 16:16:56,990:INFO:                shap: Not installed
2022-11-07 16:16:56,990:INFO:           interpret: Not installed
2022-11-07 16:16:56,990:INFO:                umap: Not installed
2022-11-07 16:16:56,991:INFO:    pandas_profiling: 1.4.1
2022-11-07 16:16:56,991:INFO:  explainerdashboard: Not installed
2022-11-07 16:16:56,991:INFO:             autoviz: Not installed
2022-11-07 16:16:56,991:INFO:           fairlearn: Not installed
2022-11-07 16:16:56,991:INFO:             xgboost: 0.90
2022-11-07 16:16:56,991:INFO:            catboost: Not installed
2022-11-07 16:16:56,991:INFO:              kmodes: Not installed
2022-11-07 16:16:56,991:INFO:             mlxtend: 0.14.0
2022-11-07 16:16:56,991:INFO:       statsforecast: Not installed
2022-11-07 16:16:56,991:INFO:        tune_sklearn: Not installed
2022-11-07 16:16:56,991:INFO:                 ray: Not installed
2022-11-07 16:16:56,991:INFO:            hyperopt: 0.1.2
2022-11-07 16:16:56,991:INFO:              optuna: Not installed
2022-11-07 16:16:56,992:INFO:               skopt: Not installed
2022-11-07 16:16:56,992:INFO:              mlflow: Not installed
2022-11-07 16:16:56,992:INFO:              gradio: Not installed
2022-11-07 16:16:56,992:INFO:             fastapi: Not installed
2022-11-07 16:16:56,992:INFO:             uvicorn: Not installed
2022-11-07 16:16:56,992:INFO:              m2cgen: Not installed
2022-11-07 16:16:56,992:INFO:           evidently: Not installed
2022-11-07 16:16:56,992:INFO:                nltk: 3.7
2022-11-07 16:16:56,992:INFO:            pyLDAvis: Not installed
2022-11-07 16:16:56,992:INFO:              gensim: 3.6.0
2022-11-07 16:16:56,992:INFO:               spacy: 3.4.2
2022-11-07 16:16:56,992:INFO:           wordcloud: 1.8.2.2
2022-11-07 16:16:56,992:INFO:            textblob: 0.15.3
2022-11-07 16:16:56,992:INFO:               fugue: Not installed
2022-11-07 16:16:56,992:INFO:           streamlit: Not installed
2022-11-07 16:16:56,993:INFO:             prophet: 1.1.1
2022-11-07 16:16:56,993:INFO:None
2022-11-07 16:16:56,993:INFO:Set up data.
2022-11-07 16:16:57,017:INFO:Set up train/test split.
2022-11-07 16:16:57,024:INFO:Set up index.
2022-11-07 16:16:57,025:INFO:Set up folding strategy.
2022-11-07 16:16:57,025:INFO:Assigning column types.
2022-11-07 16:16:57,032:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2022-11-07 16:16:57,033:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2022-11-07 16:16:57,038:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2022-11-07 16:16:57,044:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2022-11-07 16:16:57,115:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2022-11-07 16:16:57,169:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2022-11-07 16:16:57,170:INFO:Soft dependency imported: xgboost: 0.90
2022-11-07 16:16:57,170:WARNING:Wrong xgboost version. Expected xgboost>=1.1.0, got xgboost==0.90
2022-11-07 16:16:57,171:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-11-07 16:16:57,171:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2022-11-07 16:16:57,177:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2022-11-07 16:16:57,183:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2022-11-07 16:16:57,256:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2022-11-07 16:16:57,320:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2022-11-07 16:16:57,321:INFO:Soft dependency imported: xgboost: 0.90
2022-11-07 16:16:57,322:WARNING:Wrong xgboost version. Expected xgboost>=1.1.0, got xgboost==0.90
2022-11-07 16:16:57,322:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-11-07 16:16:57,323:INFO:Engine successfully changes for model 'lasso' to 'sklearn'.
2022-11-07 16:16:57,328:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2022-11-07 16:16:57,334:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2022-11-07 16:16:57,405:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2022-11-07 16:16:57,459:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2022-11-07 16:16:57,461:INFO:Soft dependency imported: xgboost: 0.90
2022-11-07 16:16:57,461:WARNING:Wrong xgboost version. Expected xgboost>=1.1.0, got xgboost==0.90
2022-11-07 16:16:57,461:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-11-07 16:16:57,471:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2022-11-07 16:16:57,481:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2022-11-07 16:16:57,567:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2022-11-07 16:16:57,625:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2022-11-07 16:16:57,626:INFO:Soft dependency imported: xgboost: 0.90
2022-11-07 16:16:57,627:WARNING:Wrong xgboost version. Expected xgboost>=1.1.0, got xgboost==0.90
2022-11-07 16:16:57,627:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-11-07 16:16:57,627:INFO:Engine successfully changes for model 'ridge' to 'sklearn'.
2022-11-07 16:16:57,638:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2022-11-07 16:16:57,709:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2022-11-07 16:16:57,773:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2022-11-07 16:16:57,774:INFO:Soft dependency imported: xgboost: 0.90
2022-11-07 16:16:57,774:WARNING:Wrong xgboost version. Expected xgboost>=1.1.0, got xgboost==0.90
2022-11-07 16:16:57,775:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-11-07 16:16:57,786:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2022-11-07 16:16:57,860:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2022-11-07 16:16:57,916:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2022-11-07 16:16:57,917:INFO:Soft dependency imported: xgboost: 0.90
2022-11-07 16:16:57,918:WARNING:Wrong xgboost version. Expected xgboost>=1.1.0, got xgboost==0.90
2022-11-07 16:16:57,918:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-11-07 16:16:57,919:INFO:Engine successfully changes for model 'en' to 'sklearn'.
2022-11-07 16:16:58,001:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2022-11-07 16:16:58,059:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2022-11-07 16:16:58,060:INFO:Soft dependency imported: xgboost: 0.90
2022-11-07 16:16:58,060:WARNING:Wrong xgboost version. Expected xgboost>=1.1.0, got xgboost==0.90
2022-11-07 16:16:58,060:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-11-07 16:16:58,142:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2022-11-07 16:16:58,199:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2022-11-07 16:16:58,208:INFO:Soft dependency imported: xgboost: 0.90
2022-11-07 16:16:58,208:WARNING:Wrong xgboost version. Expected xgboost>=1.1.0, got xgboost==0.90
2022-11-07 16:16:58,209:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-11-07 16:16:58,209:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2022-11-07 16:16:58,299:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2022-11-07 16:16:58,355:INFO:Soft dependency imported: xgboost: 0.90
2022-11-07 16:16:58,355:WARNING:Wrong xgboost version. Expected xgboost>=1.1.0, got xgboost==0.90
2022-11-07 16:16:58,356:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-11-07 16:16:58,444:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2022-11-07 16:16:58,512:INFO:Soft dependency imported: xgboost: 0.90
2022-11-07 16:16:58,512:WARNING:Wrong xgboost version. Expected xgboost>=1.1.0, got xgboost==0.90
2022-11-07 16:16:58,513:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-11-07 16:16:58,513:INFO:Engine successfully changes for model 'svm' to 'sklearn'.
2022-11-07 16:16:58,646:INFO:Soft dependency imported: xgboost: 0.90
2022-11-07 16:16:58,647:WARNING:Wrong xgboost version. Expected xgboost>=1.1.0, got xgboost==0.90
2022-11-07 16:16:58,647:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-11-07 16:16:58,782:INFO:Soft dependency imported: xgboost: 0.90
2022-11-07 16:16:58,782:WARNING:Wrong xgboost version. Expected xgboost>=1.1.0, got xgboost==0.90
2022-11-07 16:16:58,782:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-11-07 16:16:58,783:INFO:Preparing preprocessing pipeline...
2022-11-07 16:16:58,785:INFO:Set up simple imputation.
2022-11-07 16:16:58,785:INFO:Set up variance threshold.
2022-11-07 16:16:58,959:INFO:Finished creating preprocessing pipeline.
2022-11-07 16:16:58,970:INFO:Pipeline: Pipeline(memory=Memory(location=/tmp/joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['1rd_red', ' 1rd_green',
                                             ' 1rd_blue', ' 1rd_r', ' 1rd_rg',
                                             ' 1rd_most_red', ' 1rd_least_red',
                                             ' 1rd_most_yellow', ' 1rd_size',
                                             ' 2nd_red', ' 2nd_green',
                                             ' 2nd_blue', ' 2rd_r', ' 2rd_rg',
                                             ' 2rd_most_red', ' 2rd_least_red',
                                             ' 2rd_most_yellow', ' 2nd_size',
                                             ' 3rd_red', ' 3rd_green',
                                             ' 3rd_bl...d_rg',
                                             ' 3rd_most_red', ' 3rd_least_red',
                                             ' 3rd_most_yellow', ' 3rd_size',
                                             ' 4th_red', ' 4th_green',
                                             ' 4th_blue', ...],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=[],
                                    transformer=SimpleImputer(fill_value='constant',
                                                              strategy='constant'))),
                ('low_variance',
                 TransformerWrapper(exclude=[],
                                    transformer=VarianceThreshold(threshold=0)))])
2022-11-07 16:16:58,970:INFO:Creating final display dataframe.
2022-11-07 16:16:59,550:INFO:Setup display_container:                Description             Value
0               Session id              2902
1                   Target         Sweetness
2              Target type        Regression
3               Data shape         (200, 56)
4         Train data shape         (139, 56)
5          Test data shape          (61, 56)
6         Numeric features                55
7               Preprocess              True
8          Imputation type            simple
9       Numeric imputation              mean
10  Categorical imputation          constant
11  Low variance threshold                 0
12          Fold Generator             KFold
13             Fold Number                10
14                CPU Jobs                -1
15                 Use GPU             False
16          Log Experiment             False
17         Experiment Name  reg-default-name
18                     USI              05ad
2022-11-07 16:16:59,704:INFO:Soft dependency imported: xgboost: 0.90
2022-11-07 16:16:59,704:WARNING:Wrong xgboost version. Expected xgboost>=1.1.0, got xgboost==0.90
2022-11-07 16:16:59,705:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-11-07 16:16:59,841:INFO:Soft dependency imported: xgboost: 0.90
2022-11-07 16:16:59,841:WARNING:Wrong xgboost version. Expected xgboost>=1.1.0, got xgboost==0.90
2022-11-07 16:16:59,842:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-11-07 16:16:59,850:INFO:setup() successfully completed in 2.88s...............
2022-11-07 16:17:08,650:INFO:Initializing compare_models()
2022-11-07 16:17:08,651:INFO:compare_models(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f20c2a2f390>, include=None, fold=None, round=4, cross_validation=True, sort=R2, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.regression.oop.RegressionExperiment object at 0x7f20c2a2f390>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'R2', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.regression.oop.RegressionExperiment'>}, exclude=None)
2022-11-07 16:17:08,651:INFO:Checking exceptions
2022-11-07 16:17:08,655:INFO:Preparing display monitor
2022-11-07 16:17:08,751:INFO:Initializing Linear Regression
2022-11-07 16:17:08,752:INFO:Total runtime is 9.711583455403646e-06 minutes
2022-11-07 16:17:08,763:INFO:SubProcess create_model() called ==================================
2022-11-07 16:17:08,764:INFO:Initializing create_model()
2022-11-07 16:17:08,765:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f20c2a2f390>, estimator=lr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f20c22c9dd0>, model_only=True, return_train_score=False, kwargs={})
2022-11-07 16:17:08,765:INFO:Checking exceptions
2022-11-07 16:17:08,768:INFO:Importing libraries
2022-11-07 16:17:08,768:INFO:Copying training dataset
2022-11-07 16:17:08,772:INFO:Defining folds
2022-11-07 16:17:08,773:INFO:Declaring metric variables
2022-11-07 16:17:08,781:INFO:Importing untrained model
2022-11-07 16:17:08,793:INFO:Linear Regression Imported successfully
2022-11-07 16:17:08,809:INFO:Starting cross validation
2022-11-07 16:17:08,814:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-11-07 16:17:14,392:INFO:Calculating mean and std
2022-11-07 16:17:14,394:INFO:Creating metrics dataframe
2022-11-07 16:17:14,408:INFO:Uploading results into container
2022-11-07 16:17:14,409:INFO:Uploading model into container now
2022-11-07 16:17:14,409:INFO:master_model_container: 1
2022-11-07 16:17:14,409:INFO:display_container: 2
2022-11-07 16:17:14,410:INFO:LinearRegression(n_jobs=-1)
2022-11-07 16:17:14,410:INFO:create_model() successfully completed......................................
2022-11-07 16:17:14,560:INFO:SubProcess create_model() end ==================================
2022-11-07 16:17:14,560:INFO:Creating metrics dataframe
2022-11-07 16:17:14,578:INFO:Initializing Lasso Regression
2022-11-07 16:17:14,579:INFO:Total runtime is 0.09712634086608887 minutes
2022-11-07 16:17:14,588:INFO:SubProcess create_model() called ==================================
2022-11-07 16:17:14,588:INFO:Initializing create_model()
2022-11-07 16:17:14,589:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f20c2a2f390>, estimator=lasso, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f20c22c9dd0>, model_only=True, return_train_score=False, kwargs={})
2022-11-07 16:17:14,589:INFO:Checking exceptions
2022-11-07 16:17:14,592:INFO:Importing libraries
2022-11-07 16:17:14,592:INFO:Copying training dataset
2022-11-07 16:17:14,597:INFO:Defining folds
2022-11-07 16:17:14,598:INFO:Declaring metric variables
2022-11-07 16:17:14,608:INFO:Importing untrained model
2022-11-07 16:17:14,617:INFO:Lasso Regression Imported successfully
2022-11-07 16:17:14,634:INFO:Starting cross validation
2022-11-07 16:17:14,636:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-11-07 16:17:14,740:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.895e-02, tolerance: 2.114e-02
  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive

2022-11-07 16:17:14,798:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.039e-01, tolerance: 2.094e-02
  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive

2022-11-07 16:17:14,823:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.633e-01, tolerance: 1.864e-02
  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive

2022-11-07 16:17:14,881:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.956e-01, tolerance: 2.130e-02
  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive

2022-11-07 16:17:14,937:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.425e-01, tolerance: 2.034e-02
  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive

2022-11-07 16:17:14,994:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.176e-01, tolerance: 1.999e-02
  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive

2022-11-07 16:17:15,040:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.409e-01, tolerance: 2.065e-02
  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive

2022-11-07 16:17:15,054:INFO:Calculating mean and std
2022-11-07 16:17:15,056:INFO:Creating metrics dataframe
2022-11-07 16:17:15,066:INFO:Uploading results into container
2022-11-07 16:17:15,068:INFO:Uploading model into container now
2022-11-07 16:17:15,069:INFO:master_model_container: 2
2022-11-07 16:17:15,069:INFO:display_container: 2
2022-11-07 16:17:15,069:INFO:Lasso(random_state=2902)
2022-11-07 16:17:15,070:INFO:create_model() successfully completed......................................
2022-11-07 16:17:15,208:INFO:SubProcess create_model() end ==================================
2022-11-07 16:17:15,208:INFO:Creating metrics dataframe
2022-11-07 16:17:15,229:INFO:Initializing Ridge Regression
2022-11-07 16:17:15,230:INFO:Total runtime is 0.10797646045684815 minutes
2022-11-07 16:17:15,238:INFO:SubProcess create_model() called ==================================
2022-11-07 16:17:15,238:INFO:Initializing create_model()
2022-11-07 16:17:15,239:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f20c2a2f390>, estimator=ridge, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f20c22c9dd0>, model_only=True, return_train_score=False, kwargs={})
2022-11-07 16:17:15,239:INFO:Checking exceptions
2022-11-07 16:17:15,242:INFO:Importing libraries
2022-11-07 16:17:15,242:INFO:Copying training dataset
2022-11-07 16:17:15,248:INFO:Defining folds
2022-11-07 16:17:15,249:INFO:Declaring metric variables
2022-11-07 16:17:15,258:INFO:Importing untrained model
2022-11-07 16:17:15,269:INFO:Ridge Regression Imported successfully
2022-11-07 16:17:15,285:INFO:Starting cross validation
2022-11-07 16:17:15,287:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-11-07 16:17:15,631:INFO:Calculating mean and std
2022-11-07 16:17:15,633:INFO:Creating metrics dataframe
2022-11-07 16:17:15,650:INFO:Uploading results into container
2022-11-07 16:17:15,651:INFO:Uploading model into container now
2022-11-07 16:17:15,651:INFO:master_model_container: 3
2022-11-07 16:17:15,651:INFO:display_container: 2
2022-11-07 16:17:15,652:INFO:Ridge(random_state=2902)
2022-11-07 16:17:15,652:INFO:create_model() successfully completed......................................
2022-11-07 16:17:15,790:INFO:SubProcess create_model() end ==================================
2022-11-07 16:17:15,790:INFO:Creating metrics dataframe
2022-11-07 16:17:15,809:INFO:Initializing Elastic Net
2022-11-07 16:17:15,810:INFO:Total runtime is 0.11765178044637045 minutes
2022-11-07 16:17:15,823:INFO:SubProcess create_model() called ==================================
2022-11-07 16:17:15,828:INFO:Initializing create_model()
2022-11-07 16:17:15,828:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f20c2a2f390>, estimator=en, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f20c22c9dd0>, model_only=True, return_train_score=False, kwargs={})
2022-11-07 16:17:15,829:INFO:Checking exceptions
2022-11-07 16:17:15,831:INFO:Importing libraries
2022-11-07 16:17:15,831:INFO:Copying training dataset
2022-11-07 16:17:15,835:INFO:Defining folds
2022-11-07 16:17:15,836:INFO:Declaring metric variables
2022-11-07 16:17:15,850:INFO:Importing untrained model
2022-11-07 16:17:15,858:INFO:Elastic Net Imported successfully
2022-11-07 16:17:15,875:INFO:Starting cross validation
2022-11-07 16:17:15,878:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-11-07 16:17:15,948:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.083e-02, tolerance: 1.911e-02
  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive

2022-11-07 16:17:16,005:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.904e-01, tolerance: 2.114e-02
  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive

2022-11-07 16:17:16,037:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.160e+00, tolerance: 1.864e-02
  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive

2022-11-07 16:17:16,095:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.112e+00, tolerance: 2.094e-02
  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive

2022-11-07 16:17:16,128:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.156e-01, tolerance: 2.130e-02
  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive

2022-11-07 16:17:16,170:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.072e-01, tolerance: 2.059e-02
  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive

2022-11-07 16:17:16,194:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.622e-01, tolerance: 2.034e-02
  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive

2022-11-07 16:17:16,229:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.580e-01, tolerance: 1.999e-02
  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive

2022-11-07 16:17:16,255:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.567e+00, tolerance: 2.067e-02
  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive

2022-11-07 16:17:16,285:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.649e-01, tolerance: 2.065e-02
  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive

2022-11-07 16:17:16,301:INFO:Calculating mean and std
2022-11-07 16:17:16,306:INFO:Creating metrics dataframe
2022-11-07 16:17:16,317:INFO:Uploading results into container
2022-11-07 16:17:16,318:INFO:Uploading model into container now
2022-11-07 16:17:16,318:INFO:master_model_container: 4
2022-11-07 16:17:16,319:INFO:display_container: 2
2022-11-07 16:17:16,319:INFO:ElasticNet(random_state=2902)
2022-11-07 16:17:16,320:INFO:create_model() successfully completed......................................
2022-11-07 16:17:16,465:INFO:SubProcess create_model() end ==================================
2022-11-07 16:17:16,467:INFO:Creating metrics dataframe
2022-11-07 16:17:16,500:INFO:Initializing Least Angle Regression
2022-11-07 16:17:16,501:INFO:Total runtime is 0.1291649142901103 minutes
2022-11-07 16:17:16,542:INFO:SubProcess create_model() called ==================================
2022-11-07 16:17:16,545:INFO:Initializing create_model()
2022-11-07 16:17:16,547:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f20c2a2f390>, estimator=lar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f20c22c9dd0>, model_only=True, return_train_score=False, kwargs={})
2022-11-07 16:17:16,548:INFO:Checking exceptions
2022-11-07 16:17:16,579:INFO:Importing libraries
2022-11-07 16:17:16,579:INFO:Copying training dataset
2022-11-07 16:17:16,597:INFO:Defining folds
2022-11-07 16:17:16,599:INFO:Declaring metric variables
2022-11-07 16:17:16,609:INFO:Importing untrained model
2022-11-07 16:17:16,617:INFO:Least Angle Regression Imported successfully
2022-11-07 16:17:16,635:INFO:Starting cross validation
2022-11-07 16:17:16,638:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-11-07 16:17:16,681:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2022-11-07 16:17:16,699:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 65 iterations, i.e. alpha=1.434e+02, with an active set of 52 regressors, and the smallest cholesky pivot element being 7.224e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:17:16,699:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 65 iterations, i.e. alpha=1.006e+02, with an active set of 52 regressors, and the smallest cholesky pivot element being 7.224e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:17:16,700:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 66 iterations, i.e. alpha=4.217e+00, with an active set of 53 regressors, and the smallest cholesky pivot element being 7.146e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:17:16,700:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 66 iterations, i.e. alpha=9.584e-01, with an active set of 53 regressors, and the smallest cholesky pivot element being 7.146e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:17:16,720:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2022-11-07 16:17:16,753:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 74 iterations, i.e. alpha=1.942e+00, with an active set of 49 regressors, and the smallest cholesky pivot element being 7.451e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:17:16,759:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2022-11-07 16:17:16,759:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 79 iterations, i.e. alpha=1.704e+00, with an active set of 51 regressors, and the smallest cholesky pivot element being 4.829e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:17:16,762:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 79 iterations, i.e. alpha=1.076e+00, with an active set of 51 regressors, and the smallest cholesky pivot element being 7.146e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:17:16,763:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 79 iterations, i.e. alpha=9.172e-01, with an active set of 51 regressors, and the smallest cholesky pivot element being 7.146e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:17:16,764:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 81 iterations, i.e. alpha=2.295e-01, with an active set of 52 regressors, and the smallest cholesky pivot element being 4.712e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:17:16,765:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 81 iterations, i.e. alpha=1.482e-01, with an active set of 52 regressors, and the smallest cholesky pivot element being 4.712e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:17:16,766:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 82 iterations, i.e. alpha=5.644e-02, with an active set of 53 regressors, and the smallest cholesky pivot element being 4.712e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:17:16,767:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 82 iterations, i.e. alpha=2.577e-03, with an active set of 53 regressors, and the smallest cholesky pivot element being 6.495e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:17:16,777:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 64 iterations, i.e. alpha=2.500e-02, with an active set of 47 regressors, and the smallest cholesky pivot element being 7.743e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:17:16,778:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 66 iterations, i.e. alpha=2.027e-02, with an active set of 48 regressors, and the smallest cholesky pivot element being 7.743e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:17:16,778:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 67 iterations, i.e. alpha=8.051e-03, with an active set of 49 regressors, and the smallest cholesky pivot element being 7.671e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:17:16,779:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 69 iterations, i.e. alpha=5.509e-03, with an active set of 50 regressors, and the smallest cholesky pivot element being 7.671e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:17:16,780:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 70 iterations, i.e. alpha=3.409e-03, with an active set of 51 regressors, and the smallest cholesky pivot element being 7.376e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:17:16,781:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 72 iterations, i.e. alpha=9.133e-04, with an active set of 52 regressors, and the smallest cholesky pivot element being 7.598e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:17:16,781:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 72 iterations, i.e. alpha=8.159e-04, with an active set of 52 regressors, and the smallest cholesky pivot element being 5.771e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:17:16,782:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 74 iterations, i.e. alpha=2.277e-04, with an active set of 53 regressors, and the smallest cholesky pivot element being 6.829e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:17:16,782:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 74 iterations, i.e. alpha=2.127e-04, with an active set of 53 regressors, and the smallest cholesky pivot element being 6.829e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:17:16,842:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2022-11-07 16:17:16,850:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2022-11-07 16:17:16,866:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 64 iterations, i.e. alpha=9.446e-01, with an active set of 47 regressors, and the smallest cholesky pivot element being 8.093e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:17:16,866:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 64 iterations, i.e. alpha=8.893e-01, with an active set of 47 regressors, and the smallest cholesky pivot element being 8.162e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:17:16,868:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 67 iterations, i.e. alpha=3.441e-01, with an active set of 49 regressors, and the smallest cholesky pivot element being 7.300e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:17:16,868:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 67 iterations, i.e. alpha=3.352e-01, with an active set of 49 regressors, and the smallest cholesky pivot element being 6.053e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:17:16,869:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 68 iterations, i.e. alpha=1.093e-01, with an active set of 50 regressors, and the smallest cholesky pivot element being 7.300e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:17:16,869:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 70 iterations, i.e. alpha=7.104e-02, with an active set of 51 regressors, and the smallest cholesky pivot element being 6.322e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:17:16,870:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 70 iterations, i.e. alpha=3.810e-02, with an active set of 51 regressors, and the smallest cholesky pivot element being 6.053e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:17:16,870:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 70 iterations, i.e. alpha=1.694e-02, with an active set of 51 regressors, and the smallest cholesky pivot element being 7.743e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:17:16,871:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 70 iterations, i.e. alpha=6.403e-03, with an active set of 51 regressors, and the smallest cholesky pivot element being 7.671e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:17:16,872:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 60 iterations, i.e. alpha=2.976e-02, with an active set of 47 regressors, and the smallest cholesky pivot element being 7.376e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:17:16,873:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 64 iterations, i.e. alpha=1.801e-02, with an active set of 49 regressors, and the smallest cholesky pivot element being 8.941e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:17:16,874:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 64 iterations, i.e. alpha=1.553e-02, with an active set of 49 regressors, and the smallest cholesky pivot element being 7.671e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:17:16,874:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 64 iterations, i.e. alpha=7.343e-03, with an active set of 49 regressors, and the smallest cholesky pivot element being 7.743e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:17:16,875:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 64 iterations, i.e. alpha=4.546e-03, with an active set of 49 regressors, and the smallest cholesky pivot element being 7.451e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:17:16,875:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 66 iterations, i.e. alpha=2.287e-03, with an active set of 50 regressors, and the smallest cholesky pivot element being 6.580e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:17:16,876:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 66 iterations, i.e. alpha=2.163e-03, with an active set of 50 regressors, and the smallest cholesky pivot element being 6.580e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:17:16,877:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 68 iterations, i.e. alpha=1.083e-03, with an active set of 51 regressors, and the smallest cholesky pivot element being 6.989e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:17:16,877:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 68 iterations, i.e. alpha=1.045e-03, with an active set of 51 regressors, and the smallest cholesky pivot element being 7.671e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:17:16,877:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 68 iterations, i.e. alpha=5.815e-04, with an active set of 51 regressors, and the smallest cholesky pivot element being 6.495e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:17:16,878:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 68 iterations, i.e. alpha=9.480e-05, with an active set of 51 regressors, and the smallest cholesky pivot element being 7.671e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:17:16,931:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2022-11-07 16:17:16,966:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2022-11-07 16:17:16,969:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 69 iterations, i.e. alpha=7.459e-02, with an active set of 51 regressors, and the smallest cholesky pivot element being 7.376e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:17:16,970:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 72 iterations, i.e. alpha=6.990e-02, with an active set of 52 regressors, and the smallest cholesky pivot element being 6.664e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:17:16,971:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 72 iterations, i.e. alpha=5.317e-02, with an active set of 52 regressors, and the smallest cholesky pivot element being 6.664e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:17:16,971:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 72 iterations, i.e. alpha=1.132e-02, with an active set of 52 regressors, and the smallest cholesky pivot element being 7.300e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:17:16,995:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 66 iterations, i.e. alpha=5.677e-02, with an active set of 48 regressors, and the smallest cholesky pivot element being 8.363e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:17:16,998:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 66 iterations, i.e. alpha=5.263e-02, with an active set of 48 regressors, and the smallest cholesky pivot element being 9.125e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:17:17,001:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 73 iterations, i.e. alpha=4.336e-02, with an active set of 51 regressors, and the smallest cholesky pivot element being 7.451e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:17:17,006:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 74 iterations, i.e. alpha=2.341e-02, with an active set of 52 regressors, and the smallest cholesky pivot element being 8.752e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:17:17,006:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 74 iterations, i.e. alpha=1.618e-02, with an active set of 52 regressors, and the smallest cholesky pivot element being 8.752e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:17:17,007:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 76 iterations, i.e. alpha=3.610e-04, with an active set of 53 regressors, and the smallest cholesky pivot element being 6.989e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:17:17,009:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 76 iterations, i.e. alpha=7.040e-05, with an active set of 53 regressors, and the smallest cholesky pivot element being 7.451e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:17:17,020:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2022-11-07 16:17:17,040:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 74 iterations, i.e. alpha=9.238e-02, with an active set of 52 regressors, and the smallest cholesky pivot element being 6.409e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:17:17,041:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 74 iterations, i.e. alpha=8.766e-02, with an active set of 52 regressors, and the smallest cholesky pivot element being 6.747e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:17:17,041:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 74 iterations, i.e. alpha=8.672e-02, with an active set of 52 regressors, and the smallest cholesky pivot element being 8.229e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:17:17,066:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2022-11-07 16:17:17,080:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 56 iterations, i.e. alpha=1.522e-02, with an active set of 44 regressors, and the smallest cholesky pivot element being 8.162e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:17:17,082:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 62 iterations, i.e. alpha=1.558e-02, with an active set of 45 regressors, and the smallest cholesky pivot element being 9.003e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:17:17,085:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 76 iterations, i.e. alpha=1.402e-01, with an active set of 49 regressors, and the smallest cholesky pivot element being 8.625e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:17:17,087:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 80 iterations, i.e. alpha=1.176e-01, with an active set of 51 regressors, and the smallest cholesky pivot element being 8.297e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:17:17,087:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 80 iterations, i.e. alpha=4.609e-02, with an active set of 51 regressors, and the smallest cholesky pivot element being 6.829e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:17:17,088:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 80 iterations, i.e. alpha=4.042e-02, with an active set of 51 regressors, and the smallest cholesky pivot element being 6.829e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:17:17,088:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 80 iterations, i.e. alpha=1.021e-04, with an active set of 51 regressors, and the smallest cholesky pivot element being 4.712e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:17:17,097:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2022-11-07 16:17:17,114:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 74 iterations, i.e. alpha=7.094e-01, with an active set of 50 regressors, and the smallest cholesky pivot element being 8.162e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:17:17,114:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 74 iterations, i.e. alpha=5.512e-01, with an active set of 50 regressors, and the smallest cholesky pivot element being 8.162e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:17:17,115:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 78 iterations, i.e. alpha=5.024e-01, with an active set of 52 regressors, and the smallest cholesky pivot element being 6.495e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:17:17,115:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 78 iterations, i.e. alpha=2.374e-01, with an active set of 52 regressors, and the smallest cholesky pivot element being 6.495e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:17:17,116:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 78 iterations, i.e. alpha=1.687e-01, with an active set of 52 regressors, and the smallest cholesky pivot element being 9.940e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:17:17,130:INFO:Calculating mean and std
2022-11-07 16:17:17,132:INFO:Creating metrics dataframe
2022-11-07 16:17:17,141:INFO:Uploading results into container
2022-11-07 16:17:17,142:INFO:Uploading model into container now
2022-11-07 16:17:17,143:INFO:master_model_container: 5
2022-11-07 16:17:17,143:INFO:display_container: 2
2022-11-07 16:17:17,144:INFO:Lars(random_state=2902)
2022-11-07 16:17:17,144:INFO:create_model() successfully completed......................................
2022-11-07 16:17:17,296:INFO:SubProcess create_model() end ==================================
2022-11-07 16:17:17,296:INFO:Creating metrics dataframe
2022-11-07 16:17:17,322:INFO:Initializing Lasso Least Angle Regression
2022-11-07 16:17:17,322:INFO:Total runtime is 0.1428543567657471 minutes
2022-11-07 16:17:17,335:INFO:SubProcess create_model() called ==================================
2022-11-07 16:17:17,336:INFO:Initializing create_model()
2022-11-07 16:17:17,336:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f20c2a2f390>, estimator=llar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f20c22c9dd0>, model_only=True, return_train_score=False, kwargs={})
2022-11-07 16:17:17,336:INFO:Checking exceptions
2022-11-07 16:17:17,339:INFO:Importing libraries
2022-11-07 16:17:17,340:INFO:Copying training dataset
2022-11-07 16:17:17,347:INFO:Defining folds
2022-11-07 16:17:17,347:INFO:Declaring metric variables
2022-11-07 16:17:17,357:INFO:Importing untrained model
2022-11-07 16:17:17,367:INFO:Lasso Least Angle Regression Imported successfully
2022-11-07 16:17:17,385:INFO:Starting cross validation
2022-11-07 16:17:17,387:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-11-07 16:17:17,430:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  FutureWarning,

2022-11-07 16:17:17,459:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  FutureWarning,

2022-11-07 16:17:17,496:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  FutureWarning,

2022-11-07 16:17:17,531:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  FutureWarning,

2022-11-07 16:17:17,575:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  FutureWarning,

2022-11-07 16:17:17,599:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  FutureWarning,

2022-11-07 16:17:17,641:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  FutureWarning,

2022-11-07 16:17:17,654:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  FutureWarning,

2022-11-07 16:17:17,689:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  FutureWarning,

2022-11-07 16:17:17,707:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  FutureWarning,

2022-11-07 16:17:17,723:INFO:Calculating mean and std
2022-11-07 16:17:17,726:INFO:Creating metrics dataframe
2022-11-07 16:17:17,735:INFO:Uploading results into container
2022-11-07 16:17:17,736:INFO:Uploading model into container now
2022-11-07 16:17:17,737:INFO:master_model_container: 6
2022-11-07 16:17:17,737:INFO:display_container: 2
2022-11-07 16:17:17,738:INFO:LassoLars(random_state=2902)
2022-11-07 16:17:17,738:INFO:create_model() successfully completed......................................
2022-11-07 16:17:17,876:INFO:SubProcess create_model() end ==================================
2022-11-07 16:17:17,877:INFO:Creating metrics dataframe
2022-11-07 16:17:17,896:INFO:Initializing Orthogonal Matching Pursuit
2022-11-07 16:17:17,897:INFO:Total runtime is 0.15242576599121097 minutes
2022-11-07 16:17:17,904:INFO:SubProcess create_model() called ==================================
2022-11-07 16:17:17,906:INFO:Initializing create_model()
2022-11-07 16:17:17,908:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f20c2a2f390>, estimator=omp, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f20c22c9dd0>, model_only=True, return_train_score=False, kwargs={})
2022-11-07 16:17:17,912:INFO:Checking exceptions
2022-11-07 16:17:17,916:INFO:Importing libraries
2022-11-07 16:17:17,916:INFO:Copying training dataset
2022-11-07 16:17:17,921:INFO:Defining folds
2022-11-07 16:17:17,922:INFO:Declaring metric variables
2022-11-07 16:17:17,933:INFO:Importing untrained model
2022-11-07 16:17:17,943:INFO:Orthogonal Matching Pursuit Imported successfully
2022-11-07 16:17:17,960:INFO:Starting cross validation
2022-11-07 16:17:17,964:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-11-07 16:17:18,025:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2022-11-07 16:17:18,054:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2022-11-07 16:17:18,096:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2022-11-07 16:17:18,125:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2022-11-07 16:17:18,163:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2022-11-07 16:17:18,207:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2022-11-07 16:17:18,217:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2022-11-07 16:17:18,259:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2022-11-07 16:17:18,277:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2022-11-07 16:17:18,313:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2022-11-07 16:17:18,332:INFO:Calculating mean and std
2022-11-07 16:17:18,335:INFO:Creating metrics dataframe
2022-11-07 16:17:18,350:INFO:Uploading results into container
2022-11-07 16:17:18,350:INFO:Uploading model into container now
2022-11-07 16:17:18,351:INFO:master_model_container: 7
2022-11-07 16:17:18,351:INFO:display_container: 2
2022-11-07 16:17:18,352:INFO:OrthogonalMatchingPursuit()
2022-11-07 16:17:18,352:INFO:create_model() successfully completed......................................
2022-11-07 16:17:18,487:INFO:SubProcess create_model() end ==================================
2022-11-07 16:17:18,488:INFO:Creating metrics dataframe
2022-11-07 16:17:18,508:INFO:Initializing Bayesian Ridge
2022-11-07 16:17:18,508:INFO:Total runtime is 0.16261842648188277 minutes
2022-11-07 16:17:18,516:INFO:SubProcess create_model() called ==================================
2022-11-07 16:17:18,517:INFO:Initializing create_model()
2022-11-07 16:17:18,518:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f20c2a2f390>, estimator=br, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f20c22c9dd0>, model_only=True, return_train_score=False, kwargs={})
2022-11-07 16:17:18,518:INFO:Checking exceptions
2022-11-07 16:17:18,520:INFO:Importing libraries
2022-11-07 16:17:18,521:INFO:Copying training dataset
2022-11-07 16:17:18,527:INFO:Defining folds
2022-11-07 16:17:18,528:INFO:Declaring metric variables
2022-11-07 16:17:18,540:INFO:Importing untrained model
2022-11-07 16:17:18,548:INFO:Bayesian Ridge Imported successfully
2022-11-07 16:17:18,565:INFO:Starting cross validation
2022-11-07 16:17:18,568:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-11-07 16:17:18,925:INFO:Calculating mean and std
2022-11-07 16:17:18,927:INFO:Creating metrics dataframe
2022-11-07 16:17:18,934:INFO:Uploading results into container
2022-11-07 16:17:18,937:INFO:Uploading model into container now
2022-11-07 16:17:18,939:INFO:master_model_container: 8
2022-11-07 16:17:18,939:INFO:display_container: 2
2022-11-07 16:17:18,940:INFO:BayesianRidge()
2022-11-07 16:17:18,940:INFO:create_model() successfully completed......................................
2022-11-07 16:17:19,093:INFO:SubProcess create_model() end ==================================
2022-11-07 16:17:19,093:INFO:Creating metrics dataframe
2022-11-07 16:17:19,114:INFO:Initializing Passive Aggressive Regressor
2022-11-07 16:17:19,115:INFO:Total runtime is 0.1727357069651286 minutes
2022-11-07 16:17:19,123:INFO:SubProcess create_model() called ==================================
2022-11-07 16:17:19,124:INFO:Initializing create_model()
2022-11-07 16:17:19,125:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f20c2a2f390>, estimator=par, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f20c22c9dd0>, model_only=True, return_train_score=False, kwargs={})
2022-11-07 16:17:19,125:INFO:Checking exceptions
2022-11-07 16:17:19,128:INFO:Importing libraries
2022-11-07 16:17:19,128:INFO:Copying training dataset
2022-11-07 16:17:19,133:INFO:Defining folds
2022-11-07 16:17:19,134:INFO:Declaring metric variables
2022-11-07 16:17:19,143:INFO:Importing untrained model
2022-11-07 16:17:19,157:INFO:Passive Aggressive Regressor Imported successfully
2022-11-07 16:17:19,173:INFO:Starting cross validation
2022-11-07 16:17:19,181:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-11-07 16:17:19,524:INFO:Calculating mean and std
2022-11-07 16:17:19,526:INFO:Creating metrics dataframe
2022-11-07 16:17:19,537:INFO:Uploading results into container
2022-11-07 16:17:19,540:INFO:Uploading model into container now
2022-11-07 16:17:19,540:INFO:master_model_container: 9
2022-11-07 16:17:19,541:INFO:display_container: 2
2022-11-07 16:17:19,541:INFO:PassiveAggressiveRegressor(random_state=2902)
2022-11-07 16:17:19,541:INFO:create_model() successfully completed......................................
2022-11-07 16:17:19,679:INFO:SubProcess create_model() end ==================================
2022-11-07 16:17:19,679:INFO:Creating metrics dataframe
2022-11-07 16:17:19,700:INFO:Initializing Huber Regressor
2022-11-07 16:17:19,700:INFO:Total runtime is 0.18248625199000043 minutes
2022-11-07 16:17:19,711:INFO:SubProcess create_model() called ==================================
2022-11-07 16:17:19,713:INFO:Initializing create_model()
2022-11-07 16:17:19,714:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f20c2a2f390>, estimator=huber, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f20c22c9dd0>, model_only=True, return_train_score=False, kwargs={})
2022-11-07 16:17:19,714:INFO:Checking exceptions
2022-11-07 16:17:19,717:INFO:Importing libraries
2022-11-07 16:17:19,717:INFO:Copying training dataset
2022-11-07 16:17:19,721:INFO:Defining folds
2022-11-07 16:17:19,723:INFO:Declaring metric variables
2022-11-07 16:17:19,732:INFO:Importing untrained model
2022-11-07 16:17:19,742:INFO:Huber Regressor Imported successfully
2022-11-07 16:17:19,757:INFO:Starting cross validation
2022-11-07 16:17:19,760:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-11-07 16:17:19,860:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2022-11-07 16:17:19,913:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2022-11-07 16:17:19,996:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2022-11-07 16:17:20,069:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2022-11-07 16:17:20,130:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2022-11-07 16:17:20,191:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2022-11-07 16:17:20,231:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2022-11-07 16:17:20,292:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2022-11-07 16:17:20,343:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2022-11-07 16:17:20,383:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2022-11-07 16:17:20,398:INFO:Calculating mean and std
2022-11-07 16:17:20,403:INFO:Creating metrics dataframe
2022-11-07 16:17:20,415:INFO:Uploading results into container
2022-11-07 16:17:20,416:INFO:Uploading model into container now
2022-11-07 16:17:20,417:INFO:master_model_container: 10
2022-11-07 16:17:20,417:INFO:display_container: 2
2022-11-07 16:17:20,417:INFO:HuberRegressor()
2022-11-07 16:17:20,417:INFO:create_model() successfully completed......................................
2022-11-07 16:17:20,555:INFO:SubProcess create_model() end ==================================
2022-11-07 16:17:20,556:INFO:Creating metrics dataframe
2022-11-07 16:17:20,576:INFO:Initializing K Neighbors Regressor
2022-11-07 16:17:20,577:INFO:Total runtime is 0.1971017082532247 minutes
2022-11-07 16:17:20,588:INFO:SubProcess create_model() called ==================================
2022-11-07 16:17:20,589:INFO:Initializing create_model()
2022-11-07 16:17:20,589:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f20c2a2f390>, estimator=knn, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f20c22c9dd0>, model_only=True, return_train_score=False, kwargs={})
2022-11-07 16:17:20,589:INFO:Checking exceptions
2022-11-07 16:17:20,594:INFO:Importing libraries
2022-11-07 16:17:20,594:INFO:Copying training dataset
2022-11-07 16:17:20,599:INFO:Defining folds
2022-11-07 16:17:20,600:INFO:Declaring metric variables
2022-11-07 16:17:20,611:INFO:Importing untrained model
2022-11-07 16:17:20,624:INFO:K Neighbors Regressor Imported successfully
2022-11-07 16:17:20,641:INFO:Starting cross validation
2022-11-07 16:17:20,643:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-11-07 16:17:21,446:INFO:Calculating mean and std
2022-11-07 16:17:21,448:INFO:Creating metrics dataframe
2022-11-07 16:17:21,457:INFO:Uploading results into container
2022-11-07 16:17:21,458:INFO:Uploading model into container now
2022-11-07 16:17:21,458:INFO:master_model_container: 11
2022-11-07 16:17:21,459:INFO:display_container: 2
2022-11-07 16:17:21,459:INFO:KNeighborsRegressor(n_jobs=-1)
2022-11-07 16:17:21,459:INFO:create_model() successfully completed......................................
2022-11-07 16:17:21,597:INFO:SubProcess create_model() end ==================================
2022-11-07 16:17:21,597:INFO:Creating metrics dataframe
2022-11-07 16:17:21,622:INFO:Initializing Decision Tree Regressor
2022-11-07 16:17:21,623:INFO:Total runtime is 0.2145260016123454 minutes
2022-11-07 16:17:21,631:INFO:SubProcess create_model() called ==================================
2022-11-07 16:17:21,632:INFO:Initializing create_model()
2022-11-07 16:17:21,633:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f20c2a2f390>, estimator=dt, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f20c22c9dd0>, model_only=True, return_train_score=False, kwargs={})
2022-11-07 16:17:21,634:INFO:Checking exceptions
2022-11-07 16:17:21,637:INFO:Importing libraries
2022-11-07 16:17:21,638:INFO:Copying training dataset
2022-11-07 16:17:21,644:INFO:Defining folds
2022-11-07 16:17:21,647:INFO:Declaring metric variables
2022-11-07 16:17:21,659:INFO:Importing untrained model
2022-11-07 16:17:21,666:INFO:Decision Tree Regressor Imported successfully
2022-11-07 16:17:21,685:INFO:Starting cross validation
2022-11-07 16:17:21,693:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-11-07 16:17:22,091:INFO:Calculating mean and std
2022-11-07 16:17:22,095:INFO:Creating metrics dataframe
2022-11-07 16:17:22,109:INFO:Uploading results into container
2022-11-07 16:17:22,110:INFO:Uploading model into container now
2022-11-07 16:17:22,111:INFO:master_model_container: 12
2022-11-07 16:17:22,111:INFO:display_container: 2
2022-11-07 16:17:22,112:INFO:DecisionTreeRegressor(random_state=2902)
2022-11-07 16:17:22,112:INFO:create_model() successfully completed......................................
2022-11-07 16:17:22,252:INFO:SubProcess create_model() end ==================================
2022-11-07 16:17:22,253:INFO:Creating metrics dataframe
2022-11-07 16:17:22,275:INFO:Initializing Random Forest Regressor
2022-11-07 16:17:22,276:INFO:Total runtime is 0.22540885210037231 minutes
2022-11-07 16:17:22,284:INFO:SubProcess create_model() called ==================================
2022-11-07 16:17:22,286:INFO:Initializing create_model()
2022-11-07 16:17:22,288:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f20c2a2f390>, estimator=rf, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f20c22c9dd0>, model_only=True, return_train_score=False, kwargs={})
2022-11-07 16:17:22,289:INFO:Checking exceptions
2022-11-07 16:17:22,292:INFO:Importing libraries
2022-11-07 16:17:22,292:INFO:Copying training dataset
2022-11-07 16:17:22,300:INFO:Defining folds
2022-11-07 16:17:22,303:INFO:Declaring metric variables
2022-11-07 16:17:22,316:INFO:Importing untrained model
2022-11-07 16:17:22,329:INFO:Random Forest Regressor Imported successfully
2022-11-07 16:17:22,347:INFO:Starting cross validation
2022-11-07 16:17:22,350:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-11-07 16:17:25,978:INFO:Calculating mean and std
2022-11-07 16:17:25,981:INFO:Creating metrics dataframe
2022-11-07 16:17:25,993:INFO:Uploading results into container
2022-11-07 16:17:25,996:INFO:Uploading model into container now
2022-11-07 16:17:25,996:INFO:master_model_container: 13
2022-11-07 16:17:25,997:INFO:display_container: 2
2022-11-07 16:17:25,997:INFO:RandomForestRegressor(n_jobs=-1, random_state=2902)
2022-11-07 16:17:25,997:INFO:create_model() successfully completed......................................
2022-11-07 16:17:26,134:INFO:SubProcess create_model() end ==================================
2022-11-07 16:17:26,134:INFO:Creating metrics dataframe
2022-11-07 16:17:26,155:INFO:Initializing Extra Trees Regressor
2022-11-07 16:17:26,156:INFO:Total runtime is 0.2900784651438395 minutes
2022-11-07 16:17:26,166:INFO:SubProcess create_model() called ==================================
2022-11-07 16:17:26,168:INFO:Initializing create_model()
2022-11-07 16:17:26,168:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f20c2a2f390>, estimator=et, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f20c22c9dd0>, model_only=True, return_train_score=False, kwargs={})
2022-11-07 16:17:26,168:INFO:Checking exceptions
2022-11-07 16:17:26,171:INFO:Importing libraries
2022-11-07 16:17:26,172:INFO:Copying training dataset
2022-11-07 16:17:26,180:INFO:Defining folds
2022-11-07 16:17:26,181:INFO:Declaring metric variables
2022-11-07 16:17:26,190:INFO:Importing untrained model
2022-11-07 16:17:26,203:INFO:Extra Trees Regressor Imported successfully
2022-11-07 16:17:26,226:INFO:Starting cross validation
2022-11-07 16:17:26,228:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-11-07 16:17:28,866:INFO:Calculating mean and std
2022-11-07 16:17:28,872:INFO:Creating metrics dataframe
2022-11-07 16:17:28,883:INFO:Uploading results into container
2022-11-07 16:17:28,884:INFO:Uploading model into container now
2022-11-07 16:17:28,885:INFO:master_model_container: 14
2022-11-07 16:17:28,886:INFO:display_container: 2
2022-11-07 16:17:28,886:INFO:ExtraTreesRegressor(n_jobs=-1, random_state=2902)
2022-11-07 16:17:28,886:INFO:create_model() successfully completed......................................
2022-11-07 16:17:29,025:INFO:SubProcess create_model() end ==================================
2022-11-07 16:17:29,026:INFO:Creating metrics dataframe
2022-11-07 16:17:29,051:INFO:Initializing AdaBoost Regressor
2022-11-07 16:17:29,052:INFO:Total runtime is 0.33834826946258545 minutes
2022-11-07 16:17:29,066:INFO:SubProcess create_model() called ==================================
2022-11-07 16:17:29,066:INFO:Initializing create_model()
2022-11-07 16:17:29,067:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f20c2a2f390>, estimator=ada, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f20c22c9dd0>, model_only=True, return_train_score=False, kwargs={})
2022-11-07 16:17:29,067:INFO:Checking exceptions
2022-11-07 16:17:29,071:INFO:Importing libraries
2022-11-07 16:17:29,072:INFO:Copying training dataset
2022-11-07 16:17:29,076:INFO:Defining folds
2022-11-07 16:17:29,077:INFO:Declaring metric variables
2022-11-07 16:17:29,092:INFO:Importing untrained model
2022-11-07 16:17:29,100:INFO:AdaBoost Regressor Imported successfully
2022-11-07 16:17:29,119:INFO:Starting cross validation
2022-11-07 16:17:29,121:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-11-07 16:17:30,454:INFO:Calculating mean and std
2022-11-07 16:17:30,456:INFO:Creating metrics dataframe
2022-11-07 16:17:30,468:INFO:Uploading results into container
2022-11-07 16:17:30,470:INFO:Uploading model into container now
2022-11-07 16:17:30,471:INFO:master_model_container: 15
2022-11-07 16:17:30,471:INFO:display_container: 2
2022-11-07 16:17:30,471:INFO:AdaBoostRegressor(random_state=2902)
2022-11-07 16:17:30,472:INFO:create_model() successfully completed......................................
2022-11-07 16:17:30,607:INFO:SubProcess create_model() end ==================================
2022-11-07 16:17:30,607:INFO:Creating metrics dataframe
2022-11-07 16:17:30,629:INFO:Initializing Gradient Boosting Regressor
2022-11-07 16:17:30,630:INFO:Total runtime is 0.3646516521771749 minutes
2022-11-07 16:17:30,638:INFO:SubProcess create_model() called ==================================
2022-11-07 16:17:30,643:INFO:Initializing create_model()
2022-11-07 16:17:30,643:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f20c2a2f390>, estimator=gbr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f20c22c9dd0>, model_only=True, return_train_score=False, kwargs={})
2022-11-07 16:17:30,643:INFO:Checking exceptions
2022-11-07 16:17:30,645:INFO:Importing libraries
2022-11-07 16:17:30,646:INFO:Copying training dataset
2022-11-07 16:17:30,653:INFO:Defining folds
2022-11-07 16:17:30,655:INFO:Declaring metric variables
2022-11-07 16:17:30,664:INFO:Importing untrained model
2022-11-07 16:17:30,673:INFO:Gradient Boosting Regressor Imported successfully
2022-11-07 16:17:30,691:INFO:Starting cross validation
2022-11-07 16:17:30,696:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-11-07 16:17:32,365:INFO:Calculating mean and std
2022-11-07 16:17:32,368:INFO:Creating metrics dataframe
2022-11-07 16:17:32,383:INFO:Uploading results into container
2022-11-07 16:17:32,384:INFO:Uploading model into container now
2022-11-07 16:17:32,385:INFO:master_model_container: 16
2022-11-07 16:17:32,385:INFO:display_container: 2
2022-11-07 16:17:32,386:INFO:GradientBoostingRegressor(random_state=2902)
2022-11-07 16:17:32,386:INFO:create_model() successfully completed......................................
2022-11-07 16:17:32,524:INFO:SubProcess create_model() end ==================================
2022-11-07 16:17:32,525:INFO:Creating metrics dataframe
2022-11-07 16:17:32,551:INFO:Initializing Light Gradient Boosting Machine
2022-11-07 16:17:32,551:INFO:Total runtime is 0.39666785796483356 minutes
2022-11-07 16:17:32,561:INFO:SubProcess create_model() called ==================================
2022-11-07 16:17:32,561:INFO:Initializing create_model()
2022-11-07 16:17:32,562:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f20c2a2f390>, estimator=lightgbm, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f20c22c9dd0>, model_only=True, return_train_score=False, kwargs={})
2022-11-07 16:17:32,562:INFO:Checking exceptions
2022-11-07 16:17:32,565:INFO:Importing libraries
2022-11-07 16:17:32,565:INFO:Copying training dataset
2022-11-07 16:17:32,574:INFO:Defining folds
2022-11-07 16:17:32,575:INFO:Declaring metric variables
2022-11-07 16:17:32,585:INFO:Importing untrained model
2022-11-07 16:17:32,594:INFO:Light Gradient Boosting Machine Imported successfully
2022-11-07 16:17:32,609:INFO:Starting cross validation
2022-11-07 16:17:32,611:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-11-07 16:17:33,184:INFO:Calculating mean and std
2022-11-07 16:17:33,187:INFO:Creating metrics dataframe
2022-11-07 16:17:33,195:INFO:Uploading results into container
2022-11-07 16:17:33,197:INFO:Uploading model into container now
2022-11-07 16:17:33,198:INFO:master_model_container: 17
2022-11-07 16:17:33,198:INFO:display_container: 2
2022-11-07 16:17:33,199:INFO:LGBMRegressor(random_state=2902)
2022-11-07 16:17:33,203:INFO:create_model() successfully completed......................................
2022-11-07 16:17:33,347:INFO:SubProcess create_model() end ==================================
2022-11-07 16:17:33,348:INFO:Creating metrics dataframe
2022-11-07 16:17:33,374:INFO:Initializing Dummy Regressor
2022-11-07 16:17:33,375:INFO:Total runtime is 0.41040345827738445 minutes
2022-11-07 16:17:33,386:INFO:SubProcess create_model() called ==================================
2022-11-07 16:17:33,387:INFO:Initializing create_model()
2022-11-07 16:17:33,387:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f20c2a2f390>, estimator=dummy, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f20c22c9dd0>, model_only=True, return_train_score=False, kwargs={})
2022-11-07 16:17:33,388:INFO:Checking exceptions
2022-11-07 16:17:33,392:INFO:Importing libraries
2022-11-07 16:17:33,392:INFO:Copying training dataset
2022-11-07 16:17:33,396:INFO:Defining folds
2022-11-07 16:17:33,398:INFO:Declaring metric variables
2022-11-07 16:17:33,414:INFO:Importing untrained model
2022-11-07 16:17:33,431:INFO:Dummy Regressor Imported successfully
2022-11-07 16:17:33,448:INFO:Starting cross validation
2022-11-07 16:17:33,450:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-11-07 16:17:33,750:INFO:Calculating mean and std
2022-11-07 16:17:33,754:INFO:Creating metrics dataframe
2022-11-07 16:17:33,764:INFO:Uploading results into container
2022-11-07 16:17:33,766:INFO:Uploading model into container now
2022-11-07 16:17:33,766:INFO:master_model_container: 18
2022-11-07 16:17:33,767:INFO:display_container: 2
2022-11-07 16:17:33,767:INFO:DummyRegressor()
2022-11-07 16:17:33,767:INFO:create_model() successfully completed......................................
2022-11-07 16:17:33,904:INFO:SubProcess create_model() end ==================================
2022-11-07 16:17:33,904:INFO:Creating metrics dataframe
2022-11-07 16:17:33,951:INFO:Initializing create_model()
2022-11-07 16:17:33,952:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f20c2a2f390>, estimator=ExtraTreesRegressor(n_jobs=-1, random_state=2902), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2022-11-07 16:17:33,952:INFO:Checking exceptions
2022-11-07 16:17:33,958:INFO:Importing libraries
2022-11-07 16:17:33,958:INFO:Copying training dataset
2022-11-07 16:17:33,963:INFO:Defining folds
2022-11-07 16:17:33,963:INFO:Declaring metric variables
2022-11-07 16:17:33,963:INFO:Importing untrained model
2022-11-07 16:17:33,964:INFO:Declaring custom model
2022-11-07 16:17:33,965:INFO:Extra Trees Regressor Imported successfully
2022-11-07 16:17:33,966:INFO:Cross validation set to False
2022-11-07 16:17:33,966:INFO:Fitting Model
2022-11-07 16:17:34,264:INFO:ExtraTreesRegressor(n_jobs=-1, random_state=2902)
2022-11-07 16:17:34,264:INFO:create_model() successfully completed......................................
2022-11-07 16:17:34,498:INFO:master_model_container: 18
2022-11-07 16:17:34,499:INFO:display_container: 2
2022-11-07 16:17:34,499:INFO:ExtraTreesRegressor(n_jobs=-1, random_state=2902)
2022-11-07 16:17:34,500:INFO:compare_models() successfully completed......................................
2022-11-07 16:21:56,297:INFO:PyCaret RegressionExperiment
2022-11-07 16:21:56,297:INFO:Logging name: reg-default-name
2022-11-07 16:21:56,297:INFO:ML Usecase: MLUsecase.REGRESSION
2022-11-07 16:21:56,298:INFO:version 3.0.0.rc4
2022-11-07 16:21:56,298:INFO:Initializing setup()
2022-11-07 16:21:56,298:INFO:self.USI: 2320
2022-11-07 16:21:56,299:INFO:self.variable_keys: {'display_container', 'gpu_param', '_all_models', 'log_plots_param', 'fold_shuffle_param', 'fold_groups_param', 'memory', 'data', 'y_test', 'target_param', 'fold_generator', 'html_param', '_gpu_n_jobs_param', 'exp_id', 'logging_param', '_all_models_internal', 'n_jobs_param', 'variable_keys', 'idx', 'transform_target_method_param', 'exp_name_log', 'y', 'X', '_ml_usecase', 'seed', 'X_test', 'y_train', 'pipeline', '_available_plots', 'USI', 'master_model_container', 'X_train', 'transform_target_param', '_all_metrics'}
2022-11-07 16:21:56,301:INFO:Checking environment
2022-11-07 16:21:56,301:INFO:python_version: 3.7.15
2022-11-07 16:21:56,301:INFO:python_build: ('default', 'Oct 12 2022 19:14:55')
2022-11-07 16:21:56,301:INFO:machine: x86_64
2022-11-07 16:21:56,301:INFO:platform: Linux-5.10.133+-x86_64-with-Ubuntu-18.04-bionic
2022-11-07 16:21:56,302:INFO:Memory: svmem(total=13616353280, available=11627405312, percent=14.6, used=1829404672, free=7966945280, active=780877824, inactive=4501278720, buffers=421498880, cached=3398504448, shared=1339392, slab=276938752)
2022-11-07 16:21:56,302:INFO:Physical Core: 1
2022-11-07 16:21:56,303:INFO:Logical Core: 2
2022-11-07 16:21:56,303:INFO:Checking libraries
2022-11-07 16:21:56,303:INFO:System:
2022-11-07 16:21:56,304:INFO:    python: 3.7.15 (default, Oct 12 2022, 19:14:55)  [GCC 7.5.0]
2022-11-07 16:21:56,304:INFO:executable: /usr/bin/python3
2022-11-07 16:21:56,304:INFO:   machine: Linux-5.10.133+-x86_64-with-Ubuntu-18.04-bionic
2022-11-07 16:21:56,304:INFO:PyCaret required dependencies:
2022-11-07 16:21:56,305:INFO:                 pip: 21.1.3
2022-11-07 16:21:56,305:INFO:          setuptools: 57.4.0
2022-11-07 16:21:56,305:INFO:             pycaret: 3.0.0rc4
2022-11-07 16:21:56,305:INFO:             IPython: 7.9.0
2022-11-07 16:21:56,305:INFO:          ipywidgets: 7.7.1
2022-11-07 16:21:56,305:INFO:                tqdm: 4.64.1
2022-11-07 16:21:56,305:INFO:               numpy: 1.21.6
2022-11-07 16:21:56,305:INFO:              pandas: 1.3.5
2022-11-07 16:21:56,306:INFO:              jinja2: 2.11.3
2022-11-07 16:21:56,306:INFO:               scipy: 1.7.3
2022-11-07 16:21:56,306:INFO:              joblib: 1.2.0
2022-11-07 16:21:56,306:INFO:             sklearn: 1.0.2
2022-11-07 16:21:56,306:INFO:                pyod: 1.0.6
2022-11-07 16:21:56,306:INFO:            imblearn: 0.8.1
2022-11-07 16:21:56,306:INFO:   category_encoders: 2.5.1.post0
2022-11-07 16:21:56,307:INFO:            lightgbm: 3.3.3
2022-11-07 16:21:56,307:INFO:               numba: 0.55.2
2022-11-07 16:21:56,307:INFO:            requests: 2.28.1
2022-11-07 16:21:56,307:INFO:          matplotlib: 3.5.3
2022-11-07 16:21:56,307:INFO:          scikitplot: 0.3.7
2022-11-07 16:21:56,307:INFO:         yellowbrick: 1.5
2022-11-07 16:21:56,307:INFO:              plotly: 5.5.0
2022-11-07 16:21:56,307:INFO:             kaleido: 0.2.1
2022-11-07 16:21:56,308:INFO:         statsmodels: 0.12.2
2022-11-07 16:21:56,308:INFO:              sktime: 0.13.4
2022-11-07 16:21:56,308:INFO:               tbats: 1.1.1
2022-11-07 16:21:56,308:INFO:            pmdarima: 1.8.5
2022-11-07 16:21:56,309:INFO:              psutil: 5.9.3
2022-11-07 16:21:56,309:INFO:PyCaret optional dependencies:
2022-11-07 16:21:56,309:INFO:                shap: Not installed
2022-11-07 16:21:56,309:INFO:           interpret: Not installed
2022-11-07 16:21:56,310:INFO:                umap: Not installed
2022-11-07 16:21:56,310:INFO:    pandas_profiling: 1.4.1
2022-11-07 16:21:56,310:INFO:  explainerdashboard: Not installed
2022-11-07 16:21:56,310:INFO:             autoviz: Not installed
2022-11-07 16:21:56,310:INFO:           fairlearn: Not installed
2022-11-07 16:21:56,310:INFO:             xgboost: 0.90
2022-11-07 16:21:56,310:INFO:            catboost: Not installed
2022-11-07 16:21:56,310:INFO:              kmodes: Not installed
2022-11-07 16:21:56,310:INFO:             mlxtend: 0.14.0
2022-11-07 16:21:56,310:INFO:       statsforecast: Not installed
2022-11-07 16:21:56,310:INFO:        tune_sklearn: Not installed
2022-11-07 16:21:56,310:INFO:                 ray: Not installed
2022-11-07 16:21:56,311:INFO:            hyperopt: 0.1.2
2022-11-07 16:21:56,311:INFO:              optuna: Not installed
2022-11-07 16:21:56,311:INFO:               skopt: Not installed
2022-11-07 16:21:56,311:INFO:              mlflow: Not installed
2022-11-07 16:21:56,311:INFO:              gradio: Not installed
2022-11-07 16:21:56,311:INFO:             fastapi: Not installed
2022-11-07 16:21:56,311:INFO:             uvicorn: Not installed
2022-11-07 16:21:56,311:INFO:              m2cgen: Not installed
2022-11-07 16:21:56,311:INFO:           evidently: Not installed
2022-11-07 16:21:56,311:INFO:                nltk: 3.7
2022-11-07 16:21:56,312:INFO:            pyLDAvis: Not installed
2022-11-07 16:21:56,312:INFO:              gensim: 3.6.0
2022-11-07 16:21:56,312:INFO:               spacy: 3.4.2
2022-11-07 16:21:56,312:INFO:           wordcloud: 1.8.2.2
2022-11-07 16:21:56,312:INFO:            textblob: 0.15.3
2022-11-07 16:21:56,312:INFO:               fugue: Not installed
2022-11-07 16:21:56,312:INFO:           streamlit: Not installed
2022-11-07 16:21:56,313:INFO:             prophet: 1.1.1
2022-11-07 16:21:56,313:INFO:None
2022-11-07 16:21:56,313:INFO:Set up data.
2022-11-07 16:21:56,338:INFO:Set up train/test split.
2022-11-07 16:21:56,346:INFO:Set up index.
2022-11-07 16:21:56,347:INFO:Set up folding strategy.
2022-11-07 16:21:56,348:INFO:Assigning column types.
2022-11-07 16:21:56,355:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2022-11-07 16:21:56,356:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2022-11-07 16:21:56,362:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2022-11-07 16:21:56,368:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2022-11-07 16:21:56,440:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2022-11-07 16:21:56,495:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2022-11-07 16:21:56,496:INFO:Soft dependency imported: xgboost: 0.90
2022-11-07 16:21:56,496:WARNING:Wrong xgboost version. Expected xgboost>=1.1.0, got xgboost==0.90
2022-11-07 16:21:56,497:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-11-07 16:21:56,498:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2022-11-07 16:21:56,504:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2022-11-07 16:21:56,509:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2022-11-07 16:21:56,589:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2022-11-07 16:21:56,643:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2022-11-07 16:21:56,644:INFO:Soft dependency imported: xgboost: 0.90
2022-11-07 16:21:56,644:WARNING:Wrong xgboost version. Expected xgboost>=1.1.0, got xgboost==0.90
2022-11-07 16:21:56,645:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-11-07 16:21:56,645:INFO:Engine successfully changes for model 'lasso' to 'sklearn'.
2022-11-07 16:21:56,651:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2022-11-07 16:21:56,656:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2022-11-07 16:21:56,734:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2022-11-07 16:21:56,787:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2022-11-07 16:21:56,788:INFO:Soft dependency imported: xgboost: 0.90
2022-11-07 16:21:56,788:WARNING:Wrong xgboost version. Expected xgboost>=1.1.0, got xgboost==0.90
2022-11-07 16:21:56,789:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-11-07 16:21:56,795:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2022-11-07 16:21:56,800:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2022-11-07 16:21:56,885:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2022-11-07 16:21:56,939:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2022-11-07 16:21:56,940:INFO:Soft dependency imported: xgboost: 0.90
2022-11-07 16:21:56,940:WARNING:Wrong xgboost version. Expected xgboost>=1.1.0, got xgboost==0.90
2022-11-07 16:21:56,941:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-11-07 16:21:56,941:INFO:Engine successfully changes for model 'ridge' to 'sklearn'.
2022-11-07 16:21:56,952:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2022-11-07 16:21:57,024:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2022-11-07 16:21:57,079:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2022-11-07 16:21:57,080:INFO:Soft dependency imported: xgboost: 0.90
2022-11-07 16:21:57,080:WARNING:Wrong xgboost version. Expected xgboost>=1.1.0, got xgboost==0.90
2022-11-07 16:21:57,081:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-11-07 16:21:57,092:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2022-11-07 16:21:57,200:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2022-11-07 16:21:57,257:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2022-11-07 16:21:57,258:INFO:Soft dependency imported: xgboost: 0.90
2022-11-07 16:21:57,258:WARNING:Wrong xgboost version. Expected xgboost>=1.1.0, got xgboost==0.90
2022-11-07 16:21:57,259:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-11-07 16:21:57,259:INFO:Engine successfully changes for model 'en' to 'sklearn'.
2022-11-07 16:21:57,363:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2022-11-07 16:21:57,490:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2022-11-07 16:21:57,491:INFO:Soft dependency imported: xgboost: 0.90
2022-11-07 16:21:57,492:WARNING:Wrong xgboost version. Expected xgboost>=1.1.0, got xgboost==0.90
2022-11-07 16:21:57,492:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-11-07 16:21:57,686:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2022-11-07 16:21:57,791:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2022-11-07 16:21:57,793:INFO:Soft dependency imported: xgboost: 0.90
2022-11-07 16:21:57,793:WARNING:Wrong xgboost version. Expected xgboost>=1.1.0, got xgboost==0.90
2022-11-07 16:21:57,794:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-11-07 16:21:57,795:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2022-11-07 16:21:57,975:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2022-11-07 16:21:58,073:INFO:Soft dependency imported: xgboost: 0.90
2022-11-07 16:21:58,074:WARNING:Wrong xgboost version. Expected xgboost>=1.1.0, got xgboost==0.90
2022-11-07 16:21:58,074:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-11-07 16:21:58,163:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2022-11-07 16:21:58,227:INFO:Soft dependency imported: xgboost: 0.90
2022-11-07 16:21:58,227:WARNING:Wrong xgboost version. Expected xgboost>=1.1.0, got xgboost==0.90
2022-11-07 16:21:58,228:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-11-07 16:21:58,228:INFO:Engine successfully changes for model 'svm' to 'sklearn'.
2022-11-07 16:21:58,368:INFO:Soft dependency imported: xgboost: 0.90
2022-11-07 16:21:58,368:WARNING:Wrong xgboost version. Expected xgboost>=1.1.0, got xgboost==0.90
2022-11-07 16:21:58,369:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-11-07 16:21:58,513:INFO:Soft dependency imported: xgboost: 0.90
2022-11-07 16:21:58,513:WARNING:Wrong xgboost version. Expected xgboost>=1.1.0, got xgboost==0.90
2022-11-07 16:21:58,513:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-11-07 16:21:58,515:INFO:Preparing preprocessing pipeline...
2022-11-07 16:21:58,517:INFO:Set up simple imputation.
2022-11-07 16:21:58,517:INFO:Set up variance threshold.
2022-11-07 16:21:58,696:INFO:Finished creating preprocessing pipeline.
2022-11-07 16:21:58,705:INFO:Pipeline: Pipeline(memory=Memory(location=/tmp/joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['1rd_red', ' 1rd_green',
                                             ' 1rd_blue', ' 1rd_r', ' 1rd_rg',
                                             ' 1rd_most_red', ' 1rd_least_red',
                                             ' 1rd_most_yellow', ' 1rd_size',
                                             ' 2nd_red', ' 2nd_green',
                                             ' 2nd_blue', ' 2rd_r', ' 2rd_rg',
                                             ' 2rd_most_red', ' 2rd_least_red',
                                             ' 2rd_most_yellow', ' 2nd_size',
                                             ' 3rd_red', ' 3rd_green',
                                             ' 3rd_bl...d_rg',
                                             ' 3rd_most_red', ' 3rd_least_red',
                                             ' 3rd_most_yellow', ' 3rd_size',
                                             ' 4th_red', ' 4th_green',
                                             ' 4th_blue', ...],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=[],
                                    transformer=SimpleImputer(fill_value='constant',
                                                              strategy='constant'))),
                ('low_variance',
                 TransformerWrapper(exclude=[],
                                    transformer=VarianceThreshold(threshold=0)))])
2022-11-07 16:21:58,705:INFO:Creating final display dataframe.
2022-11-07 16:21:59,298:INFO:Setup display_container:                Description             Value
0               Session id              7766
1                   Target         Sweetness
2              Target type        Regression
3               Data shape         (200, 56)
4         Train data shape         (139, 56)
5          Test data shape          (61, 56)
6         Numeric features                55
7               Preprocess              True
8          Imputation type            simple
9       Numeric imputation              mean
10  Categorical imputation          constant
11  Low variance threshold                 0
12          Fold Generator             KFold
13             Fold Number                10
14                CPU Jobs                -1
15                 Use GPU             False
16          Log Experiment             False
17         Experiment Name  reg-default-name
18                     USI              2320
2022-11-07 16:21:59,462:INFO:Soft dependency imported: xgboost: 0.90
2022-11-07 16:21:59,463:WARNING:Wrong xgboost version. Expected xgboost>=1.1.0, got xgboost==0.90
2022-11-07 16:21:59,463:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-11-07 16:21:59,602:INFO:Soft dependency imported: xgboost: 0.90
2022-11-07 16:21:59,602:WARNING:Wrong xgboost version. Expected xgboost>=1.1.0, got xgboost==0.90
2022-11-07 16:21:59,603:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-11-07 16:21:59,610:INFO:setup() successfully completed in 3.32s...............
2022-11-07 16:28:35,698:INFO:Initializing compare_models()
2022-11-07 16:28:35,700:INFO:compare_models(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f20c26a8f50>, include=None, fold=None, round=4, cross_validation=True, sort=R2, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.regression.oop.RegressionExperiment object at 0x7f20c26a8f50>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'R2', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.regression.oop.RegressionExperiment'>}, exclude=None)
2022-11-07 16:28:35,701:INFO:Checking exceptions
2022-11-07 16:28:35,705:INFO:Preparing display monitor
2022-11-07 16:28:35,792:INFO:Initializing Linear Regression
2022-11-07 16:28:35,793:INFO:Total runtime is 6.341934204101563e-06 minutes
2022-11-07 16:28:35,802:INFO:SubProcess create_model() called ==================================
2022-11-07 16:28:35,803:INFO:Initializing create_model()
2022-11-07 16:28:35,803:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f20c26a8f50>, estimator=lr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f20c2532790>, model_only=True, return_train_score=False, kwargs={})
2022-11-07 16:28:35,803:INFO:Checking exceptions
2022-11-07 16:28:35,806:INFO:Importing libraries
2022-11-07 16:28:35,807:INFO:Copying training dataset
2022-11-07 16:28:35,811:INFO:Defining folds
2022-11-07 16:28:35,811:INFO:Declaring metric variables
2022-11-07 16:28:35,820:INFO:Importing untrained model
2022-11-07 16:28:35,829:INFO:Linear Regression Imported successfully
2022-11-07 16:28:35,844:INFO:Starting cross validation
2022-11-07 16:28:35,850:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-11-07 16:28:43,916:INFO:Calculating mean and std
2022-11-07 16:28:43,921:INFO:Creating metrics dataframe
2022-11-07 16:28:43,930:INFO:Uploading results into container
2022-11-07 16:28:43,935:INFO:Uploading model into container now
2022-11-07 16:28:43,937:INFO:master_model_container: 1
2022-11-07 16:28:43,937:INFO:display_container: 2
2022-11-07 16:28:43,938:INFO:LinearRegression(n_jobs=-1)
2022-11-07 16:28:43,938:INFO:create_model() successfully completed......................................
2022-11-07 16:28:44,129:INFO:SubProcess create_model() end ==================================
2022-11-07 16:28:44,130:INFO:Creating metrics dataframe
2022-11-07 16:28:44,154:INFO:Initializing Lasso Regression
2022-11-07 16:28:44,155:INFO:Total runtime is 0.13938100337982176 minutes
2022-11-07 16:28:44,165:INFO:SubProcess create_model() called ==================================
2022-11-07 16:28:44,166:INFO:Initializing create_model()
2022-11-07 16:28:44,167:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f20c26a8f50>, estimator=lasso, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f20c2532790>, model_only=True, return_train_score=False, kwargs={})
2022-11-07 16:28:44,167:INFO:Checking exceptions
2022-11-07 16:28:44,172:INFO:Importing libraries
2022-11-07 16:28:44,173:INFO:Copying training dataset
2022-11-07 16:28:44,178:INFO:Defining folds
2022-11-07 16:28:44,178:INFO:Declaring metric variables
2022-11-07 16:28:44,194:INFO:Importing untrained model
2022-11-07 16:28:44,203:INFO:Lasso Regression Imported successfully
2022-11-07 16:28:44,220:INFO:Starting cross validation
2022-11-07 16:28:44,223:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-11-07 16:28:44,281:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.067e-02, tolerance: 2.146e-02
  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive

2022-11-07 16:28:44,340:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.003e+00, tolerance: 2.303e-02
  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive

2022-11-07 16:28:44,384:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.103e-01, tolerance: 2.344e-02
  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive

2022-11-07 16:28:44,401:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.600e+00, tolerance: 2.406e-02
  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive

2022-11-07 16:28:44,462:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.885e-02, tolerance: 2.334e-02
  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive

2022-11-07 16:28:44,520:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.909e-01, tolerance: 2.257e-02
  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive

2022-11-07 16:28:44,524:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.218e-01, tolerance: 2.218e-02
  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive

2022-11-07 16:28:44,582:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.370e-01, tolerance: 2.291e-02
  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive

2022-11-07 16:28:44,593:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.781e-02, tolerance: 2.095e-02
  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive

2022-11-07 16:28:44,632:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.761e-02, tolerance: 2.284e-02
  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive

2022-11-07 16:28:44,646:INFO:Calculating mean and std
2022-11-07 16:28:44,649:INFO:Creating metrics dataframe
2022-11-07 16:28:44,666:INFO:Uploading results into container
2022-11-07 16:28:44,667:INFO:Uploading model into container now
2022-11-07 16:28:44,668:INFO:master_model_container: 2
2022-11-07 16:28:44,668:INFO:display_container: 2
2022-11-07 16:28:44,668:INFO:Lasso(random_state=7766)
2022-11-07 16:28:44,669:INFO:create_model() successfully completed......................................
2022-11-07 16:28:44,816:INFO:SubProcess create_model() end ==================================
2022-11-07 16:28:44,816:INFO:Creating metrics dataframe
2022-11-07 16:28:44,839:INFO:Initializing Ridge Regression
2022-11-07 16:28:44,839:INFO:Total runtime is 0.1507784366607666 minutes
2022-11-07 16:28:44,851:INFO:SubProcess create_model() called ==================================
2022-11-07 16:28:44,852:INFO:Initializing create_model()
2022-11-07 16:28:44,852:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f20c26a8f50>, estimator=ridge, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f20c2532790>, model_only=True, return_train_score=False, kwargs={})
2022-11-07 16:28:44,852:INFO:Checking exceptions
2022-11-07 16:28:44,856:INFO:Importing libraries
2022-11-07 16:28:44,856:INFO:Copying training dataset
2022-11-07 16:28:44,862:INFO:Defining folds
2022-11-07 16:28:44,868:INFO:Declaring metric variables
2022-11-07 16:28:44,878:INFO:Importing untrained model
2022-11-07 16:28:44,887:INFO:Ridge Regression Imported successfully
2022-11-07 16:28:44,908:INFO:Starting cross validation
2022-11-07 16:28:44,910:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-11-07 16:28:45,260:INFO:Calculating mean and std
2022-11-07 16:28:45,265:INFO:Creating metrics dataframe
2022-11-07 16:28:45,277:INFO:Uploading results into container
2022-11-07 16:28:45,278:INFO:Uploading model into container now
2022-11-07 16:28:45,279:INFO:master_model_container: 3
2022-11-07 16:28:45,279:INFO:display_container: 2
2022-11-07 16:28:45,280:INFO:Ridge(random_state=7766)
2022-11-07 16:28:45,280:INFO:create_model() successfully completed......................................
2022-11-07 16:28:45,423:INFO:SubProcess create_model() end ==================================
2022-11-07 16:28:45,424:INFO:Creating metrics dataframe
2022-11-07 16:28:45,443:INFO:Initializing Elastic Net
2022-11-07 16:28:45,443:INFO:Total runtime is 0.16085083484649657 minutes
2022-11-07 16:28:45,452:INFO:SubProcess create_model() called ==================================
2022-11-07 16:28:45,453:INFO:Initializing create_model()
2022-11-07 16:28:45,453:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f20c26a8f50>, estimator=en, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f20c2532790>, model_only=True, return_train_score=False, kwargs={})
2022-11-07 16:28:45,453:INFO:Checking exceptions
2022-11-07 16:28:45,458:INFO:Importing libraries
2022-11-07 16:28:45,458:INFO:Copying training dataset
2022-11-07 16:28:45,465:INFO:Defining folds
2022-11-07 16:28:45,466:INFO:Declaring metric variables
2022-11-07 16:28:45,481:INFO:Importing untrained model
2022-11-07 16:28:45,493:INFO:Elastic Net Imported successfully
2022-11-07 16:28:45,510:INFO:Starting cross validation
2022-11-07 16:28:45,515:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-11-07 16:28:45,573:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.187e-01, tolerance: 2.146e-02
  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive

2022-11-07 16:28:45,632:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.052e+00, tolerance: 2.344e-02
  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive

2022-11-07 16:28:45,635:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.481e+00, tolerance: 2.303e-02
  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive

2022-11-07 16:28:45,717:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.279e+00, tolerance: 2.406e-02
  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive

2022-11-07 16:28:45,738:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.254e+00, tolerance: 2.334e-02
  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive

2022-11-07 16:28:45,795:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.697e+00, tolerance: 2.257e-02
  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive

2022-11-07 16:28:45,808:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.200e-01, tolerance: 2.218e-02
  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive

2022-11-07 16:28:45,860:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.767e-01, tolerance: 2.291e-02
  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive

2022-11-07 16:28:45,885:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.569e+00, tolerance: 2.095e-02
  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive

2022-11-07 16:28:45,917:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.474e-01, tolerance: 2.284e-02
  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive

2022-11-07 16:28:45,933:INFO:Calculating mean and std
2022-11-07 16:28:45,935:INFO:Creating metrics dataframe
2022-11-07 16:28:45,941:INFO:Uploading results into container
2022-11-07 16:28:45,942:INFO:Uploading model into container now
2022-11-07 16:28:45,944:INFO:master_model_container: 4
2022-11-07 16:28:45,944:INFO:display_container: 2
2022-11-07 16:28:45,946:INFO:ElasticNet(random_state=7766)
2022-11-07 16:28:45,949:INFO:create_model() successfully completed......................................
2022-11-07 16:28:46,096:INFO:SubProcess create_model() end ==================================
2022-11-07 16:28:46,096:INFO:Creating metrics dataframe
2022-11-07 16:28:46,124:INFO:Initializing Least Angle Regression
2022-11-07 16:28:46,124:INFO:Total runtime is 0.17220029036204018 minutes
2022-11-07 16:28:46,134:INFO:SubProcess create_model() called ==================================
2022-11-07 16:28:46,136:INFO:Initializing create_model()
2022-11-07 16:28:46,140:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f20c26a8f50>, estimator=lar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f20c2532790>, model_only=True, return_train_score=False, kwargs={})
2022-11-07 16:28:46,140:INFO:Checking exceptions
2022-11-07 16:28:46,142:INFO:Importing libraries
2022-11-07 16:28:46,142:INFO:Copying training dataset
2022-11-07 16:28:46,147:INFO:Defining folds
2022-11-07 16:28:46,147:INFO:Declaring metric variables
2022-11-07 16:28:46,161:INFO:Importing untrained model
2022-11-07 16:28:46,171:INFO:Least Angle Regression Imported successfully
2022-11-07 16:28:46,187:INFO:Starting cross validation
2022-11-07 16:28:46,190:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-11-07 16:28:46,239:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2022-11-07 16:28:46,259:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2022-11-07 16:28:46,269:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 56 iterations, i.e. alpha=1.477e-03, with an active set of 47 regressors, and the smallest cholesky pivot element being 6.829e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:28:46,276:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 67 iterations, i.e. alpha=5.745e+02, with an active set of 48 regressors, and the smallest cholesky pivot element being 7.451e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:28:46,277:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 68 iterations, i.e. alpha=4.744e+02, with an active set of 49 regressors, and the smallest cholesky pivot element being 5.674e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:28:46,278:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 68 iterations, i.e. alpha=3.288e+02, with an active set of 49 regressors, and the smallest cholesky pivot element being 7.451e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:28:46,278:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 68 iterations, i.e. alpha=3.107e+02, with an active set of 49 regressors, and the smallest cholesky pivot element being 9.246e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:28:46,279:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 64 iterations, i.e. alpha=1.366e-03, with an active set of 51 regressors, and the smallest cholesky pivot element being 6.664e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:28:46,279:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 70 iterations, i.e. alpha=1.014e+02, with an active set of 50 regressors, and the smallest cholesky pivot element being 7.376e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:28:46,280:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 67 iterations, i.e. alpha=9.984e-04, with an active set of 53 regressors, and the smallest cholesky pivot element being 9.125e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:28:46,280:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 67 iterations, i.e. alpha=7.695e-04, with an active set of 53 regressors, and the smallest cholesky pivot element being 6.409e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:28:46,280:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 72 iterations, i.e. alpha=7.105e+01, with an active set of 51 regressors, and the smallest cholesky pivot element being 6.989e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:28:46,284:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 72 iterations, i.e. alpha=4.890e+01, with an active set of 51 regressors, and the smallest cholesky pivot element being 6.989e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:28:46,285:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 72 iterations, i.e. alpha=3.601e+01, with an active set of 51 regressors, and the smallest cholesky pivot element being 5.373e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:28:46,285:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 72 iterations, i.e. alpha=3.125e+01, with an active set of 51 regressors, and the smallest cholesky pivot element being 9.186e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:28:46,354:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2022-11-07 16:28:46,362:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2022-11-07 16:28:46,371:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 63 iterations, i.e. alpha=1.037e-02, with an active set of 49 regressors, and the smallest cholesky pivot element being 6.909e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:28:46,372:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 66 iterations, i.e. alpha=4.605e-03, with an active set of 51 regressors, and the smallest cholesky pivot element being 9.483e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:28:46,373:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 68 iterations, i.e. alpha=2.698e-03, with an active set of 52 regressors, and the smallest cholesky pivot element being 9.483e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:28:46,374:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 68 iterations, i.e. alpha=2.061e-03, with an active set of 52 regressors, and the smallest cholesky pivot element being 9.483e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:28:46,374:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 68 iterations, i.e. alpha=1.207e-03, with an active set of 52 regressors, and the smallest cholesky pivot element being 7.224e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:28:46,389:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 56 iterations, i.e. alpha=1.415e-02, with an active set of 45 regressors, and the smallest cholesky pivot element being 8.625e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:28:46,392:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 56 iterations, i.e. alpha=1.365e-02, with an active set of 45 regressors, and the smallest cholesky pivot element being 7.525e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:28:46,396:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 64 iterations, i.e. alpha=1.631e-02, with an active set of 48 regressors, and the smallest cholesky pivot element being 6.747e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:28:46,402:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 68 iterations, i.e. alpha=1.293e-02, with an active set of 50 regressors, and the smallest cholesky pivot element being 6.495e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:28:46,404:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 68 iterations, i.e. alpha=8.778e-03, with an active set of 50 regressors, and the smallest cholesky pivot element being 6.322e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:28:46,405:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 70 iterations, i.e. alpha=3.753e-03, with an active set of 52 regressors, and the smallest cholesky pivot element being 6.322e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:28:46,406:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 70 iterations, i.e. alpha=9.715e-04, with an active set of 52 regressors, and the smallest cholesky pivot element being 6.409e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:28:46,406:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 70 iterations, i.e. alpha=4.151e-04, with an active set of 52 regressors, and the smallest cholesky pivot element being 7.068e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:28:46,434:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2022-11-07 16:28:46,448:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 55 iterations, i.e. alpha=3.325e-02, with an active set of 45 regressors, and the smallest cholesky pivot element being 7.671e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:28:46,453:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 68 iterations, i.e. alpha=5.682e-02, with an active set of 52 regressors, and the smallest cholesky pivot element being 7.068e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:28:46,453:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 68 iterations, i.e. alpha=2.560e-02, with an active set of 52 regressors, and the smallest cholesky pivot element being 7.451e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:28:46,454:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 68 iterations, i.e. alpha=1.344e-02, with an active set of 52 regressors, and the smallest cholesky pivot element being 6.909e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:28:46,494:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2022-11-07 16:28:46,500:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2022-11-07 16:28:46,510:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 61 iterations, i.e. alpha=2.510e-02, with an active set of 46 regressors, and the smallest cholesky pivot element being 8.941e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:28:46,512:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 67 iterations, i.e. alpha=1.182e-02, with an active set of 49 regressors, and the smallest cholesky pivot element being 8.162e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:28:46,514:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 69 iterations, i.e. alpha=1.040e-02, with an active set of 50 regressors, and the smallest cholesky pivot element being 8.752e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:28:46,514:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 69 iterations, i.e. alpha=9.195e-03, with an active set of 50 regressors, and the smallest cholesky pivot element being 8.752e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:28:46,514:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 69 iterations, i.e. alpha=5.792e-03, with an active set of 50 regressors, and the smallest cholesky pivot element being 8.093e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:28:46,515:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 69 iterations, i.e. alpha=3.888e-03, with an active set of 50 regressors, and the smallest cholesky pivot element being 6.829e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:28:46,515:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 69 iterations, i.e. alpha=6.780e-04, with an active set of 50 regressors, and the smallest cholesky pivot element being 8.878e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:28:46,519:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 71 iterations, i.e. alpha=1.470e+00, with an active set of 48 regressors, and the smallest cholesky pivot element being 6.322e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:28:46,520:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 71 iterations, i.e. alpha=1.164e+00, with an active set of 48 regressors, and the smallest cholesky pivot element being 7.146e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:28:46,520:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 73 iterations, i.e. alpha=1.337e+00, with an active set of 49 regressors, and the smallest cholesky pivot element being 7.146e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:28:46,522:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 76 iterations, i.e. alpha=1.281e+00, with an active set of 50 regressors, and the smallest cholesky pivot element being 7.598e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:28:46,522:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 76 iterations, i.e. alpha=8.772e-01, with an active set of 50 regressors, and the smallest cholesky pivot element being 6.234e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:28:46,522:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 76 iterations, i.e. alpha=5.751e-01, with an active set of 50 regressors, and the smallest cholesky pivot element being 8.816e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:28:46,523:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 76 iterations, i.e. alpha=2.994e-01, with an active set of 50 regressors, and the smallest cholesky pivot element being 8.816e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:28:46,524:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 78 iterations, i.e. alpha=2.114e-01, with an active set of 51 regressors, and the smallest cholesky pivot element being 6.234e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:28:46,524:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 78 iterations, i.e. alpha=1.132e-01, with an active set of 51 regressors, and the smallest cholesky pivot element being 7.525e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:28:46,525:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 78 iterations, i.e. alpha=7.723e-02, with an active set of 51 regressors, and the smallest cholesky pivot element being 7.525e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:28:46,525:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 78 iterations, i.e. alpha=4.388e-02, with an active set of 51 regressors, and the smallest cholesky pivot element being 6.829e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:28:46,562:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2022-11-07 16:28:46,572:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2022-11-07 16:28:46,586:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 73 iterations, i.e. alpha=5.333e-01, with an active set of 52 regressors, and the smallest cholesky pivot element being 8.625e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:28:46,586:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 73 iterations, i.e. alpha=2.712e-01, with an active set of 52 regressors, and the smallest cholesky pivot element being 6.234e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:28:46,587:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 73 iterations, i.e. alpha=2.253e-01, with an active set of 52 regressors, and the smallest cholesky pivot element being 6.234e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:28:46,592:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 74 iterations, i.e. alpha=3.327e+02, with an active set of 51 regressors, and the smallest cholesky pivot element being 6.989e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:28:46,594:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 77 iterations, i.e. alpha=7.402e+01, with an active set of 53 regressors, and the smallest cholesky pivot element being 6.909e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:28:46,594:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 77 iterations, i.e. alpha=7.011e+01, with an active set of 53 regressors, and the smallest cholesky pivot element being 6.909e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:28:46,627:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2022-11-07 16:28:46,637:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 67 iterations, i.e. alpha=9.516e-02, with an active set of 49 regressors, and the smallest cholesky pivot element being 7.885e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:28:46,637:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 67 iterations, i.e. alpha=7.627e-02, with an active set of 49 regressors, and the smallest cholesky pivot element being 8.093e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:28:46,639:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 76 iterations, i.e. alpha=3.022e+00, with an active set of 51 regressors, and the smallest cholesky pivot element being 8.025e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:28:46,640:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 77 iterations, i.e. alpha=4.667e-01, with an active set of 52 regressors, and the smallest cholesky pivot element being 7.068e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:28:46,640:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 77 iterations, i.e. alpha=3.862e-01, with an active set of 52 regressors, and the smallest cholesky pivot element being 7.885e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:28:46,640:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 77 iterations, i.e. alpha=2.137e-02, with an active set of 52 regressors, and the smallest cholesky pivot element being 6.989e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:28:46,654:INFO:Calculating mean and std
2022-11-07 16:28:46,657:INFO:Creating metrics dataframe
2022-11-07 16:28:46,667:INFO:Uploading results into container
2022-11-07 16:28:46,668:INFO:Uploading model into container now
2022-11-07 16:28:46,669:INFO:master_model_container: 5
2022-11-07 16:28:46,669:INFO:display_container: 2
2022-11-07 16:28:46,670:INFO:Lars(random_state=7766)
2022-11-07 16:28:46,671:INFO:create_model() successfully completed......................................
2022-11-07 16:28:46,818:INFO:SubProcess create_model() end ==================================
2022-11-07 16:28:46,819:INFO:Creating metrics dataframe
2022-11-07 16:28:46,840:INFO:Initializing Lasso Least Angle Regression
2022-11-07 16:28:46,841:INFO:Total runtime is 0.18413676420847572 minutes
2022-11-07 16:28:46,851:INFO:SubProcess create_model() called ==================================
2022-11-07 16:28:46,856:INFO:Initializing create_model()
2022-11-07 16:28:46,856:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f20c26a8f50>, estimator=llar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f20c2532790>, model_only=True, return_train_score=False, kwargs={})
2022-11-07 16:28:46,858:INFO:Checking exceptions
2022-11-07 16:28:46,861:INFO:Importing libraries
2022-11-07 16:28:46,863:INFO:Copying training dataset
2022-11-07 16:28:46,870:INFO:Defining folds
2022-11-07 16:28:46,870:INFO:Declaring metric variables
2022-11-07 16:28:46,879:INFO:Importing untrained model
2022-11-07 16:28:46,890:INFO:Lasso Least Angle Regression Imported successfully
2022-11-07 16:28:46,909:INFO:Starting cross validation
2022-11-07 16:28:46,912:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-11-07 16:28:46,957:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  FutureWarning,

2022-11-07 16:28:46,988:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  FutureWarning,

2022-11-07 16:28:47,037:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  FutureWarning,

2022-11-07 16:28:47,090:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  FutureWarning,

2022-11-07 16:28:47,096:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  FutureWarning,

2022-11-07 16:28:47,135:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  FutureWarning,

2022-11-07 16:28:47,170:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  FutureWarning,

2022-11-07 16:28:47,183:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  FutureWarning,

2022-11-07 16:28:47,216:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  FutureWarning,

2022-11-07 16:28:47,230:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  FutureWarning,

2022-11-07 16:28:47,249:INFO:Calculating mean and std
2022-11-07 16:28:47,251:INFO:Creating metrics dataframe
2022-11-07 16:28:47,260:INFO:Uploading results into container
2022-11-07 16:28:47,261:INFO:Uploading model into container now
2022-11-07 16:28:47,262:INFO:master_model_container: 6
2022-11-07 16:28:47,262:INFO:display_container: 2
2022-11-07 16:28:47,263:INFO:LassoLars(random_state=7766)
2022-11-07 16:28:47,263:INFO:create_model() successfully completed......................................
2022-11-07 16:28:47,410:INFO:SubProcess create_model() end ==================================
2022-11-07 16:28:47,411:INFO:Creating metrics dataframe
2022-11-07 16:28:47,432:INFO:Initializing Orthogonal Matching Pursuit
2022-11-07 16:28:47,432:INFO:Total runtime is 0.1939974228541056 minutes
2022-11-07 16:28:47,442:INFO:SubProcess create_model() called ==================================
2022-11-07 16:28:47,443:INFO:Initializing create_model()
2022-11-07 16:28:47,443:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f20c26a8f50>, estimator=omp, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f20c2532790>, model_only=True, return_train_score=False, kwargs={})
2022-11-07 16:28:47,443:INFO:Checking exceptions
2022-11-07 16:28:47,447:INFO:Importing libraries
2022-11-07 16:28:47,447:INFO:Copying training dataset
2022-11-07 16:28:47,452:INFO:Defining folds
2022-11-07 16:28:47,453:INFO:Declaring metric variables
2022-11-07 16:28:47,466:INFO:Importing untrained model
2022-11-07 16:28:47,477:INFO:Orthogonal Matching Pursuit Imported successfully
2022-11-07 16:28:47,494:INFO:Starting cross validation
2022-11-07 16:28:47,498:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-11-07 16:28:47,545:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2022-11-07 16:28:47,575:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2022-11-07 16:28:47,614:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2022-11-07 16:28:47,649:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2022-11-07 16:28:47,692:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2022-11-07 16:28:47,736:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2022-11-07 16:28:47,736:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2022-11-07 16:28:47,789:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2022-11-07 16:28:47,792:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2022-11-07 16:28:47,829:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2022-11-07 16:28:47,845:INFO:Calculating mean and std
2022-11-07 16:28:47,848:INFO:Creating metrics dataframe
2022-11-07 16:28:47,859:INFO:Uploading results into container
2022-11-07 16:28:47,860:INFO:Uploading model into container now
2022-11-07 16:28:47,861:INFO:master_model_container: 7
2022-11-07 16:28:47,861:INFO:display_container: 2
2022-11-07 16:28:47,862:INFO:OrthogonalMatchingPursuit()
2022-11-07 16:28:47,862:INFO:create_model() successfully completed......................................
2022-11-07 16:28:48,006:INFO:SubProcess create_model() end ==================================
2022-11-07 16:28:48,006:INFO:Creating metrics dataframe
2022-11-07 16:28:48,029:INFO:Initializing Bayesian Ridge
2022-11-07 16:28:48,029:INFO:Total runtime is 0.20394152005513505 minutes
2022-11-07 16:28:48,041:INFO:SubProcess create_model() called ==================================
2022-11-07 16:28:48,042:INFO:Initializing create_model()
2022-11-07 16:28:48,042:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f20c26a8f50>, estimator=br, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f20c2532790>, model_only=True, return_train_score=False, kwargs={})
2022-11-07 16:28:48,042:INFO:Checking exceptions
2022-11-07 16:28:48,047:INFO:Importing libraries
2022-11-07 16:28:48,048:INFO:Copying training dataset
2022-11-07 16:28:48,056:INFO:Defining folds
2022-11-07 16:28:48,056:INFO:Declaring metric variables
2022-11-07 16:28:48,066:INFO:Importing untrained model
2022-11-07 16:28:48,075:INFO:Bayesian Ridge Imported successfully
2022-11-07 16:28:48,094:INFO:Starting cross validation
2022-11-07 16:28:48,096:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-11-07 16:28:48,458:INFO:Calculating mean and std
2022-11-07 16:28:48,462:INFO:Creating metrics dataframe
2022-11-07 16:28:48,475:INFO:Uploading results into container
2022-11-07 16:28:48,475:INFO:Uploading model into container now
2022-11-07 16:28:48,476:INFO:master_model_container: 8
2022-11-07 16:28:48,476:INFO:display_container: 2
2022-11-07 16:28:48,476:INFO:BayesianRidge()
2022-11-07 16:28:48,476:INFO:create_model() successfully completed......................................
2022-11-07 16:28:48,617:INFO:SubProcess create_model() end ==================================
2022-11-07 16:28:48,618:INFO:Creating metrics dataframe
2022-11-07 16:28:48,640:INFO:Initializing Passive Aggressive Regressor
2022-11-07 16:28:48,640:INFO:Total runtime is 0.21413391828536985 minutes
2022-11-07 16:28:48,652:INFO:SubProcess create_model() called ==================================
2022-11-07 16:28:48,656:INFO:Initializing create_model()
2022-11-07 16:28:48,656:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f20c26a8f50>, estimator=par, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f20c2532790>, model_only=True, return_train_score=False, kwargs={})
2022-11-07 16:28:48,656:INFO:Checking exceptions
2022-11-07 16:28:48,659:INFO:Importing libraries
2022-11-07 16:28:48,659:INFO:Copying training dataset
2022-11-07 16:28:48,666:INFO:Defining folds
2022-11-07 16:28:48,666:INFO:Declaring metric variables
2022-11-07 16:28:48,675:INFO:Importing untrained model
2022-11-07 16:28:48,685:INFO:Passive Aggressive Regressor Imported successfully
2022-11-07 16:28:48,700:INFO:Starting cross validation
2022-11-07 16:28:48,703:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-11-07 16:28:49,072:INFO:Calculating mean and std
2022-11-07 16:28:49,075:INFO:Creating metrics dataframe
2022-11-07 16:28:49,086:INFO:Uploading results into container
2022-11-07 16:28:49,089:INFO:Uploading model into container now
2022-11-07 16:28:49,089:INFO:master_model_container: 9
2022-11-07 16:28:49,090:INFO:display_container: 2
2022-11-07 16:28:49,090:INFO:PassiveAggressiveRegressor(random_state=7766)
2022-11-07 16:28:49,090:INFO:create_model() successfully completed......................................
2022-11-07 16:28:49,229:INFO:SubProcess create_model() end ==================================
2022-11-07 16:28:49,230:INFO:Creating metrics dataframe
2022-11-07 16:28:49,253:INFO:Initializing Huber Regressor
2022-11-07 16:28:49,253:INFO:Total runtime is 0.22434648672739663 minutes
2022-11-07 16:28:49,263:INFO:SubProcess create_model() called ==================================
2022-11-07 16:28:49,264:INFO:Initializing create_model()
2022-11-07 16:28:49,264:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f20c26a8f50>, estimator=huber, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f20c2532790>, model_only=True, return_train_score=False, kwargs={})
2022-11-07 16:28:49,264:INFO:Checking exceptions
2022-11-07 16:28:49,268:INFO:Importing libraries
2022-11-07 16:28:49,269:INFO:Copying training dataset
2022-11-07 16:28:49,274:INFO:Defining folds
2022-11-07 16:28:49,275:INFO:Declaring metric variables
2022-11-07 16:28:49,287:INFO:Importing untrained model
2022-11-07 16:28:49,300:INFO:Huber Regressor Imported successfully
2022-11-07 16:28:49,316:INFO:Starting cross validation
2022-11-07 16:28:49,319:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-11-07 16:28:49,413:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2022-11-07 16:28:49,504:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2022-11-07 16:28:49,526:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2022-11-07 16:28:49,630:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2022-11-07 16:28:49,652:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2022-11-07 16:28:49,724:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2022-11-07 16:28:49,775:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2022-11-07 16:28:49,831:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2022-11-07 16:28:49,877:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2022-11-07 16:28:49,923:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2022-11-07 16:28:49,944:INFO:Calculating mean and std
2022-11-07 16:28:49,947:INFO:Creating metrics dataframe
2022-11-07 16:28:49,956:INFO:Uploading results into container
2022-11-07 16:28:49,957:INFO:Uploading model into container now
2022-11-07 16:28:49,958:INFO:master_model_container: 10
2022-11-07 16:28:49,958:INFO:display_container: 2
2022-11-07 16:28:49,959:INFO:HuberRegressor()
2022-11-07 16:28:49,959:INFO:create_model() successfully completed......................................
2022-11-07 16:28:50,098:INFO:SubProcess create_model() end ==================================
2022-11-07 16:28:50,098:INFO:Creating metrics dataframe
2022-11-07 16:28:50,123:INFO:Initializing K Neighbors Regressor
2022-11-07 16:28:50,123:INFO:Total runtime is 0.2388426582018534 minutes
2022-11-07 16:28:50,132:INFO:SubProcess create_model() called ==================================
2022-11-07 16:28:50,132:INFO:Initializing create_model()
2022-11-07 16:28:50,133:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f20c26a8f50>, estimator=knn, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f20c2532790>, model_only=True, return_train_score=False, kwargs={})
2022-11-07 16:28:50,133:INFO:Checking exceptions
2022-11-07 16:28:50,137:INFO:Importing libraries
2022-11-07 16:28:50,137:INFO:Copying training dataset
2022-11-07 16:28:50,144:INFO:Defining folds
2022-11-07 16:28:50,144:INFO:Declaring metric variables
2022-11-07 16:28:50,155:INFO:Importing untrained model
2022-11-07 16:28:50,167:INFO:K Neighbors Regressor Imported successfully
2022-11-07 16:28:50,186:INFO:Starting cross validation
2022-11-07 16:28:50,193:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-11-07 16:28:50,993:INFO:Calculating mean and std
2022-11-07 16:28:50,996:INFO:Creating metrics dataframe
2022-11-07 16:28:51,006:INFO:Uploading results into container
2022-11-07 16:28:51,013:INFO:Uploading model into container now
2022-11-07 16:28:51,013:INFO:master_model_container: 11
2022-11-07 16:28:51,014:INFO:display_container: 2
2022-11-07 16:28:51,014:INFO:KNeighborsRegressor(n_jobs=-1)
2022-11-07 16:28:51,014:INFO:create_model() successfully completed......................................
2022-11-07 16:28:51,156:INFO:SubProcess create_model() end ==================================
2022-11-07 16:28:51,157:INFO:Creating metrics dataframe
2022-11-07 16:28:51,181:INFO:Initializing Decision Tree Regressor
2022-11-07 16:28:51,182:INFO:Total runtime is 0.2564898530642191 minutes
2022-11-07 16:28:51,192:INFO:SubProcess create_model() called ==================================
2022-11-07 16:28:51,193:INFO:Initializing create_model()
2022-11-07 16:28:51,193:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f20c26a8f50>, estimator=dt, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f20c2532790>, model_only=True, return_train_score=False, kwargs={})
2022-11-07 16:28:51,193:INFO:Checking exceptions
2022-11-07 16:28:51,197:INFO:Importing libraries
2022-11-07 16:28:51,198:INFO:Copying training dataset
2022-11-07 16:28:51,203:INFO:Defining folds
2022-11-07 16:28:51,204:INFO:Declaring metric variables
2022-11-07 16:28:51,218:INFO:Importing untrained model
2022-11-07 16:28:51,230:INFO:Decision Tree Regressor Imported successfully
2022-11-07 16:28:51,249:INFO:Starting cross validation
2022-11-07 16:28:51,251:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-11-07 16:28:51,619:INFO:Calculating mean and std
2022-11-07 16:28:51,621:INFO:Creating metrics dataframe
2022-11-07 16:28:51,635:INFO:Uploading results into container
2022-11-07 16:28:51,636:INFO:Uploading model into container now
2022-11-07 16:28:51,637:INFO:master_model_container: 12
2022-11-07 16:28:51,637:INFO:display_container: 2
2022-11-07 16:28:51,638:INFO:DecisionTreeRegressor(random_state=7766)
2022-11-07 16:28:51,638:INFO:create_model() successfully completed......................................
2022-11-07 16:28:51,775:INFO:SubProcess create_model() end ==================================
2022-11-07 16:28:51,775:INFO:Creating metrics dataframe
2022-11-07 16:28:51,796:INFO:Initializing Random Forest Regressor
2022-11-07 16:28:51,797:INFO:Total runtime is 0.26673815250396726 minutes
2022-11-07 16:28:51,804:INFO:SubProcess create_model() called ==================================
2022-11-07 16:28:51,806:INFO:Initializing create_model()
2022-11-07 16:28:51,807:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f20c26a8f50>, estimator=rf, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f20c2532790>, model_only=True, return_train_score=False, kwargs={})
2022-11-07 16:28:51,807:INFO:Checking exceptions
2022-11-07 16:28:51,809:INFO:Importing libraries
2022-11-07 16:28:51,809:INFO:Copying training dataset
2022-11-07 16:28:51,815:INFO:Defining folds
2022-11-07 16:28:51,815:INFO:Declaring metric variables
2022-11-07 16:28:51,825:INFO:Importing untrained model
2022-11-07 16:28:51,838:INFO:Random Forest Regressor Imported successfully
2022-11-07 16:28:51,855:INFO:Starting cross validation
2022-11-07 16:28:51,857:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-11-07 16:28:55,422:INFO:Calculating mean and std
2022-11-07 16:28:55,426:INFO:Creating metrics dataframe
2022-11-07 16:28:55,436:INFO:Uploading results into container
2022-11-07 16:28:55,437:INFO:Uploading model into container now
2022-11-07 16:28:55,438:INFO:master_model_container: 13
2022-11-07 16:28:55,438:INFO:display_container: 2
2022-11-07 16:28:55,439:INFO:RandomForestRegressor(n_jobs=-1, random_state=7766)
2022-11-07 16:28:55,439:INFO:create_model() successfully completed......................................
2022-11-07 16:28:55,579:INFO:SubProcess create_model() end ==================================
2022-11-07 16:28:55,579:INFO:Creating metrics dataframe
2022-11-07 16:28:55,601:INFO:Initializing Extra Trees Regressor
2022-11-07 16:28:55,602:INFO:Total runtime is 0.33015277385711667 minutes
2022-11-07 16:28:55,610:INFO:SubProcess create_model() called ==================================
2022-11-07 16:28:55,612:INFO:Initializing create_model()
2022-11-07 16:28:55,612:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f20c26a8f50>, estimator=et, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f20c2532790>, model_only=True, return_train_score=False, kwargs={})
2022-11-07 16:28:55,612:INFO:Checking exceptions
2022-11-07 16:28:55,616:INFO:Importing libraries
2022-11-07 16:28:55,616:INFO:Copying training dataset
2022-11-07 16:28:55,624:INFO:Defining folds
2022-11-07 16:28:55,624:INFO:Declaring metric variables
2022-11-07 16:28:55,636:INFO:Importing untrained model
2022-11-07 16:28:55,645:INFO:Extra Trees Regressor Imported successfully
2022-11-07 16:28:55,663:INFO:Starting cross validation
2022-11-07 16:28:55,666:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-11-07 16:28:58,215:INFO:Calculating mean and std
2022-11-07 16:28:58,220:INFO:Creating metrics dataframe
2022-11-07 16:28:58,230:INFO:Uploading results into container
2022-11-07 16:28:58,231:INFO:Uploading model into container now
2022-11-07 16:28:58,232:INFO:master_model_container: 14
2022-11-07 16:28:58,232:INFO:display_container: 2
2022-11-07 16:28:58,233:INFO:ExtraTreesRegressor(n_jobs=-1, random_state=7766)
2022-11-07 16:28:58,233:INFO:create_model() successfully completed......................................
2022-11-07 16:28:58,374:INFO:SubProcess create_model() end ==================================
2022-11-07 16:28:58,375:INFO:Creating metrics dataframe
2022-11-07 16:28:58,399:INFO:Initializing AdaBoost Regressor
2022-11-07 16:28:58,400:INFO:Total runtime is 0.376795220375061 minutes
2022-11-07 16:28:58,415:INFO:SubProcess create_model() called ==================================
2022-11-07 16:28:58,416:INFO:Initializing create_model()
2022-11-07 16:28:58,416:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f20c26a8f50>, estimator=ada, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f20c2532790>, model_only=True, return_train_score=False, kwargs={})
2022-11-07 16:28:58,416:INFO:Checking exceptions
2022-11-07 16:28:58,420:INFO:Importing libraries
2022-11-07 16:28:58,420:INFO:Copying training dataset
2022-11-07 16:28:58,424:INFO:Defining folds
2022-11-07 16:28:58,427:INFO:Declaring metric variables
2022-11-07 16:28:58,438:INFO:Importing untrained model
2022-11-07 16:28:58,445:INFO:AdaBoost Regressor Imported successfully
2022-11-07 16:28:58,461:INFO:Starting cross validation
2022-11-07 16:28:58,463:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-11-07 16:28:59,779:INFO:Calculating mean and std
2022-11-07 16:28:59,782:INFO:Creating metrics dataframe
2022-11-07 16:28:59,793:INFO:Uploading results into container
2022-11-07 16:28:59,794:INFO:Uploading model into container now
2022-11-07 16:28:59,795:INFO:master_model_container: 15
2022-11-07 16:28:59,795:INFO:display_container: 2
2022-11-07 16:28:59,795:INFO:AdaBoostRegressor(random_state=7766)
2022-11-07 16:28:59,795:INFO:create_model() successfully completed......................................
2022-11-07 16:28:59,936:INFO:SubProcess create_model() end ==================================
2022-11-07 16:28:59,937:INFO:Creating metrics dataframe
2022-11-07 16:28:59,963:INFO:Initializing Gradient Boosting Regressor
2022-11-07 16:28:59,963:INFO:Total runtime is 0.4028480569521586 minutes
2022-11-07 16:28:59,972:INFO:SubProcess create_model() called ==================================
2022-11-07 16:28:59,973:INFO:Initializing create_model()
2022-11-07 16:28:59,974:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f20c26a8f50>, estimator=gbr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f20c2532790>, model_only=True, return_train_score=False, kwargs={})
2022-11-07 16:28:59,974:INFO:Checking exceptions
2022-11-07 16:28:59,977:INFO:Importing libraries
2022-11-07 16:28:59,978:INFO:Copying training dataset
2022-11-07 16:28:59,983:INFO:Defining folds
2022-11-07 16:28:59,984:INFO:Declaring metric variables
2022-11-07 16:28:59,998:INFO:Importing untrained model
2022-11-07 16:29:00,007:INFO:Gradient Boosting Regressor Imported successfully
2022-11-07 16:29:00,024:INFO:Starting cross validation
2022-11-07 16:29:00,027:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-11-07 16:29:01,727:INFO:Calculating mean and std
2022-11-07 16:29:01,734:INFO:Creating metrics dataframe
2022-11-07 16:29:01,744:INFO:Uploading results into container
2022-11-07 16:29:01,745:INFO:Uploading model into container now
2022-11-07 16:29:01,745:INFO:master_model_container: 16
2022-11-07 16:29:01,745:INFO:display_container: 2
2022-11-07 16:29:01,746:INFO:GradientBoostingRegressor(random_state=7766)
2022-11-07 16:29:01,746:INFO:create_model() successfully completed......................................
2022-11-07 16:29:01,889:INFO:SubProcess create_model() end ==================================
2022-11-07 16:29:01,890:INFO:Creating metrics dataframe
2022-11-07 16:29:01,922:INFO:Initializing Light Gradient Boosting Machine
2022-11-07 16:29:01,922:INFO:Total runtime is 0.43549657265345254 minutes
2022-11-07 16:29:01,930:INFO:SubProcess create_model() called ==================================
2022-11-07 16:29:01,931:INFO:Initializing create_model()
2022-11-07 16:29:01,931:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f20c26a8f50>, estimator=lightgbm, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f20c2532790>, model_only=True, return_train_score=False, kwargs={})
2022-11-07 16:29:01,931:INFO:Checking exceptions
2022-11-07 16:29:01,934:INFO:Importing libraries
2022-11-07 16:29:01,934:INFO:Copying training dataset
2022-11-07 16:29:01,942:INFO:Defining folds
2022-11-07 16:29:01,943:INFO:Declaring metric variables
2022-11-07 16:29:01,954:INFO:Importing untrained model
2022-11-07 16:29:01,962:INFO:Light Gradient Boosting Machine Imported successfully
2022-11-07 16:29:01,980:INFO:Starting cross validation
2022-11-07 16:29:01,982:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-11-07 16:29:03,263:INFO:Calculating mean and std
2022-11-07 16:29:03,266:INFO:Creating metrics dataframe
2022-11-07 16:29:03,276:INFO:Uploading results into container
2022-11-07 16:29:03,281:INFO:Uploading model into container now
2022-11-07 16:29:03,282:INFO:master_model_container: 17
2022-11-07 16:29:03,282:INFO:display_container: 2
2022-11-07 16:29:03,283:INFO:LGBMRegressor(random_state=7766)
2022-11-07 16:29:03,283:INFO:create_model() successfully completed......................................
2022-11-07 16:29:03,427:INFO:SubProcess create_model() end ==================================
2022-11-07 16:29:03,428:INFO:Creating metrics dataframe
2022-11-07 16:29:03,452:INFO:Initializing Dummy Regressor
2022-11-07 16:29:03,452:INFO:Total runtime is 0.4610008080800374 minutes
2022-11-07 16:29:03,461:INFO:SubProcess create_model() called ==================================
2022-11-07 16:29:03,462:INFO:Initializing create_model()
2022-11-07 16:29:03,462:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f20c26a8f50>, estimator=dummy, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f20c2532790>, model_only=True, return_train_score=False, kwargs={})
2022-11-07 16:29:03,463:INFO:Checking exceptions
2022-11-07 16:29:03,466:INFO:Importing libraries
2022-11-07 16:29:03,466:INFO:Copying training dataset
2022-11-07 16:29:03,474:INFO:Defining folds
2022-11-07 16:29:03,474:INFO:Declaring metric variables
2022-11-07 16:29:03,483:INFO:Importing untrained model
2022-11-07 16:29:03,492:INFO:Dummy Regressor Imported successfully
2022-11-07 16:29:03,508:INFO:Starting cross validation
2022-11-07 16:29:03,511:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-11-07 16:29:03,816:INFO:Calculating mean and std
2022-11-07 16:29:03,818:INFO:Creating metrics dataframe
2022-11-07 16:29:03,824:INFO:Uploading results into container
2022-11-07 16:29:03,827:INFO:Uploading model into container now
2022-11-07 16:29:03,830:INFO:master_model_container: 18
2022-11-07 16:29:03,830:INFO:display_container: 2
2022-11-07 16:29:03,831:INFO:DummyRegressor()
2022-11-07 16:29:03,831:INFO:create_model() successfully completed......................................
2022-11-07 16:29:03,972:INFO:SubProcess create_model() end ==================================
2022-11-07 16:29:03,972:INFO:Creating metrics dataframe
2022-11-07 16:29:04,025:INFO:Initializing create_model()
2022-11-07 16:29:04,028:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f20c26a8f50>, estimator=BayesianRidge(), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2022-11-07 16:29:04,028:INFO:Checking exceptions
2022-11-07 16:29:04,037:INFO:Importing libraries
2022-11-07 16:29:04,038:INFO:Copying training dataset
2022-11-07 16:29:04,043:INFO:Defining folds
2022-11-07 16:29:04,043:INFO:Declaring metric variables
2022-11-07 16:29:04,044:INFO:Importing untrained model
2022-11-07 16:29:04,044:INFO:Declaring custom model
2022-11-07 16:29:04,045:INFO:Bayesian Ridge Imported successfully
2022-11-07 16:29:04,046:INFO:Cross validation set to False
2022-11-07 16:29:04,047:INFO:Fitting Model
2022-11-07 16:29:04,106:INFO:BayesianRidge()
2022-11-07 16:29:04,107:INFO:create_model() successfully completed......................................
2022-11-07 16:29:04,370:INFO:master_model_container: 18
2022-11-07 16:29:04,371:INFO:display_container: 2
2022-11-07 16:29:04,371:INFO:BayesianRidge()
2022-11-07 16:29:04,372:INFO:compare_models() successfully completed......................................
2022-11-07 16:44:05,939:INFO:PyCaret RegressionExperiment
2022-11-07 16:44:05,941:INFO:Logging name: reg-default-name
2022-11-07 16:44:05,941:INFO:ML Usecase: MLUsecase.REGRESSION
2022-11-07 16:44:05,945:INFO:version 3.0.0.rc4
2022-11-07 16:44:05,945:INFO:Initializing setup()
2022-11-07 16:44:05,946:INFO:self.USI: e6dc
2022-11-07 16:44:05,946:INFO:self.variable_keys: {'display_container', 'gpu_param', '_all_models', 'log_plots_param', 'fold_shuffle_param', 'fold_groups_param', 'memory', 'data', 'y_test', 'target_param', 'fold_generator', 'html_param', '_gpu_n_jobs_param', 'exp_id', 'logging_param', '_all_models_internal', 'n_jobs_param', 'variable_keys', 'idx', 'transform_target_method_param', 'exp_name_log', 'y', 'X', '_ml_usecase', 'seed', 'X_test', 'y_train', 'pipeline', '_available_plots', 'USI', 'master_model_container', 'X_train', 'transform_target_param', '_all_metrics'}
2022-11-07 16:44:05,949:INFO:Checking environment
2022-11-07 16:44:05,950:INFO:python_version: 3.7.15
2022-11-07 16:44:05,950:INFO:python_build: ('default', 'Oct 12 2022 19:14:55')
2022-11-07 16:44:05,950:INFO:machine: x86_64
2022-11-07 16:44:05,951:INFO:platform: Linux-5.10.133+-x86_64-with-Ubuntu-18.04-bionic
2022-11-07 16:44:05,951:INFO:Memory: svmem(total=13616353280, available=12208046080, percent=10.3, used=1192087552, free=9078362112, active=868548608, inactive=3321675776, buffers=426000384, cached=2919903232, shared=1331200, slab=262881280)
2022-11-07 16:44:05,952:INFO:Physical Core: 1
2022-11-07 16:44:05,952:INFO:Logical Core: 2
2022-11-07 16:44:05,952:INFO:Checking libraries
2022-11-07 16:44:05,955:INFO:System:
2022-11-07 16:44:05,956:INFO:    python: 3.7.15 (default, Oct 12 2022, 19:14:55)  [GCC 7.5.0]
2022-11-07 16:44:05,956:INFO:executable: /usr/bin/python3
2022-11-07 16:44:05,956:INFO:   machine: Linux-5.10.133+-x86_64-with-Ubuntu-18.04-bionic
2022-11-07 16:44:05,956:INFO:PyCaret required dependencies:
2022-11-07 16:44:05,957:INFO:                 pip: 21.1.3
2022-11-07 16:44:05,957:INFO:          setuptools: 57.4.0
2022-11-07 16:44:05,957:INFO:             pycaret: 3.0.0rc4
2022-11-07 16:44:05,957:INFO:             IPython: 7.9.0
2022-11-07 16:44:05,957:INFO:          ipywidgets: 7.7.1
2022-11-07 16:44:05,957:INFO:                tqdm: 4.64.1
2022-11-07 16:44:05,958:INFO:               numpy: 1.21.6
2022-11-07 16:44:05,958:INFO:              pandas: 1.3.5
2022-11-07 16:44:05,958:INFO:              jinja2: 2.11.3
2022-11-07 16:44:05,958:INFO:               scipy: 1.7.3
2022-11-07 16:44:05,958:INFO:              joblib: 1.2.0
2022-11-07 16:44:05,958:INFO:             sklearn: 1.0.2
2022-11-07 16:44:05,958:INFO:                pyod: 1.0.6
2022-11-07 16:44:05,958:INFO:            imblearn: 0.8.1
2022-11-07 16:44:05,959:INFO:   category_encoders: 2.5.1.post0
2022-11-07 16:44:05,959:INFO:            lightgbm: 3.3.3
2022-11-07 16:44:05,959:INFO:               numba: 0.55.2
2022-11-07 16:44:05,960:INFO:            requests: 2.28.1
2022-11-07 16:44:05,960:INFO:          matplotlib: 3.5.3
2022-11-07 16:44:05,960:INFO:          scikitplot: 0.3.7
2022-11-07 16:44:05,962:INFO:         yellowbrick: 1.5
2022-11-07 16:44:05,962:INFO:              plotly: 5.5.0
2022-11-07 16:44:05,962:INFO:             kaleido: 0.2.1
2022-11-07 16:44:05,963:INFO:         statsmodels: 0.12.2
2022-11-07 16:44:05,963:INFO:              sktime: 0.13.4
2022-11-07 16:44:05,963:INFO:               tbats: 1.1.1
2022-11-07 16:44:05,963:INFO:            pmdarima: 1.8.5
2022-11-07 16:44:05,963:INFO:              psutil: 5.9.3
2022-11-07 16:44:05,963:INFO:PyCaret optional dependencies:
2022-11-07 16:44:05,963:INFO:                shap: Not installed
2022-11-07 16:44:05,964:INFO:           interpret: Not installed
2022-11-07 16:44:05,966:INFO:                umap: Not installed
2022-11-07 16:44:05,966:INFO:    pandas_profiling: 1.4.1
2022-11-07 16:44:05,966:INFO:  explainerdashboard: Not installed
2022-11-07 16:44:05,967:INFO:             autoviz: Not installed
2022-11-07 16:44:05,967:INFO:           fairlearn: Not installed
2022-11-07 16:44:05,967:INFO:             xgboost: 0.90
2022-11-07 16:44:05,967:INFO:            catboost: Not installed
2022-11-07 16:44:05,967:INFO:              kmodes: Not installed
2022-11-07 16:44:05,967:INFO:             mlxtend: 0.14.0
2022-11-07 16:44:05,967:INFO:       statsforecast: Not installed
2022-11-07 16:44:05,968:INFO:        tune_sklearn: Not installed
2022-11-07 16:44:05,968:INFO:                 ray: Not installed
2022-11-07 16:44:05,969:INFO:            hyperopt: 0.1.2
2022-11-07 16:44:05,969:INFO:              optuna: Not installed
2022-11-07 16:44:05,969:INFO:               skopt: Not installed
2022-11-07 16:44:05,969:INFO:              mlflow: Not installed
2022-11-07 16:44:05,971:INFO:              gradio: Not installed
2022-11-07 16:44:05,971:INFO:             fastapi: Not installed
2022-11-07 16:44:05,971:INFO:             uvicorn: Not installed
2022-11-07 16:44:05,972:INFO:              m2cgen: Not installed
2022-11-07 16:44:05,972:INFO:           evidently: Not installed
2022-11-07 16:44:05,972:INFO:                nltk: 3.7
2022-11-07 16:44:05,973:INFO:            pyLDAvis: Not installed
2022-11-07 16:44:05,973:INFO:              gensim: 3.6.0
2022-11-07 16:44:05,974:INFO:               spacy: 3.4.2
2022-11-07 16:44:05,974:INFO:           wordcloud: 1.8.2.2
2022-11-07 16:44:05,974:INFO:            textblob: 0.15.3
2022-11-07 16:44:05,975:INFO:               fugue: Not installed
2022-11-07 16:44:05,976:INFO:           streamlit: Not installed
2022-11-07 16:44:05,976:INFO:             prophet: 1.1.1
2022-11-07 16:44:05,978:INFO:None
2022-11-07 16:44:05,978:INFO:Set up data.
2022-11-07 16:44:06,065:INFO:Set up train/test split.
2022-11-07 16:44:06,086:INFO:Set up index.
2022-11-07 16:44:06,088:INFO:Set up folding strategy.
2022-11-07 16:44:06,088:INFO:Assigning column types.
2022-11-07 16:44:06,102:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2022-11-07 16:44:06,103:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2022-11-07 16:44:06,114:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2022-11-07 16:44:06,132:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2022-11-07 16:44:06,332:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2022-11-07 16:44:06,481:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2022-11-07 16:44:06,483:INFO:Soft dependency imported: xgboost: 0.90
2022-11-07 16:44:06,483:WARNING:Wrong xgboost version. Expected xgboost>=1.1.0, got xgboost==0.90
2022-11-07 16:44:06,484:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-11-07 16:44:06,485:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2022-11-07 16:44:06,496:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2022-11-07 16:44:06,508:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2022-11-07 16:44:06,705:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2022-11-07 16:44:06,808:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2022-11-07 16:44:06,810:INFO:Soft dependency imported: xgboost: 0.90
2022-11-07 16:44:06,810:WARNING:Wrong xgboost version. Expected xgboost>=1.1.0, got xgboost==0.90
2022-11-07 16:44:06,811:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-11-07 16:44:06,812:INFO:Engine successfully changes for model 'lasso' to 'sklearn'.
2022-11-07 16:44:06,823:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2022-11-07 16:44:06,834:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2022-11-07 16:44:07,072:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2022-11-07 16:44:07,192:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2022-11-07 16:44:07,194:INFO:Soft dependency imported: xgboost: 0.90
2022-11-07 16:44:07,195:WARNING:Wrong xgboost version. Expected xgboost>=1.1.0, got xgboost==0.90
2022-11-07 16:44:07,195:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-11-07 16:44:07,207:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2022-11-07 16:44:07,219:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2022-11-07 16:44:07,466:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2022-11-07 16:44:07,618:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2022-11-07 16:44:07,619:INFO:Soft dependency imported: xgboost: 0.90
2022-11-07 16:44:07,620:WARNING:Wrong xgboost version. Expected xgboost>=1.1.0, got xgboost==0.90
2022-11-07 16:44:07,621:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-11-07 16:44:07,621:INFO:Engine successfully changes for model 'ridge' to 'sklearn'.
2022-11-07 16:44:07,643:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2022-11-07 16:44:07,793:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2022-11-07 16:44:07,907:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2022-11-07 16:44:07,909:INFO:Soft dependency imported: xgboost: 0.90
2022-11-07 16:44:07,909:WARNING:Wrong xgboost version. Expected xgboost>=1.1.0, got xgboost==0.90
2022-11-07 16:44:07,910:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-11-07 16:44:07,932:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2022-11-07 16:44:08,061:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2022-11-07 16:44:08,259:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2022-11-07 16:44:08,261:INFO:Soft dependency imported: xgboost: 0.90
2022-11-07 16:44:08,262:WARNING:Wrong xgboost version. Expected xgboost>=1.1.0, got xgboost==0.90
2022-11-07 16:44:08,262:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-11-07 16:44:08,263:INFO:Engine successfully changes for model 'en' to 'sklearn'.
2022-11-07 16:44:08,484:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2022-11-07 16:44:08,599:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2022-11-07 16:44:08,600:INFO:Soft dependency imported: xgboost: 0.90
2022-11-07 16:44:08,601:WARNING:Wrong xgboost version. Expected xgboost>=1.1.0, got xgboost==0.90
2022-11-07 16:44:08,602:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-11-07 16:44:08,799:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2022-11-07 16:44:08,957:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2022-11-07 16:44:08,960:INFO:Soft dependency imported: xgboost: 0.90
2022-11-07 16:44:08,964:WARNING:Wrong xgboost version. Expected xgboost>=1.1.0, got xgboost==0.90
2022-11-07 16:44:08,970:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-11-07 16:44:08,971:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2022-11-07 16:44:09,303:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2022-11-07 16:44:09,499:INFO:Soft dependency imported: xgboost: 0.90
2022-11-07 16:44:09,504:WARNING:Wrong xgboost version. Expected xgboost>=1.1.0, got xgboost==0.90
2022-11-07 16:44:09,505:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-11-07 16:44:09,683:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2022-11-07 16:44:09,956:INFO:Soft dependency imported: xgboost: 0.90
2022-11-07 16:44:09,962:WARNING:Wrong xgboost version. Expected xgboost>=1.1.0, got xgboost==0.90
2022-11-07 16:44:09,963:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-11-07 16:44:09,964:INFO:Engine successfully changes for model 'svm' to 'sklearn'.
2022-11-07 16:44:10,258:INFO:Soft dependency imported: xgboost: 0.90
2022-11-07 16:44:10,259:WARNING:Wrong xgboost version. Expected xgboost>=1.1.0, got xgboost==0.90
2022-11-07 16:44:10,259:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-11-07 16:44:10,703:INFO:Soft dependency imported: xgboost: 0.90
2022-11-07 16:44:10,704:WARNING:Wrong xgboost version. Expected xgboost>=1.1.0, got xgboost==0.90
2022-11-07 16:44:10,704:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-11-07 16:44:10,706:INFO:Preparing preprocessing pipeline...
2022-11-07 16:44:10,708:INFO:Set up simple imputation.
2022-11-07 16:44:10,709:INFO:Set up variance threshold.
2022-11-07 16:44:11,094:INFO:Finished creating preprocessing pipeline.
2022-11-07 16:44:11,109:INFO:Pipeline: Pipeline(memory=Memory(location=/tmp/joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['1rd_red', ' 1rd_green',
                                             ' 1rd_blue', ' 1rd_r', ' 1rd_rg',
                                             ' 1rd_most_red', ' 1rd_least_red',
                                             ' 1rd_most_yellow', ' 1rd_size',
                                             ' 2nd_red', ' 2nd_green',
                                             ' 2nd_blue', ' 2rd_r', ' 2rd_rg',
                                             ' 2rd_most_red', ' 2rd_least_red',
                                             ' 2rd_most_yellow', ' 2nd_size',
                                             ' 3rd_red', ' 3rd_green',
                                             ' 3rd_bl...d_rg',
                                             ' 3rd_most_red', ' 3rd_least_red',
                                             ' 3rd_most_yellow', ' 3rd_size',
                                             ' 4th_red', ' 4th_green',
                                             ' 4th_blue', ...],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=[],
                                    transformer=SimpleImputer(fill_value='constant',
                                                              strategy='constant'))),
                ('low_variance',
                 TransformerWrapper(exclude=[],
                                    transformer=VarianceThreshold(threshold=0)))])
2022-11-07 16:44:11,109:INFO:Creating final display dataframe.
2022-11-07 16:44:12,663:INFO:Setup display_container:                Description             Value
0               Session id              2174
1                   Target         Sweetness
2              Target type        Regression
3               Data shape         (200, 56)
4         Train data shape         (139, 56)
5          Test data shape          (61, 56)
6         Numeric features                55
7               Preprocess              True
8          Imputation type            simple
9       Numeric imputation              mean
10  Categorical imputation          constant
11  Low variance threshold                 0
12          Fold Generator             KFold
13             Fold Number                10
14                CPU Jobs                -1
15                 Use GPU             False
16          Log Experiment             False
17         Experiment Name  reg-default-name
18                     USI              e6dc
2022-11-07 16:44:12,831:INFO:Soft dependency imported: xgboost: 0.90
2022-11-07 16:44:12,831:WARNING:Wrong xgboost version. Expected xgboost>=1.1.0, got xgboost==0.90
2022-11-07 16:44:12,832:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-11-07 16:44:12,982:INFO:Soft dependency imported: xgboost: 0.90
2022-11-07 16:44:12,983:WARNING:Wrong xgboost version. Expected xgboost>=1.1.0, got xgboost==0.90
2022-11-07 16:44:12,984:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-11-07 16:44:12,993:INFO:setup() successfully completed in 7.09s...............
2022-11-07 16:44:15,443:INFO:Initializing compare_models()
2022-11-07 16:44:15,443:INFO:compare_models(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f20c296c9d0>, include=None, fold=None, round=4, cross_validation=True, sort=R2, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.regression.oop.RegressionExperiment object at 0x7f20c296c9d0>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'R2', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.regression.oop.RegressionExperiment'>}, exclude=None)
2022-11-07 16:44:15,443:INFO:Checking exceptions
2022-11-07 16:44:15,449:INFO:Preparing display monitor
2022-11-07 16:44:15,546:INFO:Initializing Linear Regression
2022-11-07 16:44:15,547:INFO:Total runtime is 2.1775563557942707e-05 minutes
2022-11-07 16:44:15,560:INFO:SubProcess create_model() called ==================================
2022-11-07 16:44:15,562:INFO:Initializing create_model()
2022-11-07 16:44:15,563:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f20c296c9d0>, estimator=lr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f20bff57e10>, model_only=True, return_train_score=False, kwargs={})
2022-11-07 16:44:15,563:INFO:Checking exceptions
2022-11-07 16:44:15,566:INFO:Importing libraries
2022-11-07 16:44:15,567:INFO:Copying training dataset
2022-11-07 16:44:15,571:INFO:Defining folds
2022-11-07 16:44:15,572:INFO:Declaring metric variables
2022-11-07 16:44:15,579:INFO:Importing untrained model
2022-11-07 16:44:15,590:INFO:Linear Regression Imported successfully
2022-11-07 16:44:15,609:INFO:Starting cross validation
2022-11-07 16:44:15,618:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-11-07 16:44:24,001:INFO:Calculating mean and std
2022-11-07 16:44:24,009:INFO:Creating metrics dataframe
2022-11-07 16:44:24,020:INFO:Uploading results into container
2022-11-07 16:44:24,024:INFO:Uploading model into container now
2022-11-07 16:44:24,025:INFO:master_model_container: 1
2022-11-07 16:44:24,025:INFO:display_container: 2
2022-11-07 16:44:24,026:INFO:LinearRegression(n_jobs=-1)
2022-11-07 16:44:24,026:INFO:create_model() successfully completed......................................
2022-11-07 16:44:24,269:INFO:SubProcess create_model() end ==================================
2022-11-07 16:44:24,269:INFO:Creating metrics dataframe
2022-11-07 16:44:24,291:INFO:Initializing Lasso Regression
2022-11-07 16:44:24,291:INFO:Total runtime is 0.14575579166412356 minutes
2022-11-07 16:44:24,303:INFO:SubProcess create_model() called ==================================
2022-11-07 16:44:24,308:INFO:Initializing create_model()
2022-11-07 16:44:24,308:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f20c296c9d0>, estimator=lasso, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f20bff57e10>, model_only=True, return_train_score=False, kwargs={})
2022-11-07 16:44:24,308:INFO:Checking exceptions
2022-11-07 16:44:24,312:INFO:Importing libraries
2022-11-07 16:44:24,315:INFO:Copying training dataset
2022-11-07 16:44:24,330:INFO:Defining folds
2022-11-07 16:44:24,333:INFO:Declaring metric variables
2022-11-07 16:44:24,350:INFO:Importing untrained model
2022-11-07 16:44:24,363:INFO:Lasso Regression Imported successfully
2022-11-07 16:44:24,382:INFO:Starting cross validation
2022-11-07 16:44:24,387:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-11-07 16:44:24,475:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.704e+00, tolerance: 2.123e-02
  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive

2022-11-07 16:44:24,484:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.334e+00, tolerance: 1.930e-02
  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive

2022-11-07 16:44:24,547:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.259e-01, tolerance: 2.024e-02
  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive

2022-11-07 16:44:24,580:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.927e-01, tolerance: 1.915e-02
  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive

2022-11-07 16:44:24,636:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.967e+00, tolerance: 2.042e-02
  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive

2022-11-07 16:44:24,687:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.841e+00, tolerance: 2.143e-02
  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive

2022-11-07 16:44:24,693:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.366e-01, tolerance: 2.010e-02
  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive

2022-11-07 16:44:24,748:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.557e+00, tolerance: 2.148e-02
  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive

2022-11-07 16:44:24,751:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.423e+00, tolerance: 2.071e-02
  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive

2022-11-07 16:44:24,795:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.028e+00, tolerance: 2.072e-02
  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive

2022-11-07 16:44:24,810:INFO:Calculating mean and std
2022-11-07 16:44:24,812:INFO:Creating metrics dataframe
2022-11-07 16:44:24,818:INFO:Uploading results into container
2022-11-07 16:44:24,825:INFO:Uploading model into container now
2022-11-07 16:44:24,827:INFO:master_model_container: 2
2022-11-07 16:44:24,830:INFO:display_container: 2
2022-11-07 16:44:24,831:INFO:Lasso(random_state=2174)
2022-11-07 16:44:24,831:INFO:create_model() successfully completed......................................
2022-11-07 16:44:24,976:INFO:SubProcess create_model() end ==================================
2022-11-07 16:44:24,977:INFO:Creating metrics dataframe
2022-11-07 16:44:25,000:INFO:Initializing Ridge Regression
2022-11-07 16:44:25,000:INFO:Total runtime is 0.15756824413935347 minutes
2022-11-07 16:44:25,008:INFO:SubProcess create_model() called ==================================
2022-11-07 16:44:25,009:INFO:Initializing create_model()
2022-11-07 16:44:25,010:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f20c296c9d0>, estimator=ridge, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f20bff57e10>, model_only=True, return_train_score=False, kwargs={})
2022-11-07 16:44:25,011:INFO:Checking exceptions
2022-11-07 16:44:25,014:INFO:Importing libraries
2022-11-07 16:44:25,014:INFO:Copying training dataset
2022-11-07 16:44:25,024:INFO:Defining folds
2022-11-07 16:44:25,024:INFO:Declaring metric variables
2022-11-07 16:44:25,033:INFO:Importing untrained model
2022-11-07 16:44:25,044:INFO:Ridge Regression Imported successfully
2022-11-07 16:44:25,068:INFO:Starting cross validation
2022-11-07 16:44:25,070:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-11-07 16:44:25,424:INFO:Calculating mean and std
2022-11-07 16:44:25,427:INFO:Creating metrics dataframe
2022-11-07 16:44:25,436:INFO:Uploading results into container
2022-11-07 16:44:25,438:INFO:Uploading model into container now
2022-11-07 16:44:25,438:INFO:master_model_container: 3
2022-11-07 16:44:25,439:INFO:display_container: 2
2022-11-07 16:44:25,439:INFO:Ridge(random_state=2174)
2022-11-07 16:44:25,439:INFO:create_model() successfully completed......................................
2022-11-07 16:44:25,581:INFO:SubProcess create_model() end ==================================
2022-11-07 16:44:25,581:INFO:Creating metrics dataframe
2022-11-07 16:44:25,601:INFO:Initializing Elastic Net
2022-11-07 16:44:25,602:INFO:Total runtime is 0.16759550571441653 minutes
2022-11-07 16:44:25,610:INFO:SubProcess create_model() called ==================================
2022-11-07 16:44:25,610:INFO:Initializing create_model()
2022-11-07 16:44:25,611:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f20c296c9d0>, estimator=en, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f20bff57e10>, model_only=True, return_train_score=False, kwargs={})
2022-11-07 16:44:25,611:INFO:Checking exceptions
2022-11-07 16:44:25,614:INFO:Importing libraries
2022-11-07 16:44:25,614:INFO:Copying training dataset
2022-11-07 16:44:25,620:INFO:Defining folds
2022-11-07 16:44:25,621:INFO:Declaring metric variables
2022-11-07 16:44:25,633:INFO:Importing untrained model
2022-11-07 16:44:25,642:INFO:Elastic Net Imported successfully
2022-11-07 16:44:25,660:INFO:Starting cross validation
2022-11-07 16:44:25,663:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-11-07 16:44:25,729:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.163e+00, tolerance: 2.123e-02
  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive

2022-11-07 16:44:25,771:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.419e+00, tolerance: 1.930e-02
  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive

2022-11-07 16:44:25,796:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.370e+00, tolerance: 2.024e-02
  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive

2022-11-07 16:44:25,843:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.761e+00, tolerance: 1.915e-02
  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive

2022-11-07 16:44:25,894:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.195e+00, tolerance: 2.042e-02
  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive

2022-11-07 16:44:25,959:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.438e+00, tolerance: 2.143e-02
  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive

2022-11-07 16:44:26,009:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.018e+01, tolerance: 2.071e-02
  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive

2022-11-07 16:44:26,025:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.855e+00, tolerance: 2.148e-02
  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive

2022-11-07 16:44:26,063:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.507e+00, tolerance: 2.072e-02
  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive

2022-11-07 16:44:26,080:INFO:Calculating mean and std
2022-11-07 16:44:26,087:INFO:Creating metrics dataframe
2022-11-07 16:44:26,098:INFO:Uploading results into container
2022-11-07 16:44:26,100:INFO:Uploading model into container now
2022-11-07 16:44:26,100:INFO:master_model_container: 4
2022-11-07 16:44:26,101:INFO:display_container: 2
2022-11-07 16:44:26,101:INFO:ElasticNet(random_state=2174)
2022-11-07 16:44:26,101:INFO:create_model() successfully completed......................................
2022-11-07 16:44:26,248:INFO:SubProcess create_model() end ==================================
2022-11-07 16:44:26,249:INFO:Creating metrics dataframe
2022-11-07 16:44:26,268:INFO:Initializing Least Angle Regression
2022-11-07 16:44:26,269:INFO:Total runtime is 0.17871542374293012 minutes
2022-11-07 16:44:26,279:INFO:SubProcess create_model() called ==================================
2022-11-07 16:44:26,280:INFO:Initializing create_model()
2022-11-07 16:44:26,280:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f20c296c9d0>, estimator=lar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f20bff57e10>, model_only=True, return_train_score=False, kwargs={})
2022-11-07 16:44:26,280:INFO:Checking exceptions
2022-11-07 16:44:26,284:INFO:Importing libraries
2022-11-07 16:44:26,284:INFO:Copying training dataset
2022-11-07 16:44:26,289:INFO:Defining folds
2022-11-07 16:44:26,291:INFO:Declaring metric variables
2022-11-07 16:44:26,305:INFO:Importing untrained model
2022-11-07 16:44:26,314:INFO:Least Angle Regression Imported successfully
2022-11-07 16:44:26,333:INFO:Starting cross validation
2022-11-07 16:44:26,337:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-11-07 16:44:26,384:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2022-11-07 16:44:26,405:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 71 iterations, i.e. alpha=1.257e+00, with an active set of 49 regressors, and the smallest cholesky pivot element being 8.878e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:44:26,407:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 72 iterations, i.e. alpha=7.720e-01, with an active set of 50 regressors, and the smallest cholesky pivot element being 7.224e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:44:26,410:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 75 iterations, i.e. alpha=6.071e-02, with an active set of 52 regressors, and the smallest cholesky pivot element being 8.689e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:44:26,411:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 75 iterations, i.e. alpha=5.701e-02, with an active set of 52 regressors, and the smallest cholesky pivot element being 7.224e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:44:26,411:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 76 iterations, i.e. alpha=1.199e-02, with an active set of 53 regressors, and the smallest cholesky pivot element being 8.689e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:44:26,412:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 76 iterations, i.e. alpha=9.664e-03, with an active set of 53 regressors, and the smallest cholesky pivot element being 5.475e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:44:26,433:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2022-11-07 16:44:26,460:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2022-11-07 16:44:26,476:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 62 iterations, i.e. alpha=2.152e-02, with an active set of 48 regressors, and the smallest cholesky pivot element being 8.093e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:44:26,477:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 62 iterations, i.e. alpha=1.114e-02, with an active set of 48 regressors, and the smallest cholesky pivot element being 8.560e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:44:26,479:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 70 iterations, i.e. alpha=7.564e-03, with an active set of 51 regressors, and the smallest cholesky pivot element being 7.743e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:44:26,479:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 70 iterations, i.e. alpha=3.547e-03, with an active set of 51 regressors, and the smallest cholesky pivot element being 7.743e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:44:26,480:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 70 iterations, i.e. alpha=3.483e-03, with an active set of 51 regressors, and the smallest cholesky pivot element being 7.525e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:44:26,480:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 70 iterations, i.e. alpha=1.291e-03, with an active set of 51 regressors, and the smallest cholesky pivot element being 6.053e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:44:26,480:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 68 iterations, i.e. alpha=1.891e-02, with an active set of 53 regressors, and the smallest cholesky pivot element being 6.053e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:44:26,481:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 68 iterations, i.e. alpha=7.976e-03, with an active set of 53 regressors, and the smallest cholesky pivot element being 6.053e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:44:26,539:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2022-11-07 16:44:26,561:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 71 iterations, i.e. alpha=9.054e+00, with an active set of 49 regressors, and the smallest cholesky pivot element being 8.229e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:44:26,562:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 72 iterations, i.e. alpha=4.470e+00, with an active set of 50 regressors, and the smallest cholesky pivot element being 8.162e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:44:26,563:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 72 iterations, i.e. alpha=3.521e+00, with an active set of 50 regressors, and the smallest cholesky pivot element being 8.162e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:44:26,563:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 72 iterations, i.e. alpha=3.110e+00, with an active set of 50 regressors, and the smallest cholesky pivot element being 7.376e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:44:26,564:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 74 iterations, i.e. alpha=1.815e-02, with an active set of 51 regressors, and the smallest cholesky pivot element being 8.162e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:44:26,564:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 74 iterations, i.e. alpha=1.731e-02, with an active set of 51 regressors, and the smallest cholesky pivot element being 7.376e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:44:26,565:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 74 iterations, i.e. alpha=8.098e-03, with an active set of 51 regressors, and the smallest cholesky pivot element being 7.376e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:44:26,565:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 74 iterations, i.e. alpha=2.035e-03, with an active set of 51 regressors, and the smallest cholesky pivot element being 6.909e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:44:26,583:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2022-11-07 16:44:26,601:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 68 iterations, i.e. alpha=1.678e-01, with an active set of 48 regressors, and the smallest cholesky pivot element being 5.867e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:44:26,602:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 70 iterations, i.e. alpha=1.071e-01, with an active set of 49 regressors, and the smallest cholesky pivot element being 9.657e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:44:26,603:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 74 iterations, i.e. alpha=5.014e-02, with an active set of 51 regressors, and the smallest cholesky pivot element being 7.671e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:44:26,604:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 74 iterations, i.e. alpha=3.644e-02, with an active set of 51 regressors, and the smallest cholesky pivot element being 4.712e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:44:26,604:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 74 iterations, i.e. alpha=1.311e-02, with an active set of 51 regressors, and the smallest cholesky pivot element being 6.909e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:44:26,604:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 74 iterations, i.e. alpha=1.131e-02, with an active set of 51 regressors, and the smallest cholesky pivot element being 6.909e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:44:26,633:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2022-11-07 16:44:26,651:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2022-11-07 16:44:26,653:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 80 iterations, i.e. alpha=6.488e+03, with an active set of 52 regressors, and the smallest cholesky pivot element being 6.829e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:44:26,653:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 80 iterations, i.e. alpha=2.211e+03, with an active set of 52 regressors, and the smallest cholesky pivot element being 8.162e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:44:26,654:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 80 iterations, i.e. alpha=1.524e+03, with an active set of 52 regressors, and the smallest cholesky pivot element being 7.955e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:44:26,666:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 58 iterations, i.e. alpha=2.566e+02, with an active set of 46 regressors, and the smallest cholesky pivot element being 8.297e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:44:26,669:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 67 iterations, i.e. alpha=2.645e+02, with an active set of 50 regressors, and the smallest cholesky pivot element being 8.816e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:44:26,669:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 67 iterations, i.e. alpha=2.494e+02, with an active set of 50 regressors, and the smallest cholesky pivot element being 8.162e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:44:26,670:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 67 iterations, i.e. alpha=2.442e+02, with an active set of 50 regressors, and the smallest cholesky pivot element being 8.162e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:44:26,671:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 69 iterations, i.e. alpha=1.486e+02, with an active set of 51 regressors, and the smallest cholesky pivot element being 8.689e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:44:26,671:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 69 iterations, i.e. alpha=1.032e+02, with an active set of 51 regressors, and the smallest cholesky pivot element being 5.674e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:44:26,671:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 69 iterations, i.e. alpha=6.592e+01, with an active set of 51 regressors, and the smallest cholesky pivot element being 9.714e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:44:26,672:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 69 iterations, i.e. alpha=3.042e+01, with an active set of 51 regressors, and the smallest cholesky pivot element being 9.714e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:44:26,699:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2022-11-07 16:44:26,715:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 61 iterations, i.e. alpha=5.038e-02, with an active set of 47 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:44:26,715:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 61 iterations, i.e. alpha=4.043e-02, with an active set of 47 regressors, and the smallest cholesky pivot element being 7.885e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:44:26,717:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 67 iterations, i.e. alpha=4.914e-02, with an active set of 50 regressors, and the smallest cholesky pivot element being 8.816e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:44:26,717:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 67 iterations, i.e. alpha=4.779e-02, with an active set of 50 regressors, and the smallest cholesky pivot element being 7.525e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:44:26,718:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 67 iterations, i.e. alpha=3.057e-02, with an active set of 50 regressors, and the smallest cholesky pivot element being 6.909e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:44:26,718:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 67 iterations, i.e. alpha=1.585e-02, with an active set of 50 regressors, and the smallest cholesky pivot element being 6.909e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:44:26,719:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 68 iterations, i.e. alpha=5.939e-03, with an active set of 51 regressors, and the smallest cholesky pivot element being 8.816e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:44:26,719:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 68 iterations, i.e. alpha=4.355e-03, with an active set of 51 regressors, and the smallest cholesky pivot element being 8.816e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:44:26,719:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 68 iterations, i.e. alpha=3.489e-03, with an active set of 51 regressors, and the smallest cholesky pivot element being 6.909e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:44:26,720:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 68 iterations, i.e. alpha=1.905e-03, with an active set of 51 regressors, and the smallest cholesky pivot element being 8.297e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:44:26,721:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2022-11-07 16:44:26,739:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 74 iterations, i.e. alpha=2.814e-01, with an active set of 49 regressors, and the smallest cholesky pivot element being 5.867e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:44:26,740:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 74 iterations, i.e. alpha=1.955e-01, with an active set of 49 regressors, and the smallest cholesky pivot element being 5.867e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:44:26,740:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 74 iterations, i.e. alpha=1.258e-01, with an active set of 49 regressors, and the smallest cholesky pivot element being 8.625e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:44:26,742:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 79 iterations, i.e. alpha=5.222e-02, with an active set of 52 regressors, and the smallest cholesky pivot element being 6.144e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:44:26,742:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 79 iterations, i.e. alpha=4.196e-02, with an active set of 52 regressors, and the smallest cholesky pivot element being 8.297e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:44:26,743:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 79 iterations, i.e. alpha=3.625e-02, with an active set of 52 regressors, and the smallest cholesky pivot element being 5.674e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:44:26,767:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2022-11-07 16:44:26,775:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 56 iterations, i.e. alpha=1.096e-01, with an active set of 44 regressors, and the smallest cholesky pivot element being 9.064e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:44:26,777:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 64 iterations, i.e. alpha=8.191e-02, with an active set of 49 regressors, and the smallest cholesky pivot element being 8.752e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:44:26,778:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 65 iterations, i.e. alpha=6.646e-02, with an active set of 50 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:44:26,778:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 67 iterations, i.e. alpha=7.347e-03, with an active set of 51 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:44:26,779:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 67 iterations, i.e. alpha=6.370e-03, with an active set of 51 regressors, and the smallest cholesky pivot element being 8.560e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:44:26,779:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 67 iterations, i.e. alpha=3.874e-03, with an active set of 51 regressors, and the smallest cholesky pivot element being 8.560e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:44:26,779:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 67 iterations, i.e. alpha=3.373e-03, with an active set of 51 regressors, and the smallest cholesky pivot element being 6.495e-08. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-11-07 16:44:26,793:INFO:Calculating mean and std
2022-11-07 16:44:26,796:INFO:Creating metrics dataframe
2022-11-07 16:44:26,801:INFO:Uploading results into container
2022-11-07 16:44:26,808:INFO:Uploading model into container now
2022-11-07 16:44:26,809:INFO:master_model_container: 5
2022-11-07 16:44:26,810:INFO:display_container: 2
2022-11-07 16:44:26,810:INFO:Lars(random_state=2174)
2022-11-07 16:44:26,811:INFO:create_model() successfully completed......................................
2022-11-07 16:44:26,954:INFO:SubProcess create_model() end ==================================
2022-11-07 16:44:26,954:INFO:Creating metrics dataframe
2022-11-07 16:44:26,974:INFO:Initializing Lasso Least Angle Regression
2022-11-07 16:44:26,979:INFO:Total runtime is 0.19054199059804283 minutes
2022-11-07 16:44:26,985:INFO:SubProcess create_model() called ==================================
2022-11-07 16:44:26,986:INFO:Initializing create_model()
2022-11-07 16:44:26,986:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f20c296c9d0>, estimator=llar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f20bff57e10>, model_only=True, return_train_score=False, kwargs={})
2022-11-07 16:44:26,989:INFO:Checking exceptions
2022-11-07 16:44:26,992:INFO:Importing libraries
2022-11-07 16:44:26,993:INFO:Copying training dataset
2022-11-07 16:44:27,000:INFO:Defining folds
2022-11-07 16:44:27,001:INFO:Declaring metric variables
2022-11-07 16:44:27,013:INFO:Importing untrained model
2022-11-07 16:44:27,022:INFO:Lasso Least Angle Regression Imported successfully
2022-11-07 16:44:27,040:INFO:Starting cross validation
2022-11-07 16:44:27,044:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-11-07 16:44:27,090:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  FutureWarning,

2022-11-07 16:44:27,151:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  FutureWarning,

2022-11-07 16:44:27,154:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  FutureWarning,

2022-11-07 16:44:27,218:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  FutureWarning,

2022-11-07 16:44:27,239:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  FutureWarning,

2022-11-07 16:44:27,290:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  FutureWarning,

2022-11-07 16:44:27,314:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  FutureWarning,

2022-11-07 16:44:27,350:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  FutureWarning,

2022-11-07 16:44:27,367:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  FutureWarning,

2022-11-07 16:44:27,395:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  FutureWarning,

2022-11-07 16:44:27,409:INFO:Calculating mean and std
2022-11-07 16:44:27,411:INFO:Creating metrics dataframe
2022-11-07 16:44:27,417:INFO:Uploading results into container
2022-11-07 16:44:27,421:INFO:Uploading model into container now
2022-11-07 16:44:27,423:INFO:master_model_container: 6
2022-11-07 16:44:27,424:INFO:display_container: 2
2022-11-07 16:44:27,425:INFO:LassoLars(random_state=2174)
2022-11-07 16:44:27,425:INFO:create_model() successfully completed......................................
2022-11-07 16:44:27,567:INFO:SubProcess create_model() end ==================================
2022-11-07 16:44:27,568:INFO:Creating metrics dataframe
2022-11-07 16:44:27,588:INFO:Initializing Orthogonal Matching Pursuit
2022-11-07 16:44:27,588:INFO:Total runtime is 0.2007057348887126 minutes
2022-11-07 16:44:27,597:INFO:SubProcess create_model() called ==================================
2022-11-07 16:44:27,598:INFO:Initializing create_model()
2022-11-07 16:44:27,598:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f20c296c9d0>, estimator=omp, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f20bff57e10>, model_only=True, return_train_score=False, kwargs={})
2022-11-07 16:44:27,599:INFO:Checking exceptions
2022-11-07 16:44:27,602:INFO:Importing libraries
2022-11-07 16:44:27,602:INFO:Copying training dataset
2022-11-07 16:44:27,608:INFO:Defining folds
2022-11-07 16:44:27,609:INFO:Declaring metric variables
2022-11-07 16:44:27,620:INFO:Importing untrained model
2022-11-07 16:44:27,630:INFO:Orthogonal Matching Pursuit Imported successfully
2022-11-07 16:44:27,653:INFO:Starting cross validation
2022-11-07 16:44:27,655:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-11-07 16:44:27,698:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2022-11-07 16:44:27,733:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2022-11-07 16:44:27,765:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2022-11-07 16:44:27,813:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2022-11-07 16:44:27,843:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2022-11-07 16:44:27,892:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2022-11-07 16:44:27,893:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2022-11-07 16:44:27,941:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2022-11-07 16:44:27,961:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2022-11-07 16:44:27,988:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2022-11-07 16:44:28,005:INFO:Calculating mean and std
2022-11-07 16:44:28,007:INFO:Creating metrics dataframe
2022-11-07 16:44:28,015:INFO:Uploading results into container
2022-11-07 16:44:28,016:INFO:Uploading model into container now
2022-11-07 16:44:28,016:INFO:master_model_container: 7
2022-11-07 16:44:28,017:INFO:display_container: 2
2022-11-07 16:44:28,017:INFO:OrthogonalMatchingPursuit()
2022-11-07 16:44:28,017:INFO:create_model() successfully completed......................................
2022-11-07 16:44:28,162:INFO:SubProcess create_model() end ==================================
2022-11-07 16:44:28,162:INFO:Creating metrics dataframe
2022-11-07 16:44:28,188:INFO:Initializing Bayesian Ridge
2022-11-07 16:44:28,189:INFO:Total runtime is 0.210716716448466 minutes
2022-11-07 16:44:28,197:INFO:SubProcess create_model() called ==================================
2022-11-07 16:44:28,199:INFO:Initializing create_model()
2022-11-07 16:44:28,199:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f20c296c9d0>, estimator=br, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f20bff57e10>, model_only=True, return_train_score=False, kwargs={})
2022-11-07 16:44:28,200:INFO:Checking exceptions
2022-11-07 16:44:28,203:INFO:Importing libraries
2022-11-07 16:44:28,203:INFO:Copying training dataset
2022-11-07 16:44:28,208:INFO:Defining folds
2022-11-07 16:44:28,210:INFO:Declaring metric variables
2022-11-07 16:44:28,221:INFO:Importing untrained model
2022-11-07 16:44:28,231:INFO:Bayesian Ridge Imported successfully
2022-11-07 16:44:28,247:INFO:Starting cross validation
2022-11-07 16:44:28,249:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-11-07 16:44:28,609:INFO:Calculating mean and std
2022-11-07 16:44:28,613:INFO:Creating metrics dataframe
2022-11-07 16:44:28,627:INFO:Uploading results into container
2022-11-07 16:44:28,628:INFO:Uploading model into container now
2022-11-07 16:44:28,629:INFO:master_model_container: 8
2022-11-07 16:44:28,629:INFO:display_container: 2
2022-11-07 16:44:28,629:INFO:BayesianRidge()
2022-11-07 16:44:28,629:INFO:create_model() successfully completed......................................
2022-11-07 16:44:28,769:INFO:SubProcess create_model() end ==================================
2022-11-07 16:44:28,770:INFO:Creating metrics dataframe
2022-11-07 16:44:28,791:INFO:Initializing Passive Aggressive Regressor
2022-11-07 16:44:28,792:INFO:Total runtime is 0.220762312412262 minutes
2022-11-07 16:44:28,801:INFO:SubProcess create_model() called ==================================
2022-11-07 16:44:28,802:INFO:Initializing create_model()
2022-11-07 16:44:28,803:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f20c296c9d0>, estimator=par, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f20bff57e10>, model_only=True, return_train_score=False, kwargs={})
2022-11-07 16:44:28,803:INFO:Checking exceptions
2022-11-07 16:44:28,806:INFO:Importing libraries
2022-11-07 16:44:28,806:INFO:Copying training dataset
2022-11-07 16:44:28,812:INFO:Defining folds
2022-11-07 16:44:28,813:INFO:Declaring metric variables
2022-11-07 16:44:28,823:INFO:Importing untrained model
2022-11-07 16:44:28,837:INFO:Passive Aggressive Regressor Imported successfully
2022-11-07 16:44:28,853:INFO:Starting cross validation
2022-11-07 16:44:28,855:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-11-07 16:44:29,213:INFO:Calculating mean and std
2022-11-07 16:44:29,216:INFO:Creating metrics dataframe
2022-11-07 16:44:29,231:INFO:Uploading results into container
2022-11-07 16:44:29,232:INFO:Uploading model into container now
2022-11-07 16:44:29,233:INFO:master_model_container: 9
2022-11-07 16:44:29,233:INFO:display_container: 2
2022-11-07 16:44:29,234:INFO:PassiveAggressiveRegressor(random_state=2174)
2022-11-07 16:44:29,234:INFO:create_model() successfully completed......................................
2022-11-07 16:44:29,380:INFO:SubProcess create_model() end ==================================
2022-11-07 16:44:29,381:INFO:Creating metrics dataframe
2022-11-07 16:44:29,405:INFO:Initializing Huber Regressor
2022-11-07 16:44:29,405:INFO:Total runtime is 0.23098438183466596 minutes
2022-11-07 16:44:29,415:INFO:SubProcess create_model() called ==================================
2022-11-07 16:44:29,418:INFO:Initializing create_model()
2022-11-07 16:44:29,420:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f20c296c9d0>, estimator=huber, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f20bff57e10>, model_only=True, return_train_score=False, kwargs={})
2022-11-07 16:44:29,420:INFO:Checking exceptions
2022-11-07 16:44:29,423:INFO:Importing libraries
2022-11-07 16:44:29,424:INFO:Copying training dataset
2022-11-07 16:44:29,429:INFO:Defining folds
2022-11-07 16:44:29,430:INFO:Declaring metric variables
2022-11-07 16:44:29,444:INFO:Importing untrained model
2022-11-07 16:44:29,455:INFO:Huber Regressor Imported successfully
2022-11-07 16:44:29,473:INFO:Starting cross validation
2022-11-07 16:44:29,475:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-11-07 16:44:29,572:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2022-11-07 16:44:29,629:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2022-11-07 16:44:29,724:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2022-11-07 16:44:29,745:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2022-11-07 16:44:29,838:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2022-11-07 16:44:29,875:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2022-11-07 16:44:29,949:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2022-11-07 16:44:29,994:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2022-11-07 16:44:30,056:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2022-11-07 16:44:30,092:WARNING:/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2022-11-07 16:44:30,107:INFO:Calculating mean and std
2022-11-07 16:44:30,109:INFO:Creating metrics dataframe
2022-11-07 16:44:30,123:INFO:Uploading results into container
2022-11-07 16:44:30,124:INFO:Uploading model into container now
2022-11-07 16:44:30,124:INFO:master_model_container: 10
2022-11-07 16:44:30,125:INFO:display_container: 2
2022-11-07 16:44:30,125:INFO:HuberRegressor()
2022-11-07 16:44:30,125:INFO:create_model() successfully completed......................................
2022-11-07 16:44:30,274:INFO:SubProcess create_model() end ==================================
2022-11-07 16:44:30,274:INFO:Creating metrics dataframe
2022-11-07 16:44:30,296:INFO:Initializing K Neighbors Regressor
2022-11-07 16:44:30,297:INFO:Total runtime is 0.2458457191785177 minutes
2022-11-07 16:44:30,305:INFO:SubProcess create_model() called ==================================
2022-11-07 16:44:30,306:INFO:Initializing create_model()
2022-11-07 16:44:30,306:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f20c296c9d0>, estimator=knn, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f20bff57e10>, model_only=True, return_train_score=False, kwargs={})
2022-11-07 16:44:30,306:INFO:Checking exceptions
2022-11-07 16:44:30,310:INFO:Importing libraries
2022-11-07 16:44:30,310:INFO:Copying training dataset
2022-11-07 16:44:30,319:INFO:Defining folds
2022-11-07 16:44:30,319:INFO:Declaring metric variables
2022-11-07 16:44:30,328:INFO:Importing untrained model
2022-11-07 16:44:30,337:INFO:K Neighbors Regressor Imported successfully
2022-11-07 16:44:30,353:INFO:Starting cross validation
2022-11-07 16:44:30,356:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-11-07 16:44:31,169:INFO:Calculating mean and std
2022-11-07 16:44:31,171:INFO:Creating metrics dataframe
2022-11-07 16:44:31,185:INFO:Uploading results into container
2022-11-07 16:44:31,187:INFO:Uploading model into container now
2022-11-07 16:44:31,188:INFO:master_model_container: 11
2022-11-07 16:44:31,188:INFO:display_container: 2
2022-11-07 16:44:31,188:INFO:KNeighborsRegressor(n_jobs=-1)
2022-11-07 16:44:31,189:INFO:create_model() successfully completed......................................
2022-11-07 16:44:31,339:INFO:SubProcess create_model() end ==================================
2022-11-07 16:44:31,340:INFO:Creating metrics dataframe
2022-11-07 16:44:31,365:INFO:Initializing Decision Tree Regressor
2022-11-07 16:44:31,366:INFO:Total runtime is 0.2636694749196371 minutes
2022-11-07 16:44:31,376:INFO:SubProcess create_model() called ==================================
2022-11-07 16:44:31,377:INFO:Initializing create_model()
2022-11-07 16:44:31,381:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f20c296c9d0>, estimator=dt, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f20bff57e10>, model_only=True, return_train_score=False, kwargs={})
2022-11-07 16:44:31,381:INFO:Checking exceptions
2022-11-07 16:44:31,384:INFO:Importing libraries
2022-11-07 16:44:31,384:INFO:Copying training dataset
2022-11-07 16:44:31,393:INFO:Defining folds
2022-11-07 16:44:31,396:INFO:Declaring metric variables
2022-11-07 16:44:31,404:INFO:Importing untrained model
2022-11-07 16:44:31,413:INFO:Decision Tree Regressor Imported successfully
2022-11-07 16:44:31,433:INFO:Starting cross validation
2022-11-07 16:44:31,436:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-11-07 16:44:31,815:INFO:Calculating mean and std
2022-11-07 16:44:31,817:INFO:Creating metrics dataframe
2022-11-07 16:44:31,823:INFO:Uploading results into container
2022-11-07 16:44:31,824:INFO:Uploading model into container now
2022-11-07 16:44:31,826:INFO:master_model_container: 12
2022-11-07 16:44:31,826:INFO:display_container: 2
2022-11-07 16:44:31,828:INFO:DecisionTreeRegressor(random_state=2174)
2022-11-07 16:44:31,829:INFO:create_model() successfully completed......................................
2022-11-07 16:44:31,971:INFO:SubProcess create_model() end ==================================
2022-11-07 16:44:31,972:INFO:Creating metrics dataframe
2022-11-07 16:44:32,001:INFO:Initializing Random Forest Regressor
2022-11-07 16:44:32,001:INFO:Total runtime is 0.27425579627354946 minutes
2022-11-07 16:44:32,010:INFO:SubProcess create_model() called ==================================
2022-11-07 16:44:32,011:INFO:Initializing create_model()
2022-11-07 16:44:32,011:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f20c296c9d0>, estimator=rf, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f20bff57e10>, model_only=True, return_train_score=False, kwargs={})
2022-11-07 16:44:32,012:INFO:Checking exceptions
2022-11-07 16:44:32,016:INFO:Importing libraries
2022-11-07 16:44:32,017:INFO:Copying training dataset
2022-11-07 16:44:32,021:INFO:Defining folds
2022-11-07 16:44:32,022:INFO:Declaring metric variables
2022-11-07 16:44:32,038:INFO:Importing untrained model
2022-11-07 16:44:32,047:INFO:Random Forest Regressor Imported successfully
2022-11-07 16:44:32,063:INFO:Starting cross validation
2022-11-07 16:44:32,065:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-11-07 16:44:35,775:INFO:Calculating mean and std
2022-11-07 16:44:35,779:INFO:Creating metrics dataframe
2022-11-07 16:44:35,789:INFO:Uploading results into container
2022-11-07 16:44:35,791:INFO:Uploading model into container now
2022-11-07 16:44:35,792:INFO:master_model_container: 13
2022-11-07 16:44:35,792:INFO:display_container: 2
2022-11-07 16:44:35,793:INFO:RandomForestRegressor(n_jobs=-1, random_state=2174)
2022-11-07 16:44:35,793:INFO:create_model() successfully completed......................................
2022-11-07 16:44:35,948:INFO:SubProcess create_model() end ==================================
2022-11-07 16:44:35,949:INFO:Creating metrics dataframe
2022-11-07 16:44:35,978:INFO:Initializing Extra Trees Regressor
2022-11-07 16:44:35,978:INFO:Total runtime is 0.3405374964078268 minutes
2022-11-07 16:44:35,991:INFO:SubProcess create_model() called ==================================
2022-11-07 16:44:35,992:INFO:Initializing create_model()
2022-11-07 16:44:35,992:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f20c296c9d0>, estimator=et, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f20bff57e10>, model_only=True, return_train_score=False, kwargs={})
2022-11-07 16:44:35,993:INFO:Checking exceptions
2022-11-07 16:44:35,996:INFO:Importing libraries
2022-11-07 16:44:35,997:INFO:Copying training dataset
2022-11-07 16:44:36,009:INFO:Defining folds
2022-11-07 16:44:36,009:INFO:Declaring metric variables
2022-11-07 16:44:36,021:INFO:Importing untrained model
2022-11-07 16:44:36,031:INFO:Extra Trees Regressor Imported successfully
2022-11-07 16:44:36,049:INFO:Starting cross validation
2022-11-07 16:44:36,051:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-11-07 16:44:38,745:INFO:Calculating mean and std
2022-11-07 16:44:38,750:INFO:Creating metrics dataframe
2022-11-07 16:44:38,760:INFO:Uploading results into container
2022-11-07 16:44:38,761:INFO:Uploading model into container now
2022-11-07 16:44:38,762:INFO:master_model_container: 14
2022-11-07 16:44:38,762:INFO:display_container: 2
2022-11-07 16:44:38,763:INFO:ExtraTreesRegressor(n_jobs=-1, random_state=2174)
2022-11-07 16:44:38,763:INFO:create_model() successfully completed......................................
2022-11-07 16:44:38,916:INFO:SubProcess create_model() end ==================================
2022-11-07 16:44:38,917:INFO:Creating metrics dataframe
2022-11-07 16:44:38,940:INFO:Initializing AdaBoost Regressor
2022-11-07 16:44:38,941:INFO:Total runtime is 0.38991283575693775 minutes
2022-11-07 16:44:38,952:INFO:SubProcess create_model() called ==================================
2022-11-07 16:44:38,952:INFO:Initializing create_model()
2022-11-07 16:44:38,953:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f20c296c9d0>, estimator=ada, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f20bff57e10>, model_only=True, return_train_score=False, kwargs={})
2022-11-07 16:44:38,953:INFO:Checking exceptions
2022-11-07 16:44:38,956:INFO:Importing libraries
2022-11-07 16:44:38,957:INFO:Copying training dataset
2022-11-07 16:44:38,969:INFO:Defining folds
2022-11-07 16:44:38,969:INFO:Declaring metric variables
2022-11-07 16:44:38,982:INFO:Importing untrained model
2022-11-07 16:44:38,994:INFO:AdaBoost Regressor Imported successfully
2022-11-07 16:44:39,018:INFO:Starting cross validation
2022-11-07 16:44:39,021:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-11-07 16:44:40,455:INFO:Calculating mean and std
2022-11-07 16:44:40,458:INFO:Creating metrics dataframe
2022-11-07 16:44:40,467:INFO:Uploading results into container
2022-11-07 16:44:40,468:INFO:Uploading model into container now
2022-11-07 16:44:40,469:INFO:master_model_container: 15
2022-11-07 16:44:40,469:INFO:display_container: 2
2022-11-07 16:44:40,470:INFO:AdaBoostRegressor(random_state=2174)
2022-11-07 16:44:40,470:INFO:create_model() successfully completed......................................
2022-11-07 16:44:40,627:INFO:SubProcess create_model() end ==================================
2022-11-07 16:44:40,628:INFO:Creating metrics dataframe
2022-11-07 16:44:40,655:INFO:Initializing Gradient Boosting Regressor
2022-11-07 16:44:40,656:INFO:Total runtime is 0.4184953729311626 minutes
2022-11-07 16:44:40,668:INFO:SubProcess create_model() called ==================================
2022-11-07 16:44:40,668:INFO:Initializing create_model()
2022-11-07 16:44:40,669:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f20c296c9d0>, estimator=gbr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f20bff57e10>, model_only=True, return_train_score=False, kwargs={})
2022-11-07 16:44:40,669:INFO:Checking exceptions
2022-11-07 16:44:40,672:INFO:Importing libraries
2022-11-07 16:44:40,673:INFO:Copying training dataset
2022-11-07 16:44:40,682:INFO:Defining folds
2022-11-07 16:44:40,682:INFO:Declaring metric variables
2022-11-07 16:44:40,695:INFO:Importing untrained model
2022-11-07 16:44:40,704:INFO:Gradient Boosting Regressor Imported successfully
2022-11-07 16:44:40,725:INFO:Starting cross validation
2022-11-07 16:44:40,729:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-11-07 16:44:42,496:INFO:Calculating mean and std
2022-11-07 16:44:42,505:INFO:Creating metrics dataframe
2022-11-07 16:44:42,515:INFO:Uploading results into container
2022-11-07 16:44:42,516:INFO:Uploading model into container now
2022-11-07 16:44:42,516:INFO:master_model_container: 16
2022-11-07 16:44:42,516:INFO:display_container: 2
2022-11-07 16:44:42,517:INFO:GradientBoostingRegressor(random_state=2174)
2022-11-07 16:44:42,517:INFO:create_model() successfully completed......................................
2022-11-07 16:44:42,667:INFO:SubProcess create_model() end ==================================
2022-11-07 16:44:42,667:INFO:Creating metrics dataframe
2022-11-07 16:44:42,690:INFO:Initializing Light Gradient Boosting Machine
2022-11-07 16:44:42,691:INFO:Total runtime is 0.45241483449935926 minutes
2022-11-07 16:44:42,702:INFO:SubProcess create_model() called ==================================
2022-11-07 16:44:42,709:INFO:Initializing create_model()
2022-11-07 16:44:42,709:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f20c296c9d0>, estimator=lightgbm, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f20bff57e10>, model_only=True, return_train_score=False, kwargs={})
2022-11-07 16:44:42,710:INFO:Checking exceptions
2022-11-07 16:44:42,712:INFO:Importing libraries
2022-11-07 16:44:42,712:INFO:Copying training dataset
2022-11-07 16:44:42,718:INFO:Defining folds
2022-11-07 16:44:42,718:INFO:Declaring metric variables
2022-11-07 16:44:42,730:INFO:Importing untrained model
2022-11-07 16:44:42,744:INFO:Light Gradient Boosting Machine Imported successfully
2022-11-07 16:44:42,765:INFO:Starting cross validation
2022-11-07 16:44:42,770:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-11-07 16:44:44,016:INFO:Calculating mean and std
2022-11-07 16:44:44,022:INFO:Creating metrics dataframe
2022-11-07 16:44:44,029:INFO:Uploading results into container
2022-11-07 16:44:44,031:INFO:Uploading model into container now
2022-11-07 16:44:44,031:INFO:master_model_container: 17
2022-11-07 16:44:44,032:INFO:display_container: 2
2022-11-07 16:44:44,032:INFO:LGBMRegressor(random_state=2174)
2022-11-07 16:44:44,033:INFO:create_model() successfully completed......................................
2022-11-07 16:44:44,177:INFO:SubProcess create_model() end ==================================
2022-11-07 16:44:44,177:INFO:Creating metrics dataframe
2022-11-07 16:44:44,203:INFO:Initializing Dummy Regressor
2022-11-07 16:44:44,203:INFO:Total runtime is 0.4776216506958009 minutes
2022-11-07 16:44:44,213:INFO:SubProcess create_model() called ==================================
2022-11-07 16:44:44,213:INFO:Initializing create_model()
2022-11-07 16:44:44,214:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f20c296c9d0>, estimator=dummy, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f20bff57e10>, model_only=True, return_train_score=False, kwargs={})
2022-11-07 16:44:44,215:INFO:Checking exceptions
2022-11-07 16:44:44,218:INFO:Importing libraries
2022-11-07 16:44:44,219:INFO:Copying training dataset
2022-11-07 16:44:44,227:INFO:Defining folds
2022-11-07 16:44:44,228:INFO:Declaring metric variables
2022-11-07 16:44:44,241:INFO:Importing untrained model
2022-11-07 16:44:44,252:INFO:Dummy Regressor Imported successfully
2022-11-07 16:44:44,270:INFO:Starting cross validation
2022-11-07 16:44:44,277:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-11-07 16:44:44,595:INFO:Calculating mean and std
2022-11-07 16:44:44,600:INFO:Creating metrics dataframe
2022-11-07 16:44:44,610:INFO:Uploading results into container
2022-11-07 16:44:44,611:INFO:Uploading model into container now
2022-11-07 16:44:44,612:INFO:master_model_container: 18
2022-11-07 16:44:44,612:INFO:display_container: 2
2022-11-07 16:44:44,613:INFO:DummyRegressor()
2022-11-07 16:44:44,613:INFO:create_model() successfully completed......................................
2022-11-07 16:44:44,761:INFO:SubProcess create_model() end ==================================
2022-11-07 16:44:44,761:INFO:Creating metrics dataframe
2022-11-07 16:44:44,819:INFO:Initializing create_model()
2022-11-07 16:44:44,820:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f20c296c9d0>, estimator=ExtraTreesRegressor(n_jobs=-1, random_state=2174), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2022-11-07 16:44:44,820:INFO:Checking exceptions
2022-11-07 16:44:44,828:INFO:Importing libraries
2022-11-07 16:44:44,828:INFO:Copying training dataset
2022-11-07 16:44:44,831:INFO:Defining folds
2022-11-07 16:44:44,832:INFO:Declaring metric variables
2022-11-07 16:44:44,832:INFO:Importing untrained model
2022-11-07 16:44:44,832:INFO:Declaring custom model
2022-11-07 16:44:44,833:INFO:Extra Trees Regressor Imported successfully
2022-11-07 16:44:44,835:INFO:Cross validation set to False
2022-11-07 16:44:44,835:INFO:Fitting Model
2022-11-07 16:44:45,138:INFO:ExtraTreesRegressor(n_jobs=-1, random_state=2174)
2022-11-07 16:44:45,138:INFO:create_model() successfully completed......................................
2022-11-07 16:44:45,378:INFO:master_model_container: 18
2022-11-07 16:44:45,378:INFO:display_container: 2
2022-11-07 16:44:45,379:INFO:ExtraTreesRegressor(n_jobs=-1, random_state=2174)
2022-11-07 16:44:45,380:INFO:compare_models() successfully completed......................................
2022-11-07 16:46:37,260:INFO:Initializing plot_model()
2022-11-07 16:46:37,260:INFO:plot_model(plot=residuals, fold=None, use_train_data=False, verbose=True, display=None, display_format=None, estimator=ExtraTreesRegressor(n_jobs=-1, random_state=2174), feature_name=None, fit_kwargs=None, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x7f20c296c9d0>, system=True)
2022-11-07 16:46:37,260:INFO:Checking exceptions
2022-11-07 16:46:37,372:INFO:Preloading libraries
2022-11-07 16:46:37,388:INFO:Copying training dataset
2022-11-07 16:46:37,388:INFO:Plot type: residuals
2022-11-07 16:47:03,778:INFO:Initializing create_model()
2022-11-07 16:47:03,780:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f20c296c9d0>, estimator=xgboost, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2022-11-07 16:47:03,780:INFO:Checking exceptions
2022-11-07 16:47:51,375:INFO:Initializing plot_model()
2022-11-07 16:47:51,376:INFO:plot_model(plot=parameter, fold=KFold(n_splits=10, random_state=None, shuffle=False), use_train_data=False, verbose=False, display=None, display_format=None, estimator=ExtraTreesRegressor(n_jobs=-1, random_state=4769), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x7f20cc3726d0>, system=True)
2022-11-07 16:47:51,377:INFO:Checking exceptions
2022-11-07 16:47:51,483:INFO:Preloading libraries
2022-11-07 16:47:51,495:INFO:Copying training dataset
2022-11-07 16:47:51,495:INFO:Plot type: parameter
2022-11-07 16:47:51,505:INFO:Visual Rendered Successfully
2022-11-07 16:47:51,688:INFO:plot_model() successfully completed......................................
2022-11-07 16:47:54,511:INFO:Initializing plot_model()
2022-11-07 16:47:54,513:INFO:plot_model(plot=feature_all, fold=KFold(n_splits=10, random_state=None, shuffle=False), use_train_data=False, verbose=False, display=None, display_format=None, estimator=ExtraTreesRegressor(n_jobs=-1, random_state=4769), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x7f20cc3726d0>, system=True)
2022-11-07 16:47:54,514:INFO:Checking exceptions
2022-11-07 16:47:54,620:INFO:Preloading libraries
2022-11-07 16:47:54,630:INFO:Copying training dataset
2022-11-07 16:47:54,631:INFO:Plot type: feature_all
2022-11-07 16:47:54,649:WARNING:No coef_ found. Trying feature_importances_
2022-11-07 16:47:54,849:INFO:Visual Rendered Successfully
2022-11-07 16:47:55,008:INFO:plot_model() successfully completed......................................
2022-11-07 16:47:56,272:INFO:Initializing plot_model()
2022-11-07 16:47:56,273:INFO:plot_model(plot=parameter, fold=KFold(n_splits=10, random_state=None, shuffle=False), use_train_data=False, verbose=False, display=None, display_format=None, estimator=ExtraTreesRegressor(n_jobs=-1, random_state=4769), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x7f20cc3726d0>, system=True)
2022-11-07 16:47:56,273:INFO:Checking exceptions
2022-11-07 16:47:56,382:INFO:Preloading libraries
2022-11-07 16:47:56,393:INFO:Copying training dataset
2022-11-07 16:47:56,393:INFO:Plot type: parameter
2022-11-07 16:47:56,401:INFO:Visual Rendered Successfully
2022-11-07 16:47:56,560:INFO:plot_model() successfully completed......................................
2022-11-07 16:48:08,154:INFO:Initializing plot_model()
2022-11-07 16:48:08,154:INFO:plot_model(plot=cooks, fold=KFold(n_splits=10, random_state=None, shuffle=False), use_train_data=False, verbose=False, display=None, display_format=None, estimator=ExtraTreesRegressor(n_jobs=-1, random_state=4769), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x7f20cc3726d0>, system=True)
2022-11-07 16:48:08,154:INFO:Checking exceptions
2022-11-07 16:48:08,261:INFO:Preloading libraries
2022-11-07 16:48:08,271:INFO:Copying training dataset
2022-11-07 16:48:08,272:INFO:Plot type: cooks
2022-11-07 16:48:09,437:INFO:Initializing plot_model()
2022-11-07 16:48:09,438:INFO:plot_model(plot=rfe, fold=KFold(n_splits=10, random_state=None, shuffle=False), use_train_data=False, verbose=False, display=None, display_format=None, estimator=ExtraTreesRegressor(n_jobs=-1, random_state=4769), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x7f20cc3726d0>, system=True)
2022-11-07 16:48:09,438:INFO:Checking exceptions
2022-11-07 16:48:09,545:INFO:Preloading libraries
2022-11-07 16:48:09,557:INFO:Copying training dataset
2022-11-07 16:48:09,557:INFO:Plot type: rfe
2022-11-07 16:48:09,632:INFO:Fitting Model
